{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Jupyter notebook for the case study (using Python 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the necessary libraries: \n",
    "* Pandas package to efficiently work with DataFrames\n",
    "* NumPy package for math / linear algebra\n",
    "* Datetime to work with date/time data\n",
    "* StandardScaler to normalize the data\n",
    "* LogisticRegression - ML-model to determine key factors in Task 2\n",
    "* RandomForestClassifier - ML-model for predictions in Task 3\n",
    "* Train_test_split - to split data in training and test set\n",
    "* cros_val_score to perform cross-validation when calibrating the model\n",
    "* GridSearchCV - to perform Grid Search to find optimal hyperparameters for ML-models\n",
    "* matplotlib (plt and mpath) for visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.path as mpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_1) Setup_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining dataset names. Can change names to add other datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#uncomment for testing with small datasets\n",
    "name_dataset_0 = 'small_app_dataset.csv' # 'app_dataset.csv'\n",
    "name_dataset_1 = 'small_dataset_1.csv' # 'dataset_1.csv'\n",
    "name_dataset_2 = 'small_dataset_2.csv' # 'dataset_2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name_dataset_0 = 'app_dataset.csv'\n",
    "name_dataset_1 = 'dataset_1.csv'\n",
    "name_dataset_2 = 'dataset_2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining key names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "key1 = 'key1'\n",
    "key2 = 'key2'\n",
    "key_names = [key1, key2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saving CSV fomratted datasets as Pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_0 = pd.read_csv(name_dataset_0, sep=';')\n",
    "dataset_1 = pd.read_csv(name_dataset_1, sep=';')\n",
    "dataset_2 = pd.read_csv(name_dataset_2, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_2) Investigating the datasets - checking how many rows, columns and elements they have_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "function to print the number of columns, rows and elements for each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_col_row_and_cell_count(df):\n",
    "    row_count, column_count = df.shape\n",
    "    element_count = column_count*row_count\n",
    "    print('column count:  ', column_count)\n",
    "    print('row count:     ', row_count)\n",
    "    print('element count: ', element_count)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total number of row and column count for each dataset (including NA values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) dataset 0\n",
      "column count:   5\n",
      "row count:      798\n",
      "element count:  3990\n",
      "\n",
      "2) dataset 1\n",
      "column count:   169\n",
      "row count:      14571\n",
      "element count:  2462499\n",
      "\n",
      "3) dataset 2\n",
      "column count:   37\n",
      "row count:      10137\n",
      "element count:  375069\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('1) dataset 0')\n",
    "print_col_row_and_cell_count(dataset_0)\n",
    "print('2) dataset 1')\n",
    "print_col_row_and_cell_count(dataset_1)\n",
    "print('3) dataset 2')\n",
    "print_col_row_and_cell_count(dataset_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_3) Joining the datasets_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_0_and_1 = pd.merge(dataset_0, dataset_1, how='left', on=key2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_not_cleaned = pd.merge(dataset_0_and_1, dataset_2, how='left', on=key1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_full - before cleaning NAs\n",
      "column count:   209\n",
      "row count:      798\n",
      "element count:  166782\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('dataset_full - before cleaning NAs')\n",
    "print_col_row_and_cell_count(dataset_full_not_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_4) Dropping columns with keys. Removing columns and rows containing many NA values. Saving the final dateset to CSV file_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the join is done, keys are not needed. Dropping them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_not_cleaned_keys_dropped = dataset_full_not_cleaned.drop(key_names, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to deal with NA values. It will drop rows and columns if the amount of non-NA values in a given column or row is below a given threshold. By default it is 20% for columns and 5% for rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_rows_and_cols_with_NA_below_thresholds(input_df, key_names=key_names, col_thresh=0.20, row_thresh=0.05):\n",
    "    df = input_df.copy(deep=True)\n",
    "    \n",
    "    number_of_cols = len(list(df.columns))\n",
    "    row_threshold_integer = round(row_thresh * number_of_cols)\n",
    "    df = df.dropna(axis=0, thresh=row_threshold_integer) # droping rows that have non-NA cell count below threshold\n",
    "    \n",
    "    number_of_rows = len(df)\n",
    "    col_threshold_integer = round(col_thresh * number_of_rows)\n",
    "    output_df = df.dropna(axis=1, thresh=col_threshold_integer).loc[:] # droping columns that have non-NA cell count below threshold\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean = drop_rows_and_cols_with_NA_below_thresholds(dataset_full_not_cleaned_keys_dropped, \n",
    "                                                                 col_thresh=0.20, row_thresh=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_full_clean - after some columns and rows with many missing values are removed\n",
      "column count:   60\n",
      "row count:      772\n",
      "element count:  46320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('dataset_full_clean - after some columns and rows with many missing values are removed')\n",
    "print_col_row_and_cell_count(dataset_full_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the final dataset as a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean.to_csv('output_dataset_full_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_5) Observations on data integrity _**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we see that a lot of data is not used. In the final table we have 798 rows (the same as in the 'master' dataset_0, because that dataset is used in left outer join). Dataset1 has 14571 rows, and dataset2 - 10137. Since response variable is available only for these 798 rows, we have to ignore most of the rows from dataset1 and dataset2. \n",
    "\n",
    "On top of that, there are a lot of missing values (NA), especially in the dataset1. The combined dataset has 209 columns, before the columns with many NAs are removed. After I remove them, applying 20% threshold, only 62 columns remain. [UPDATE - provide counts on NA in each table. Maybe update print function to show NA cells as well]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_1) Setup_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to clean the dataset and to make various transformations before performing any analysis on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Defining the name of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'response'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Defining function to get all column names except for the target and key columns. Will allow to dynamically analyze dataframes without the need to know exact columns they have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_col_names_without_target(dataframe, target = target):\n",
    "    column_names_list = list(dataframe.columns)\n",
    "    if target in column_names_list:\n",
    "        column_names_list.remove(target)\n",
    "    return column_names_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) We want to determine which factors are the most important in predicting target variable (response). Many variables still has too many NAs, so I will use more agressive column threshold (60%) to remove columns/factors with many missing values. Otherwise, we would introduce too much bias if we would try to impute them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column count:   38\n",
      "row count:      772\n",
      "element count:  29336\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### can play with that a bit ## initially was 0.60 and 0.05\n",
    "dataset_full = drop_rows_and_cols_with_NA_below_thresholds(dataset_full_clean, col_thresh=0.60, row_thresh=0.05)\n",
    "print_col_row_and_cell_count(dataset_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Python uses '.' as a decimal point. However, in datasets sometimes we get ',' as a decimal point. Need to replace ',' with '.'. I will replace all '+' and '_' with ' ' and convert all string to lower case, that would help to allign string formatting and reduce some noise in the data. After this is done, will need to convert floats stored as string to Python floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_commas_with_dots_in_string(single_string):\n",
    "    if type(single_string) == str:\n",
    "        single_string = single_string.replace(',','.')\n",
    "    return single_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying a function on each cell of a dataframe\n",
    "dataset_full = dataset_full.applymap(replace_commas_with_dots_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_spec_chars_with_space_in_string(single_string):\n",
    "    if type(single_string) == str:\n",
    "        single_string = single_string.replace('+',' ')\n",
    "        single_string = single_string.replace('_',' ')\n",
    "    return single_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full = dataset_full.applymap(replace_spec_chars_with_space_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def strings_to_lower_case(col):\n",
    "    if col.dtype=='O':\n",
    "        col = col.str.lower()\n",
    "    return col        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full = dataset_full.apply(strings_to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to convert floats stored as string to floats\n",
    "def convert_floats_in_string_to_floats(element):\n",
    "    if type(element) == str:\n",
    "        try:\n",
    "            return float(element)\n",
    "        except (ValueError, TypeError):\n",
    "            return element\n",
    "    return element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full = dataset_full.applymap(convert_floats_in_string_to_floats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(e) Some columns might contain dates in string format. I will convert those to floats. It is done by firstly converting string dates to datetime format. Then from those datetimes I substract epoch date (1 jan 1970) and convert it to seconds, which is in float format. Essentially, each cell with a date after the transformation will show how many seconds has passed after 1 jan 1970 till this cell's initial date. This number is in float, so regression ML algorithms (logistic regression, random forest classifier, etc) can be applied on it.\n",
    "\n",
    "The function below will do this transformation. It is a vectorized function, so it is efficient. Also, it will convert only those columns, that initially contain dates in string, otherwise it will not change the columns. Thus, it is very general and would work on various datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_dates_to_sec_after_epoch_as_float(input_col):\n",
    "    if input_col.dtype=='O': #in pandas dataframe columns containing Strings, has type Object, or 'O'\n",
    "        col_datetime=pd.to_datetime(input_col, format='%Y-%m-%d %H:%M', errors='ignore') #convert to datetime only if format is '%Y-%m-%d %H:%M'\n",
    "        if col_datetime.dtype=='datetime64[ns]': \n",
    "            epoch_timestamp_col = col_datetime - dt.datetime(1970, 1, 1)\n",
    "            sec_float_col = epoch_timestamp_col / np.timedelta64(1, 's')\n",
    "            return sec_float_col\n",
    "        return col_datetime\n",
    "    else:\n",
    "        return input_col           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_time_converted = dataset_full.apply(string_dates_to_sec_after_epoch_as_float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) We have converted string columns that contain dates and floats. Now, the remaining columns with string (text) contain only categorical variables (e.g. 'big', 'small' and 'medium'). We need to convert this information to numerical data. I will do it by  creating a binary variable for each category. Binary variable (dummy) being 1 means that a given record belongs to a given category, and 0 indicates that it does not belong. If the value is missing, then a new category ('missing') is created. The initial column with strings is dropped. For example, column B contains 'yes', 'no' and 'N/A', then column B is dropped, and 3 new columns are created: B_yes, B_no and B_NA.\n",
    "However, it might be not optimal to create too many binary columns for 1 column. If the number of unique categories in one string column is over a specified threshold, then I will drop it. For that, I will defina a function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_string_cols_with_unique_value_count_over_threshold(input_df, unique_count_threshold = 50):\n",
    "    df = input_df.copy(deep=True)\n",
    "    unique_counts_in_string_cols_series = df.select_dtypes(include=[object]).nunique() #string is 'object' type\n",
    "    string_cols_over_threshold_series = unique_counts_in_string_cols_series[unique_counts_in_string_cols_series>unique_count_threshold]\n",
    "    list_of_string_cols_over_threshold = list(string_cols_over_threshold_series.keys())\n",
    "    df = df.drop(list_of_string_cols_over_threshold, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772, 38)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full_time_converted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_some_string_cols_removed = remove_string_cols_with_unique_value_count_over_threshold(\n",
    "    input_df = dataset_full_time_converted, unique_count_threshold = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(772, 37)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_full_with_some_string_cols_removed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_dummies = pd.get_dummies(dataset_full_with_some_string_cols_removed, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_dummies.to_csv('out_dataset_full_with_dummies.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) Imputing remaining missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still many missing values. In order to use Machine Learning models in Task 2 and 3, I need to remove or impute missing values (NAs). In the previous parts I have removed some. The remaining will be imputed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am imputing missing values with a median value for each feature, as it is less biased than mean (outliers have a significant impact on mean, but not on median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_filled = dataset_full_with_dummies.fillna(dataset_full_with_dummies.median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) Normalizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving target column separately, as it should not be transformed (we need 1 and 0 for classification)\n",
    "y = pd.DataFrame(dataset_filled[target].values)\n",
    "predictors = get_col_names_without_target(dataset_filled)\n",
    "\n",
    "scaler = StandardScaler() #creating an instance fo StandardScaler\n",
    "scaler.fit(dataset_filled[predictors]) #normalizing only predictors\n",
    "dataset_normalized_np_array = scaler.transform(dataset_filled[predictors].values) #creating numpy dataframe\n",
    "dataset_normalized = pd.DataFrame(dataset_normalized_np_array, columns=predictors) #converting numpy to pandas dataframe. Adding columns\n",
    "dataset_normalized[target] = y #adding target back\n",
    "dataset_normalized = dataset_normalized[[target] + predictors] #for convenience putting target as a first columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_2) Looking at correlations_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creating a correlation matrix between target and all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>v001</th>\n",
       "      <th>v002</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v14</th>\n",
       "      <th>v29</th>\n",
       "      <th>v120</th>\n",
       "      <th>v123</th>\n",
       "      <th>v173</th>\n",
       "      <th>...</th>\n",
       "      <th>v204_mobile</th>\n",
       "      <th>v204_residential</th>\n",
       "      <th>v204_wifi</th>\n",
       "      <th>v204_wired</th>\n",
       "      <th>v204_nan</th>\n",
       "      <th>v172.1_n</th>\n",
       "      <th>v172.1_p</th>\n",
       "      <th>v172.1_u</th>\n",
       "      <th>v172.1_y</th>\n",
       "      <th>v172.1_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>response</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016594</td>\n",
       "      <td>-0.072353</td>\n",
       "      <td>-0.007668</td>\n",
       "      <td>-0.013845</td>\n",
       "      <td>-0.017492</td>\n",
       "      <td>-0.172579</td>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.042317</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.077241</td>\n",
       "      <td>-0.148913</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>-0.045414</td>\n",
       "      <td>-0.010143</td>\n",
       "      <td>0.009589</td>\n",
       "      <td>-0.032629</td>\n",
       "      <td>0.010840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v001</th>\n",
       "      <td>0.016594</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.028172</td>\n",
       "      <td>0.026159</td>\n",
       "      <td>0.042749</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>-0.042262</td>\n",
       "      <td>0.062640</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>0.017588</td>\n",
       "      <td>-0.037195</td>\n",
       "      <td>-0.082489</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>0.039101</td>\n",
       "      <td>-0.055188</td>\n",
       "      <td>-0.057636</td>\n",
       "      <td>0.060728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v002</th>\n",
       "      <td>-0.072353</td>\n",
       "      <td>-0.028172</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.161154</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>0.156599</td>\n",
       "      <td>0.150948</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>0.056365</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>-0.090875</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.067423</td>\n",
       "      <td>0.046851</td>\n",
       "      <td>-0.035833</td>\n",
       "      <td>-0.040280</td>\n",
       "      <td>-0.022865</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.039053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v4</th>\n",
       "      <td>-0.007668</td>\n",
       "      <td>0.026159</td>\n",
       "      <td>0.161154</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.738650</td>\n",
       "      <td>0.985423</td>\n",
       "      <td>0.302075</td>\n",
       "      <td>0.209135</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>-0.061750</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>-0.044838</td>\n",
       "      <td>0.061015</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>-0.031783</td>\n",
       "      <td>-0.048033</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>0.061561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v5</th>\n",
       "      <td>-0.013845</td>\n",
       "      <td>0.042749</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>0.738650</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.739335</td>\n",
       "      <td>0.312050</td>\n",
       "      <td>0.329635</td>\n",
       "      <td>0.383194</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>-0.120228</td>\n",
       "      <td>0.070998</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.048088</td>\n",
       "      <td>-0.061443</td>\n",
       "      <td>-0.012874</td>\n",
       "      <td>0.079078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v14</th>\n",
       "      <td>-0.017492</td>\n",
       "      <td>0.026423</td>\n",
       "      <td>0.156599</td>\n",
       "      <td>0.985423</td>\n",
       "      <td>0.739335</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.303795</td>\n",
       "      <td>0.225246</td>\n",
       "      <td>0.289810</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>-0.059451</td>\n",
       "      <td>0.048794</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.023801</td>\n",
       "      <td>-0.032463</td>\n",
       "      <td>-0.039713</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.052564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v29</th>\n",
       "      <td>-0.172579</td>\n",
       "      <td>-0.042262</td>\n",
       "      <td>0.150948</td>\n",
       "      <td>0.302075</td>\n",
       "      <td>0.312050</td>\n",
       "      <td>0.303795</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.157450</td>\n",
       "      <td>0.163382</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>-0.048993</td>\n",
       "      <td>0.101896</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>-0.012713</td>\n",
       "      <td>-0.013708</td>\n",
       "      <td>-0.061391</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>-0.010858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v120</th>\n",
       "      <td>0.051613</td>\n",
       "      <td>0.062640</td>\n",
       "      <td>-0.019944</td>\n",
       "      <td>0.209135</td>\n",
       "      <td>0.329635</td>\n",
       "      <td>0.225246</td>\n",
       "      <td>0.157450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.733878</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.030635</td>\n",
       "      <td>-0.050454</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>-0.037192</td>\n",
       "      <td>0.143450</td>\n",
       "      <td>-0.024368</td>\n",
       "      <td>-0.015127</td>\n",
       "      <td>-0.134820</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>0.145945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v123</th>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.015985</td>\n",
       "      <td>0.277026</td>\n",
       "      <td>0.383194</td>\n",
       "      <td>0.289810</td>\n",
       "      <td>0.163382</td>\n",
       "      <td>0.733878</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.051716</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>-0.049136</td>\n",
       "      <td>0.017068</td>\n",
       "      <td>-0.029950</td>\n",
       "      <td>0.063949</td>\n",
       "      <td>-0.018716</td>\n",
       "      <td>-0.016883</td>\n",
       "      <td>-0.055532</td>\n",
       "      <td>-0.007136</td>\n",
       "      <td>0.066902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v173</th>\n",
       "      <td>0.042317</td>\n",
       "      <td>-0.023079</td>\n",
       "      <td>0.056365</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.016430</td>\n",
       "      <td>0.058766</td>\n",
       "      <td>0.017526</td>\n",
       "      <td>0.051716</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>-0.099213</td>\n",
       "      <td>0.031243</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>-0.074746</td>\n",
       "      <td>-0.059899</td>\n",
       "      <td>-0.015655</td>\n",
       "      <td>-0.058134</td>\n",
       "      <td>0.060699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v174</th>\n",
       "      <td>-0.044762</td>\n",
       "      <td>0.023308</td>\n",
       "      <td>-0.054430</td>\n",
       "      <td>-0.009915</td>\n",
       "      <td>-0.020723</td>\n",
       "      <td>-0.015724</td>\n",
       "      <td>-0.056870</td>\n",
       "      <td>-0.016925</td>\n",
       "      <td>-0.050899</td>\n",
       "      <td>-0.999832</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011385</td>\n",
       "      <td>0.086854</td>\n",
       "      <td>-0.025317</td>\n",
       "      <td>-0.042905</td>\n",
       "      <td>-0.055338</td>\n",
       "      <td>0.074804</td>\n",
       "      <td>0.058405</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.057632</td>\n",
       "      <td>-0.056421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v175</th>\n",
       "      <td>0.016363</td>\n",
       "      <td>0.001833</td>\n",
       "      <td>0.036745</td>\n",
       "      <td>0.022826</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>-0.002560</td>\n",
       "      <td>0.011377</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>0.096303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009334</td>\n",
       "      <td>-0.098594</td>\n",
       "      <td>-0.037143</td>\n",
       "      <td>-0.022360</td>\n",
       "      <td>0.165227</td>\n",
       "      <td>-0.215025</td>\n",
       "      <td>-0.271529</td>\n",
       "      <td>0.040504</td>\n",
       "      <td>-0.369961</td>\n",
       "      <td>0.163113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v176</th>\n",
       "      <td>-0.019849</td>\n",
       "      <td>-0.002542</td>\n",
       "      <td>-0.034321</td>\n",
       "      <td>-0.022698</td>\n",
       "      <td>-0.020502</td>\n",
       "      <td>-0.020099</td>\n",
       "      <td>0.005673</td>\n",
       "      <td>-0.012557</td>\n",
       "      <td>-0.010168</td>\n",
       "      <td>-0.094509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>0.084703</td>\n",
       "      <td>0.050265</td>\n",
       "      <td>0.028594</td>\n",
       "      <td>-0.173165</td>\n",
       "      <td>0.215696</td>\n",
       "      <td>0.269962</td>\n",
       "      <td>-0.032460</td>\n",
       "      <td>0.369446</td>\n",
       "      <td>-0.171096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v177</th>\n",
       "      <td>0.048428</td>\n",
       "      <td>-0.002857</td>\n",
       "      <td>-0.021130</td>\n",
       "      <td>0.009787</td>\n",
       "      <td>0.030602</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>-0.045187</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>-0.069621</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.040159</td>\n",
       "      <td>-0.023157</td>\n",
       "      <td>-0.060555</td>\n",
       "      <td>-0.029300</td>\n",
       "      <td>0.121614</td>\n",
       "      <td>-0.150750</td>\n",
       "      <td>0.016069</td>\n",
       "      <td>-0.030689</td>\n",
       "      <td>-0.217245</td>\n",
       "      <td>0.120156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v180</th>\n",
       "      <td>0.022540</td>\n",
       "      <td>0.034855</td>\n",
       "      <td>0.036069</td>\n",
       "      <td>0.018493</td>\n",
       "      <td>0.031174</td>\n",
       "      <td>0.016046</td>\n",
       "      <td>-0.021656</td>\n",
       "      <td>0.013454</td>\n",
       "      <td>-0.003963</td>\n",
       "      <td>0.065332</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023473</td>\n",
       "      <td>-0.132832</td>\n",
       "      <td>-0.054981</td>\n",
       "      <td>-0.029743</td>\n",
       "      <td>0.227068</td>\n",
       "      <td>-0.170986</td>\n",
       "      <td>-0.175666</td>\n",
       "      <td>-0.053170</td>\n",
       "      <td>-0.348206</td>\n",
       "      <td>0.224360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v181</th>\n",
       "      <td>0.026332</td>\n",
       "      <td>-0.013670</td>\n",
       "      <td>0.046281</td>\n",
       "      <td>0.040124</td>\n",
       "      <td>0.032608</td>\n",
       "      <td>0.042488</td>\n",
       "      <td>0.034544</td>\n",
       "      <td>-0.011165</td>\n",
       "      <td>-0.008010</td>\n",
       "      <td>0.096625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103508</td>\n",
       "      <td>-0.010617</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.099824</td>\n",
       "      <td>-0.140203</td>\n",
       "      <td>-0.053073</td>\n",
       "      <td>-0.053073</td>\n",
       "      <td>0.163087</td>\n",
       "      <td>-0.027995</td>\n",
       "      <td>-0.138417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v182</th>\n",
       "      <td>0.066963</td>\n",
       "      <td>0.027891</td>\n",
       "      <td>0.049610</td>\n",
       "      <td>-0.013755</td>\n",
       "      <td>0.037750</td>\n",
       "      <td>-0.017427</td>\n",
       "      <td>0.010566</td>\n",
       "      <td>0.037434</td>\n",
       "      <td>0.028961</td>\n",
       "      <td>0.514751</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.117849</td>\n",
       "      <td>-0.152248</td>\n",
       "      <td>-0.138372</td>\n",
       "      <td>-0.071711</td>\n",
       "      <td>0.435554</td>\n",
       "      <td>-0.054048</td>\n",
       "      <td>-0.085544</td>\n",
       "      <td>-0.364803</td>\n",
       "      <td>-0.108406</td>\n",
       "      <td>0.436544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v183</th>\n",
       "      <td>0.021228</td>\n",
       "      <td>0.031512</td>\n",
       "      <td>0.023708</td>\n",
       "      <td>-0.007962</td>\n",
       "      <td>-0.008904</td>\n",
       "      <td>-0.009189</td>\n",
       "      <td>-0.039029</td>\n",
       "      <td>-0.008797</td>\n",
       "      <td>-0.017028</td>\n",
       "      <td>0.052509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023692</td>\n",
       "      <td>-0.101339</td>\n",
       "      <td>-0.016571</td>\n",
       "      <td>0.034264</td>\n",
       "      <td>0.093479</td>\n",
       "      <td>-0.075338</td>\n",
       "      <td>-0.075338</td>\n",
       "      <td>-0.017642</td>\n",
       "      <td>-0.152879</td>\n",
       "      <td>0.092288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v185</th>\n",
       "      <td>0.018609</td>\n",
       "      <td>0.032782</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>0.016085</td>\n",
       "      <td>0.026678</td>\n",
       "      <td>0.013559</td>\n",
       "      <td>-0.022860</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>-0.006240</td>\n",
       "      <td>0.057240</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023946</td>\n",
       "      <td>-0.124233</td>\n",
       "      <td>-0.055139</td>\n",
       "      <td>-0.017898</td>\n",
       "      <td>0.214553</td>\n",
       "      <td>-0.162560</td>\n",
       "      <td>-0.190442</td>\n",
       "      <td>-0.041469</td>\n",
       "      <td>-0.341731</td>\n",
       "      <td>0.211820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v191</th>\n",
       "      <td>0.194025</td>\n",
       "      <td>-0.026984</td>\n",
       "      <td>-0.011798</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>-0.013643</td>\n",
       "      <td>-0.074641</td>\n",
       "      <td>-0.006081</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>-0.004563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.070954</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>-0.122082</td>\n",
       "      <td>-0.018557</td>\n",
       "      <td>-0.018557</td>\n",
       "      <td>0.130693</td>\n",
       "      <td>-0.024377</td>\n",
       "      <td>-0.120527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v192</th>\n",
       "      <td>0.194025</td>\n",
       "      <td>-0.026984</td>\n",
       "      <td>-0.011798</td>\n",
       "      <td>-0.017876</td>\n",
       "      <td>-0.017656</td>\n",
       "      <td>-0.013643</td>\n",
       "      <td>-0.074641</td>\n",
       "      <td>-0.006081</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>-0.004563</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001890</td>\n",
       "      <td>0.070954</td>\n",
       "      <td>0.032342</td>\n",
       "      <td>0.018522</td>\n",
       "      <td>-0.122082</td>\n",
       "      <td>-0.018557</td>\n",
       "      <td>-0.018557</td>\n",
       "      <td>0.130693</td>\n",
       "      <td>-0.024377</td>\n",
       "      <td>-0.120527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v196</th>\n",
       "      <td>0.034318</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.003506</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>-0.014796</td>\n",
       "      <td>-0.011881</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.063331</td>\n",
       "      <td>-0.026424</td>\n",
       "      <td>-0.072091</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.014395</td>\n",
       "      <td>-0.071173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v197</th>\n",
       "      <td>0.034318</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.003506</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>-0.014796</td>\n",
       "      <td>-0.011881</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.063331</td>\n",
       "      <td>-0.026424</td>\n",
       "      <td>-0.072091</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.014395</td>\n",
       "      <td>-0.071173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v200</th>\n",
       "      <td>0.029653</td>\n",
       "      <td>0.030867</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>-0.013900</td>\n",
       "      <td>0.011399</td>\n",
       "      <td>-0.011734</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.007363</td>\n",
       "      <td>0.009828</td>\n",
       "      <td>0.008125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028754</td>\n",
       "      <td>0.032062</td>\n",
       "      <td>0.029781</td>\n",
       "      <td>0.014407</td>\n",
       "      <td>-0.099252</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>0.005975</td>\n",
       "      <td>-0.042078</td>\n",
       "      <td>0.007848</td>\n",
       "      <td>0.038805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v178_certified</th>\n",
       "      <td>-0.024196</td>\n",
       "      <td>0.020057</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>-0.007672</td>\n",
       "      <td>0.023338</td>\n",
       "      <td>-0.007340</td>\n",
       "      <td>-0.024240</td>\n",
       "      <td>-0.011752</td>\n",
       "      <td>-0.014852</td>\n",
       "      <td>-0.085115</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023462</td>\n",
       "      <td>-0.026161</td>\n",
       "      <td>-0.024300</td>\n",
       "      <td>-0.011756</td>\n",
       "      <td>-0.032072</td>\n",
       "      <td>0.263955</td>\n",
       "      <td>0.263955</td>\n",
       "      <td>-0.075650</td>\n",
       "      <td>-0.006404</td>\n",
       "      <td>-0.031664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v178_emailinexistent</th>\n",
       "      <td>0.012306</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.012175</td>\n",
       "      <td>-0.018744</td>\n",
       "      <td>-0.036280</td>\n",
       "      <td>-0.018398</td>\n",
       "      <td>-0.034325</td>\n",
       "      <td>-0.019692</td>\n",
       "      <td>-0.013774</td>\n",
       "      <td>0.039195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061771</td>\n",
       "      <td>-0.037046</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>0.065733</td>\n",
       "      <td>-0.045416</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>-0.009068</td>\n",
       "      <td>-0.044837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v178_validdomain</th>\n",
       "      <td>0.021854</td>\n",
       "      <td>-0.012438</td>\n",
       "      <td>0.045308</td>\n",
       "      <td>0.043204</td>\n",
       "      <td>0.042259</td>\n",
       "      <td>0.045596</td>\n",
       "      <td>0.038166</td>\n",
       "      <td>-0.009455</td>\n",
       "      <td>-0.007618</td>\n",
       "      <td>0.081651</td>\n",
       "      <td>...</td>\n",
       "      <td>0.092262</td>\n",
       "      <td>-0.007678</td>\n",
       "      <td>0.005244</td>\n",
       "      <td>0.089239</td>\n",
       "      <td>-0.139178</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>-0.021156</td>\n",
       "      <td>0.148996</td>\n",
       "      <td>-0.027790</td>\n",
       "      <td>-0.137405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v178_verified</th>\n",
       "      <td>-0.019354</td>\n",
       "      <td>-0.055177</td>\n",
       "      <td>-0.059419</td>\n",
       "      <td>-0.074205</td>\n",
       "      <td>-0.091117</td>\n",
       "      <td>-0.066806</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>-0.130327</td>\n",
       "      <td>-0.056529</td>\n",
       "      <td>-0.091060</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.315263</td>\n",
       "      <td>0.280222</td>\n",
       "      <td>0.087643</td>\n",
       "      <td>-0.866443</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>0.038576</td>\n",
       "      <td>0.811306</td>\n",
       "      <td>0.088766</td>\n",
       "      <td>-0.879486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v178_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v193_yes</th>\n",
       "      <td>-0.022007</td>\n",
       "      <td>-0.053118</td>\n",
       "      <td>-0.059391</td>\n",
       "      <td>-0.075180</td>\n",
       "      <td>-0.088775</td>\n",
       "      <td>-0.067731</td>\n",
       "      <td>-0.001522</td>\n",
       "      <td>-0.131854</td>\n",
       "      <td>-0.058246</td>\n",
       "      <td>-0.100439</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221806</td>\n",
       "      <td>0.313057</td>\n",
       "      <td>0.278148</td>\n",
       "      <td>0.086545</td>\n",
       "      <td>-0.871616</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>0.804728</td>\n",
       "      <td>0.088249</td>\n",
       "      <td>-0.884640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v193_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v194_yes</th>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.060728</td>\n",
       "      <td>-0.039053</td>\n",
       "      <td>-0.061561</td>\n",
       "      <td>-0.079078</td>\n",
       "      <td>-0.052564</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>-0.145945</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.296234</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>-0.987261</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.922211</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v194_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v195_low</th>\n",
       "      <td>0.034318</td>\n",
       "      <td>-0.003731</td>\n",
       "      <td>-0.003506</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.008439</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>-0.014796</td>\n",
       "      <td>-0.011881</td>\n",
       "      <td>0.229137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037737</td>\n",
       "      <td>-0.002411</td>\n",
       "      <td>0.063331</td>\n",
       "      <td>-0.026424</td>\n",
       "      <td>-0.072091</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>-0.010958</td>\n",
       "      <td>0.077176</td>\n",
       "      <td>-0.014395</td>\n",
       "      <td>-0.071173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v195_moderate</th>\n",
       "      <td>-0.019232</td>\n",
       "      <td>-0.058975</td>\n",
       "      <td>-0.037650</td>\n",
       "      <td>-0.061829</td>\n",
       "      <td>-0.080104</td>\n",
       "      <td>-0.052669</td>\n",
       "      <td>0.009971</td>\n",
       "      <td>-0.140282</td>\n",
       "      <td>-0.063038</td>\n",
       "      <td>-0.116889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.272741</td>\n",
       "      <td>0.315191</td>\n",
       "      <td>0.276453</td>\n",
       "      <td>0.147937</td>\n",
       "      <td>-0.955916</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.061350</td>\n",
       "      <td>0.890484</td>\n",
       "      <td>0.080590</td>\n",
       "      <td>-0.968711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v195_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v199_moderate</th>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.060728</td>\n",
       "      <td>-0.039053</td>\n",
       "      <td>-0.061561</td>\n",
       "      <td>-0.079078</td>\n",
       "      <td>-0.052564</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>-0.145945</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.296234</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>-0.987261</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.922211</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v199_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v201_moderate by proxy reputation and country code</th>\n",
       "      <td>-0.006696</td>\n",
       "      <td>-0.056206</td>\n",
       "      <td>-0.038764</td>\n",
       "      <td>-0.063222</td>\n",
       "      <td>-0.077169</td>\n",
       "      <td>-0.053964</td>\n",
       "      <td>0.010944</td>\n",
       "      <td>-0.144312</td>\n",
       "      <td>-0.065262</td>\n",
       "      <td>-0.059319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.288782</td>\n",
       "      <td>0.322007</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>0.144695</td>\n",
       "      <td>-0.996813</td>\n",
       "      <td>0.060006</td>\n",
       "      <td>0.060006</td>\n",
       "      <td>0.912504</td>\n",
       "      <td>0.078824</td>\n",
       "      <td>-0.990417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v201_moderate risk</th>\n",
       "      <td>-0.029653</td>\n",
       "      <td>-0.030867</td>\n",
       "      <td>-0.000895</td>\n",
       "      <td>0.013900</td>\n",
       "      <td>-0.011399</td>\n",
       "      <td>0.011734</td>\n",
       "      <td>-0.000951</td>\n",
       "      <td>-0.007363</td>\n",
       "      <td>-0.009828</td>\n",
       "      <td>-0.008125</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.028754</td>\n",
       "      <td>-0.032062</td>\n",
       "      <td>-0.029781</td>\n",
       "      <td>-0.014407</td>\n",
       "      <td>0.099252</td>\n",
       "      <td>-0.005975</td>\n",
       "      <td>-0.005975</td>\n",
       "      <td>0.042078</td>\n",
       "      <td>-0.007848</td>\n",
       "      <td>-0.038805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v201_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v202_good</th>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.060728</td>\n",
       "      <td>-0.039053</td>\n",
       "      <td>-0.061561</td>\n",
       "      <td>-0.079078</td>\n",
       "      <td>-0.052564</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>-0.145945</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.296234</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>-0.987261</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.922211</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v202_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v203_no</th>\n",
       "      <td>-0.010840</td>\n",
       "      <td>-0.060728</td>\n",
       "      <td>-0.039053</td>\n",
       "      <td>-0.061561</td>\n",
       "      <td>-0.079078</td>\n",
       "      <td>-0.052564</td>\n",
       "      <td>0.010858</td>\n",
       "      <td>-0.145945</td>\n",
       "      <td>-0.066902</td>\n",
       "      <td>-0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286015</td>\n",
       "      <td>0.318922</td>\n",
       "      <td>0.296234</td>\n",
       "      <td>0.143308</td>\n",
       "      <td>-0.987261</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.059431</td>\n",
       "      <td>0.922211</td>\n",
       "      <td>0.078068</td>\n",
       "      <td>-1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v203_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_business</th>\n",
       "      <td>0.105443</td>\n",
       "      <td>0.028402</td>\n",
       "      <td>0.018046</td>\n",
       "      <td>-0.015789</td>\n",
       "      <td>0.015715</td>\n",
       "      <td>-0.015172</td>\n",
       "      <td>-0.038260</td>\n",
       "      <td>-0.007491</td>\n",
       "      <td>-0.013774</td>\n",
       "      <td>0.037171</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.033224</td>\n",
       "      <td>-0.037046</td>\n",
       "      <td>-0.034411</td>\n",
       "      <td>-0.016647</td>\n",
       "      <td>-0.045416</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>-0.006903</td>\n",
       "      <td>0.048620</td>\n",
       "      <td>-0.009068</td>\n",
       "      <td>-0.044837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_cellular</th>\n",
       "      <td>0.097367</td>\n",
       "      <td>-0.031968</td>\n",
       "      <td>-0.139309</td>\n",
       "      <td>-0.051358</td>\n",
       "      <td>-0.044091</td>\n",
       "      <td>-0.052360</td>\n",
       "      <td>-0.120952</td>\n",
       "      <td>-0.033863</td>\n",
       "      <td>-0.031032</td>\n",
       "      <td>-0.070660</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.144226</td>\n",
       "      <td>-0.160820</td>\n",
       "      <td>-0.149379</td>\n",
       "      <td>-0.072265</td>\n",
       "      <td>-0.197154</td>\n",
       "      <td>0.065831</td>\n",
       "      <td>0.065831</td>\n",
       "      <td>0.152270</td>\n",
       "      <td>0.034042</td>\n",
       "      <td>-0.194643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_college</th>\n",
       "      <td>-0.017098</td>\n",
       "      <td>0.010337</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>-0.019185</td>\n",
       "      <td>-0.018105</td>\n",
       "      <td>-0.020001</td>\n",
       "      <td>-0.027601</td>\n",
       "      <td>-0.014393</td>\n",
       "      <td>-0.005667</td>\n",
       "      <td>0.003519</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016579</td>\n",
       "      <td>-0.018487</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.008307</td>\n",
       "      <td>-0.022664</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0.024262</td>\n",
       "      <td>-0.004525</td>\n",
       "      <td>-0.022375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_dialup</th>\n",
       "      <td>-0.017098</td>\n",
       "      <td>-0.005009</td>\n",
       "      <td>-0.015165</td>\n",
       "      <td>0.020140</td>\n",
       "      <td>0.016491</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>-0.022365</td>\n",
       "      <td>-0.014393</td>\n",
       "      <td>-0.005667</td>\n",
       "      <td>0.055606</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016579</td>\n",
       "      <td>-0.018487</td>\n",
       "      <td>-0.017172</td>\n",
       "      <td>-0.008307</td>\n",
       "      <td>-0.022664</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>-0.003445</td>\n",
       "      <td>0.024262</td>\n",
       "      <td>-0.004525</td>\n",
       "      <td>-0.022375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_mobile</th>\n",
       "      <td>-0.024923</td>\n",
       "      <td>0.021698</td>\n",
       "      <td>-0.010204</td>\n",
       "      <td>0.016620</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.019737</td>\n",
       "      <td>0.033399</td>\n",
       "      <td>-0.030635</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>0.017673</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.236314</td>\n",
       "      <td>-0.219503</td>\n",
       "      <td>-0.106188</td>\n",
       "      <td>-0.289705</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>0.280704</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>-0.286015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_residential</th>\n",
       "      <td>0.077241</td>\n",
       "      <td>0.017588</td>\n",
       "      <td>-0.090875</td>\n",
       "      <td>-0.061750</td>\n",
       "      <td>-0.120228</td>\n",
       "      <td>-0.059451</td>\n",
       "      <td>-0.048993</td>\n",
       "      <td>-0.050454</td>\n",
       "      <td>-0.049136</td>\n",
       "      <td>-0.099213</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236314</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.244757</td>\n",
       "      <td>-0.118406</td>\n",
       "      <td>-0.323037</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>0.051805</td>\n",
       "      <td>0.290778</td>\n",
       "      <td>0.038595</td>\n",
       "      <td>-0.318922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_wifi</th>\n",
       "      <td>-0.148913</td>\n",
       "      <td>-0.037195</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>0.043348</td>\n",
       "      <td>0.070998</td>\n",
       "      <td>0.048794</td>\n",
       "      <td>0.101896</td>\n",
       "      <td>-0.033850</td>\n",
       "      <td>0.017068</td>\n",
       "      <td>0.031243</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.219503</td>\n",
       "      <td>-0.244757</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.109982</td>\n",
       "      <td>-0.300056</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>-0.045610</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.047903</td>\n",
       "      <td>-0.296234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_wired</th>\n",
       "      <td>-0.002650</td>\n",
       "      <td>-0.082489</td>\n",
       "      <td>0.067423</td>\n",
       "      <td>-0.044838</td>\n",
       "      <td>-0.036415</td>\n",
       "      <td>-0.043985</td>\n",
       "      <td>0.056682</td>\n",
       "      <td>-0.037192</td>\n",
       "      <td>-0.029950</td>\n",
       "      <td>0.045614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.106188</td>\n",
       "      <td>-0.118406</td>\n",
       "      <td>-0.109982</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.145158</td>\n",
       "      <td>-0.022065</td>\n",
       "      <td>0.040331</td>\n",
       "      <td>0.142633</td>\n",
       "      <td>-0.028984</td>\n",
       "      <td>-0.143308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v204_nan</th>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.053279</td>\n",
       "      <td>0.046851</td>\n",
       "      <td>0.061015</td>\n",
       "      <td>0.075617</td>\n",
       "      <td>0.051670</td>\n",
       "      <td>-0.012713</td>\n",
       "      <td>0.143450</td>\n",
       "      <td>0.063949</td>\n",
       "      <td>0.059518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.289705</td>\n",
       "      <td>-0.323037</td>\n",
       "      <td>-0.300056</td>\n",
       "      <td>-0.145158</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.060197</td>\n",
       "      <td>-0.060197</td>\n",
       "      <td>-0.909306</td>\n",
       "      <td>-0.079076</td>\n",
       "      <td>0.987261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v172.1_n</th>\n",
       "      <td>-0.045414</td>\n",
       "      <td>0.018721</td>\n",
       "      <td>-0.035833</td>\n",
       "      <td>-0.023792</td>\n",
       "      <td>-0.008706</td>\n",
       "      <td>-0.023801</td>\n",
       "      <td>-0.013708</td>\n",
       "      <td>-0.024368</td>\n",
       "      <td>-0.018716</td>\n",
       "      <td>-0.074746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>-0.015467</td>\n",
       "      <td>0.059917</td>\n",
       "      <td>-0.022065</td>\n",
       "      <td>-0.060197</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.009150</td>\n",
       "      <td>-0.141990</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.059431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v172.1_p</th>\n",
       "      <td>-0.010143</td>\n",
       "      <td>0.039101</td>\n",
       "      <td>-0.040280</td>\n",
       "      <td>-0.031783</td>\n",
       "      <td>-0.048088</td>\n",
       "      <td>-0.032463</td>\n",
       "      <td>-0.061391</td>\n",
       "      <td>-0.015127</td>\n",
       "      <td>-0.016883</td>\n",
       "      <td>-0.059899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008062</td>\n",
       "      <td>0.051805</td>\n",
       "      <td>-0.045610</td>\n",
       "      <td>0.040331</td>\n",
       "      <td>-0.060197</td>\n",
       "      <td>-0.009150</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.141990</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.059431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v172.1_u</th>\n",
       "      <td>0.009589</td>\n",
       "      <td>-0.055188</td>\n",
       "      <td>-0.022865</td>\n",
       "      <td>-0.048033</td>\n",
       "      <td>-0.061443</td>\n",
       "      <td>-0.039713</td>\n",
       "      <td>0.019671</td>\n",
       "      <td>-0.134820</td>\n",
       "      <td>-0.055532</td>\n",
       "      <td>-0.015655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280704</td>\n",
       "      <td>0.290778</td>\n",
       "      <td>0.270852</td>\n",
       "      <td>0.142633</td>\n",
       "      <td>-0.909306</td>\n",
       "      <td>-0.141990</td>\n",
       "      <td>-0.141990</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.186519</td>\n",
       "      <td>-0.922211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v172.1_y</th>\n",
       "      <td>-0.032629</td>\n",
       "      <td>-0.057636</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>-0.000563</td>\n",
       "      <td>-0.012874</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.023207</td>\n",
       "      <td>0.006430</td>\n",
       "      <td>-0.007136</td>\n",
       "      <td>-0.058134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002714</td>\n",
       "      <td>0.038595</td>\n",
       "      <td>0.047903</td>\n",
       "      <td>-0.028984</td>\n",
       "      <td>-0.079076</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.012020</td>\n",
       "      <td>-0.186519</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.078068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v172.1_nan</th>\n",
       "      <td>0.010840</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.039053</td>\n",
       "      <td>0.061561</td>\n",
       "      <td>0.079078</td>\n",
       "      <td>0.052564</td>\n",
       "      <td>-0.010858</td>\n",
       "      <td>0.145945</td>\n",
       "      <td>0.066902</td>\n",
       "      <td>0.060699</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286015</td>\n",
       "      <td>-0.318922</td>\n",
       "      <td>-0.296234</td>\n",
       "      <td>-0.143308</td>\n",
       "      <td>0.987261</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.059431</td>\n",
       "      <td>-0.922211</td>\n",
       "      <td>-0.078068</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>71 rows  71 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    response      v001  \\\n",
       "response                                            1.000000  0.016594   \n",
       "v001                                                0.016594  1.000000   \n",
       "v002                                               -0.072353 -0.028172   \n",
       "v4                                                 -0.007668  0.026159   \n",
       "v5                                                 -0.013845  0.042749   \n",
       "v14                                                -0.017492  0.026423   \n",
       "v29                                                -0.172579 -0.042262   \n",
       "v120                                                0.051613  0.062640   \n",
       "v123                                                0.001951  0.065808   \n",
       "v173                                                0.042317 -0.023079   \n",
       "v174                                               -0.044762  0.023308   \n",
       "v175                                                0.016363  0.001833   \n",
       "v176                                               -0.019849 -0.002542   \n",
       "v177                                                0.048428 -0.002857   \n",
       "v180                                                0.022540  0.034855   \n",
       "v181                                                0.026332 -0.013670   \n",
       "v182                                                0.066963  0.027891   \n",
       "v183                                                0.021228  0.031512   \n",
       "v185                                                0.018609  0.032782   \n",
       "v191                                                0.194025 -0.026984   \n",
       "v192                                                0.194025 -0.026984   \n",
       "v196                                                0.034318 -0.003731   \n",
       "v197                                                0.034318 -0.003731   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.029653  0.030867   \n",
       "v178_certified                                     -0.024196  0.020057   \n",
       "v178_emailinexistent                                0.012306  0.005338   \n",
       "v178_validdomain                                    0.021854 -0.012438   \n",
       "v178_verified                                      -0.019354 -0.055177   \n",
       "v178_nan                                            0.010840  0.060728   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.022007 -0.053118   \n",
       "v193_nan                                            0.010840  0.060728   \n",
       "v194_yes                                           -0.010840 -0.060728   \n",
       "v194_nan                                            0.010840  0.060728   \n",
       "v195_low                                            0.034318 -0.003731   \n",
       "v195_moderate                                      -0.019232 -0.058975   \n",
       "v195_nan                                            0.010840  0.060728   \n",
       "v199_moderate                                      -0.010840 -0.060728   \n",
       "v199_nan                                            0.010840  0.060728   \n",
       "v201_moderate by proxy reputation and country code -0.006696 -0.056206   \n",
       "v201_moderate risk                                 -0.029653 -0.030867   \n",
       "v201_nan                                            0.010840  0.060728   \n",
       "v202_good                                          -0.010840 -0.060728   \n",
       "v202_nan                                            0.010840  0.060728   \n",
       "v203_no                                            -0.010840 -0.060728   \n",
       "v203_nan                                            0.010840  0.060728   \n",
       "v204_business                                       0.105443  0.028402   \n",
       "v204_cellular                                       0.097367 -0.031968   \n",
       "v204_college                                       -0.017098  0.010337   \n",
       "v204_dialup                                        -0.017098 -0.005009   \n",
       "v204_mobile                                        -0.024923  0.021698   \n",
       "v204_residential                                    0.077241  0.017588   \n",
       "v204_wifi                                          -0.148913 -0.037195   \n",
       "v204_wired                                         -0.002650 -0.082489   \n",
       "v204_nan                                            0.005322  0.053279   \n",
       "v172.1_n                                           -0.045414  0.018721   \n",
       "v172.1_p                                           -0.010143  0.039101   \n",
       "v172.1_u                                            0.009589 -0.055188   \n",
       "v172.1_y                                           -0.032629 -0.057636   \n",
       "v172.1_nan                                          0.010840  0.060728   \n",
       "\n",
       "                                                        v002        v4  \\\n",
       "response                                           -0.072353 -0.007668   \n",
       "v001                                               -0.028172  0.026159   \n",
       "v002                                                1.000000  0.161154   \n",
       "v4                                                  0.161154  1.000000   \n",
       "v5                                                  0.089772  0.738650   \n",
       "v14                                                 0.156599  0.985423   \n",
       "v29                                                 0.150948  0.302075   \n",
       "v120                                               -0.019944  0.209135   \n",
       "v123                                                0.015985  0.277026   \n",
       "v173                                                0.056365  0.010641   \n",
       "v174                                               -0.054430 -0.009915   \n",
       "v175                                                0.036745  0.022826   \n",
       "v176                                               -0.034321 -0.022698   \n",
       "v177                                               -0.021130  0.009787   \n",
       "v180                                                0.036069  0.018493   \n",
       "v181                                                0.046281  0.040124   \n",
       "v182                                                0.049610 -0.013755   \n",
       "v183                                                0.023708 -0.007962   \n",
       "v185                                                0.030687  0.016085   \n",
       "v191                                               -0.011798 -0.017876   \n",
       "v192                                               -0.011798 -0.017876   \n",
       "v196                                               -0.003506  0.004440   \n",
       "v197                                               -0.003506  0.004440   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.000895 -0.013900   \n",
       "v178_certified                                      0.001343 -0.007672   \n",
       "v178_emailinexistent                                0.012175 -0.018744   \n",
       "v178_validdomain                                    0.045308  0.043204   \n",
       "v178_verified                                      -0.059419 -0.074205   \n",
       "v178_nan                                            0.039053  0.061561   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.059391 -0.075180   \n",
       "v193_nan                                            0.039053  0.061561   \n",
       "v194_yes                                           -0.039053 -0.061561   \n",
       "v194_nan                                            0.039053  0.061561   \n",
       "v195_low                                           -0.003506  0.004440   \n",
       "v195_moderate                                      -0.037650 -0.061829   \n",
       "v195_nan                                            0.039053  0.061561   \n",
       "v199_moderate                                      -0.039053 -0.061561   \n",
       "v199_nan                                            0.039053  0.061561   \n",
       "v201_moderate by proxy reputation and country code -0.038764 -0.063222   \n",
       "v201_moderate risk                                 -0.000895  0.013900   \n",
       "v201_nan                                            0.039053  0.061561   \n",
       "v202_good                                          -0.039053 -0.061561   \n",
       "v202_nan                                            0.039053  0.061561   \n",
       "v203_no                                            -0.039053 -0.061561   \n",
       "v203_nan                                            0.039053  0.061561   \n",
       "v204_business                                       0.018046 -0.015789   \n",
       "v204_cellular                                      -0.139309 -0.051358   \n",
       "v204_college                                        0.093237 -0.019185   \n",
       "v204_dialup                                        -0.015165  0.020140   \n",
       "v204_mobile                                        -0.010204  0.016620   \n",
       "v204_residential                                   -0.090875 -0.061750   \n",
       "v204_wifi                                           0.104379  0.043348   \n",
       "v204_wired                                          0.067423 -0.044838   \n",
       "v204_nan                                            0.046851  0.061015   \n",
       "v172.1_n                                           -0.035833 -0.023792   \n",
       "v172.1_p                                           -0.040280 -0.031783   \n",
       "v172.1_u                                           -0.022865 -0.048033   \n",
       "v172.1_y                                            0.002458 -0.000563   \n",
       "v172.1_nan                                          0.039053  0.061561   \n",
       "\n",
       "                                                          v5       v14  \\\n",
       "response                                           -0.013845 -0.017492   \n",
       "v001                                                0.042749  0.026423   \n",
       "v002                                                0.089772  0.156599   \n",
       "v4                                                  0.738650  0.985423   \n",
       "v5                                                  1.000000  0.739335   \n",
       "v14                                                 0.739335  1.000000   \n",
       "v29                                                 0.312050  0.303795   \n",
       "v120                                                0.329635  0.225246   \n",
       "v123                                                0.383194  0.289810   \n",
       "v173                                                0.022785  0.016430   \n",
       "v174                                               -0.020723 -0.015724   \n",
       "v175                                                0.022109  0.020325   \n",
       "v176                                               -0.020502 -0.020099   \n",
       "v177                                                0.030602  0.006996   \n",
       "v180                                                0.031174  0.016046   \n",
       "v181                                                0.032608  0.042488   \n",
       "v182                                                0.037750 -0.017427   \n",
       "v183                                               -0.008904 -0.009189   \n",
       "v185                                                0.026678  0.013559   \n",
       "v191                                               -0.017656 -0.013643   \n",
       "v192                                               -0.017656 -0.013643   \n",
       "v196                                                0.008439  0.003290   \n",
       "v197                                                0.008439  0.003290   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.011399 -0.011734   \n",
       "v178_certified                                      0.023338 -0.007340   \n",
       "v178_emailinexistent                               -0.036280 -0.018398   \n",
       "v178_validdomain                                    0.042259  0.045596   \n",
       "v178_verified                                      -0.091117 -0.066806   \n",
       "v178_nan                                            0.079078  0.052564   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.088775 -0.067731   \n",
       "v193_nan                                            0.079078  0.052564   \n",
       "v194_yes                                           -0.079078 -0.052564   \n",
       "v194_nan                                            0.079078  0.052564   \n",
       "v195_low                                            0.008439  0.003290   \n",
       "v195_moderate                                      -0.080104 -0.052669   \n",
       "v195_nan                                            0.079078  0.052564   \n",
       "v199_moderate                                      -0.079078 -0.052564   \n",
       "v199_nan                                            0.079078  0.052564   \n",
       "v201_moderate by proxy reputation and country code -0.077169 -0.053964   \n",
       "v201_moderate risk                                 -0.011399  0.011734   \n",
       "v201_nan                                            0.079078  0.052564   \n",
       "v202_good                                          -0.079078 -0.052564   \n",
       "v202_nan                                            0.079078  0.052564   \n",
       "v203_no                                            -0.079078 -0.052564   \n",
       "v203_nan                                            0.079078  0.052564   \n",
       "v204_business                                       0.015715 -0.015172   \n",
       "v204_cellular                                      -0.044091 -0.052360   \n",
       "v204_college                                       -0.018105 -0.020001   \n",
       "v204_dialup                                         0.016491  0.021903   \n",
       "v204_mobile                                         0.017533  0.019737   \n",
       "v204_residential                                   -0.120228 -0.059451   \n",
       "v204_wifi                                           0.070998  0.048794   \n",
       "v204_wired                                         -0.036415 -0.043985   \n",
       "v204_nan                                            0.075617  0.051670   \n",
       "v172.1_n                                           -0.008706 -0.023801   \n",
       "v172.1_p                                           -0.048088 -0.032463   \n",
       "v172.1_u                                           -0.061443 -0.039713   \n",
       "v172.1_y                                           -0.012874  0.001402   \n",
       "v172.1_nan                                          0.079078  0.052564   \n",
       "\n",
       "                                                         v29      v120  \\\n",
       "response                                           -0.172579  0.051613   \n",
       "v001                                               -0.042262  0.062640   \n",
       "v002                                                0.150948 -0.019944   \n",
       "v4                                                  0.302075  0.209135   \n",
       "v5                                                  0.312050  0.329635   \n",
       "v14                                                 0.303795  0.225246   \n",
       "v29                                                 1.000000  0.157450   \n",
       "v120                                                0.157450  1.000000   \n",
       "v123                                                0.163382  0.733878   \n",
       "v173                                                0.058766  0.017526   \n",
       "v174                                               -0.056870 -0.016925   \n",
       "v175                                               -0.002560  0.011377   \n",
       "v176                                                0.005673 -0.012557   \n",
       "v177                                               -0.045187  0.001383   \n",
       "v180                                               -0.021656  0.013454   \n",
       "v181                                                0.034544 -0.011165   \n",
       "v182                                                0.010566  0.037434   \n",
       "v183                                               -0.039029 -0.008797   \n",
       "v185                                               -0.022860  0.010256   \n",
       "v191                                               -0.074641 -0.006081   \n",
       "v192                                               -0.074641 -0.006081   \n",
       "v196                                                0.002975 -0.014796   \n",
       "v197                                                0.002975 -0.014796   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.000951  0.007363   \n",
       "v178_certified                                     -0.024240 -0.011752   \n",
       "v178_emailinexistent                               -0.034325 -0.019692   \n",
       "v178_validdomain                                    0.038166 -0.009455   \n",
       "v178_verified                                       0.001096 -0.130327   \n",
       "v178_nan                                           -0.010858  0.145945   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.001522 -0.131854   \n",
       "v193_nan                                           -0.010858  0.145945   \n",
       "v194_yes                                            0.010858 -0.145945   \n",
       "v194_nan                                           -0.010858  0.145945   \n",
       "v195_low                                            0.002975 -0.014796   \n",
       "v195_moderate                                       0.009971 -0.140282   \n",
       "v195_nan                                           -0.010858  0.145945   \n",
       "v199_moderate                                       0.010858 -0.145945   \n",
       "v199_nan                                           -0.010858  0.145945   \n",
       "v201_moderate by proxy reputation and country code  0.010944 -0.144312   \n",
       "v201_moderate risk                                 -0.000951 -0.007363   \n",
       "v201_nan                                           -0.010858  0.145945   \n",
       "v202_good                                           0.010858 -0.145945   \n",
       "v202_nan                                           -0.010858  0.145945   \n",
       "v203_no                                             0.010858 -0.145945   \n",
       "v203_nan                                           -0.010858  0.145945   \n",
       "v204_business                                      -0.038260 -0.007491   \n",
       "v204_cellular                                      -0.120952 -0.033863   \n",
       "v204_college                                       -0.027601 -0.014393   \n",
       "v204_dialup                                        -0.022365 -0.014393   \n",
       "v204_mobile                                         0.033399 -0.030635   \n",
       "v204_residential                                   -0.048993 -0.050454   \n",
       "v204_wifi                                           0.101896 -0.033850   \n",
       "v204_wired                                          0.056682 -0.037192   \n",
       "v204_nan                                           -0.012713  0.143450   \n",
       "v172.1_n                                           -0.013708 -0.024368   \n",
       "v172.1_p                                           -0.061391 -0.015127   \n",
       "v172.1_u                                            0.019671 -0.134820   \n",
       "v172.1_y                                            0.023207  0.006430   \n",
       "v172.1_nan                                         -0.010858  0.145945   \n",
       "\n",
       "                                                        v123      v173  \\\n",
       "response                                            0.001951  0.042317   \n",
       "v001                                                0.065808 -0.023079   \n",
       "v002                                                0.015985  0.056365   \n",
       "v4                                                  0.277026  0.010641   \n",
       "v5                                                  0.383194  0.022785   \n",
       "v14                                                 0.289810  0.016430   \n",
       "v29                                                 0.163382  0.058766   \n",
       "v120                                                0.733878  0.017526   \n",
       "v123                                                1.000000  0.051716   \n",
       "v173                                                0.051716  1.000000   \n",
       "v174                                               -0.050899 -0.999832   \n",
       "v175                                                0.010423  0.096303   \n",
       "v176                                               -0.010168 -0.094509   \n",
       "v177                                               -0.001916 -0.069621   \n",
       "v180                                               -0.003963  0.065332   \n",
       "v181                                               -0.008010  0.096625   \n",
       "v182                                                0.028961  0.514751   \n",
       "v183                                               -0.017028  0.052509   \n",
       "v185                                               -0.006240  0.057240   \n",
       "v191                                                0.005703 -0.004563   \n",
       "v192                                                0.005703 -0.004563   \n",
       "v196                                               -0.011881  0.229137   \n",
       "v197                                               -0.011881  0.229137   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.009828  0.008125   \n",
       "v178_certified                                     -0.014852 -0.085115   \n",
       "v178_emailinexistent                               -0.013774  0.039195   \n",
       "v178_validdomain                                   -0.007618  0.081651   \n",
       "v178_verified                                      -0.056529 -0.091060   \n",
       "v178_nan                                            0.066902  0.060699   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.058246 -0.100439   \n",
       "v193_nan                                            0.066902  0.060699   \n",
       "v194_yes                                           -0.066902 -0.060699   \n",
       "v194_nan                                            0.066902  0.060699   \n",
       "v195_low                                           -0.011881  0.229137   \n",
       "v195_moderate                                      -0.063038 -0.116889   \n",
       "v195_nan                                            0.066902  0.060699   \n",
       "v199_moderate                                      -0.066902 -0.060699   \n",
       "v199_nan                                            0.066902  0.060699   \n",
       "v201_moderate by proxy reputation and country code -0.065262 -0.059319   \n",
       "v201_moderate risk                                 -0.009828 -0.008125   \n",
       "v201_nan                                            0.066902  0.060699   \n",
       "v202_good                                          -0.066902 -0.060699   \n",
       "v202_nan                                            0.066902  0.060699   \n",
       "v203_no                                            -0.066902 -0.060699   \n",
       "v203_nan                                            0.066902  0.060699   \n",
       "v204_business                                      -0.013774  0.037171   \n",
       "v204_cellular                                      -0.031032 -0.070660   \n",
       "v204_college                                       -0.005667  0.003519   \n",
       "v204_dialup                                        -0.005667  0.055606   \n",
       "v204_mobile                                         0.003460  0.017673   \n",
       "v204_residential                                   -0.049136 -0.099213   \n",
       "v204_wifi                                           0.017068  0.031243   \n",
       "v204_wired                                         -0.029950  0.045614   \n",
       "v204_nan                                            0.063949  0.059518   \n",
       "v172.1_n                                           -0.018716 -0.074746   \n",
       "v172.1_p                                           -0.016883 -0.059899   \n",
       "v172.1_u                                           -0.055532 -0.015655   \n",
       "v172.1_y                                           -0.007136 -0.058134   \n",
       "v172.1_nan                                          0.066902  0.060699   \n",
       "\n",
       "                                                       ...      v204_mobile  \\\n",
       "response                                               ...        -0.024923   \n",
       "v001                                                   ...         0.021698   \n",
       "v002                                                   ...        -0.010204   \n",
       "v4                                                     ...         0.016620   \n",
       "v5                                                     ...         0.017533   \n",
       "v14                                                    ...         0.019737   \n",
       "v29                                                    ...         0.033399   \n",
       "v120                                                   ...        -0.030635   \n",
       "v123                                                   ...         0.003460   \n",
       "v173                                                   ...         0.017673   \n",
       "v174                                                   ...        -0.011385   \n",
       "v175                                                   ...         0.009334   \n",
       "v176                                                   ...         0.004150   \n",
       "v177                                                   ...        -0.040159   \n",
       "v180                                                   ...        -0.023473   \n",
       "v181                                                   ...         0.103508   \n",
       "v182                                                   ...        -0.117849   \n",
       "v183                                                   ...         0.023692   \n",
       "v185                                                   ...        -0.023946   \n",
       "v191                                                   ...         0.001890   \n",
       "v192                                                   ...         0.001890   \n",
       "v196                                                   ...         0.037737   \n",
       "v197                                                   ...         0.037737   \n",
       "v198                                                   ...              NaN   \n",
       "v200                                                   ...         0.028754   \n",
       "v178_certified                                         ...        -0.023462   \n",
       "v178_emailinexistent                                   ...         0.061771   \n",
       "v178_validdomain                                       ...         0.092262   \n",
       "v178_verified                                          ...         0.223900   \n",
       "v178_nan                                               ...        -0.286015   \n",
       "...                                                    ...              ...   \n",
       "v193_yes                                               ...         0.221806   \n",
       "v193_nan                                               ...        -0.286015   \n",
       "v194_yes                                               ...         0.286015   \n",
       "v194_nan                                               ...        -0.286015   \n",
       "v195_low                                               ...         0.037737   \n",
       "v195_moderate                                          ...         0.272741   \n",
       "v195_nan                                               ...        -0.286015   \n",
       "v199_moderate                                          ...         0.286015   \n",
       "v199_nan                                               ...        -0.286015   \n",
       "v201_moderate by proxy reputation and country code     ...         0.288782   \n",
       "v201_moderate risk                                     ...        -0.028754   \n",
       "v201_nan                                               ...        -0.286015   \n",
       "v202_good                                              ...         0.286015   \n",
       "v202_nan                                               ...        -0.286015   \n",
       "v203_no                                                ...         0.286015   \n",
       "v203_nan                                               ...        -0.286015   \n",
       "v204_business                                          ...        -0.033224   \n",
       "v204_cellular                                          ...        -0.144226   \n",
       "v204_college                                           ...        -0.016579   \n",
       "v204_dialup                                            ...        -0.016579   \n",
       "v204_mobile                                            ...         1.000000   \n",
       "v204_residential                                       ...        -0.236314   \n",
       "v204_wifi                                              ...        -0.219503   \n",
       "v204_wired                                             ...        -0.106188   \n",
       "v204_nan                                               ...        -0.289705   \n",
       "v172.1_n                                               ...        -0.008062   \n",
       "v172.1_p                                               ...        -0.008062   \n",
       "v172.1_u                                               ...         0.280704   \n",
       "v172.1_y                                               ...        -0.002714   \n",
       "v172.1_nan                                             ...        -0.286015   \n",
       "\n",
       "                                                    v204_residential  \\\n",
       "response                                                    0.077241   \n",
       "v001                                                        0.017588   \n",
       "v002                                                       -0.090875   \n",
       "v4                                                         -0.061750   \n",
       "v5                                                         -0.120228   \n",
       "v14                                                        -0.059451   \n",
       "v29                                                        -0.048993   \n",
       "v120                                                       -0.050454   \n",
       "v123                                                       -0.049136   \n",
       "v173                                                       -0.099213   \n",
       "v174                                                        0.086854   \n",
       "v175                                                       -0.098594   \n",
       "v176                                                        0.084703   \n",
       "v177                                                       -0.023157   \n",
       "v180                                                       -0.132832   \n",
       "v181                                                       -0.010617   \n",
       "v182                                                       -0.152248   \n",
       "v183                                                       -0.101339   \n",
       "v185                                                       -0.124233   \n",
       "v191                                                        0.070954   \n",
       "v192                                                        0.070954   \n",
       "v196                                                       -0.002411   \n",
       "v197                                                       -0.002411   \n",
       "v198                                                             NaN   \n",
       "v200                                                        0.032062   \n",
       "v178_certified                                             -0.026161   \n",
       "v178_emailinexistent                                       -0.037046   \n",
       "v178_validdomain                                           -0.007678   \n",
       "v178_verified                                               0.315263   \n",
       "v178_nan                                                   -0.318922   \n",
       "...                                                              ...   \n",
       "v193_yes                                                    0.313057   \n",
       "v193_nan                                                   -0.318922   \n",
       "v194_yes                                                    0.318922   \n",
       "v194_nan                                                   -0.318922   \n",
       "v195_low                                                   -0.002411   \n",
       "v195_moderate                                               0.315191   \n",
       "v195_nan                                                   -0.318922   \n",
       "v199_moderate                                               0.318922   \n",
       "v199_nan                                                   -0.318922   \n",
       "v201_moderate by proxy reputation and country code          0.322007   \n",
       "v201_moderate risk                                         -0.032062   \n",
       "v201_nan                                                   -0.318922   \n",
       "v202_good                                                   0.318922   \n",
       "v202_nan                                                   -0.318922   \n",
       "v203_no                                                     0.318922   \n",
       "v203_nan                                                   -0.318922   \n",
       "v204_business                                              -0.037046   \n",
       "v204_cellular                                              -0.160820   \n",
       "v204_college                                               -0.018487   \n",
       "v204_dialup                                                -0.018487   \n",
       "v204_mobile                                                -0.236314   \n",
       "v204_residential                                            1.000000   \n",
       "v204_wifi                                                  -0.244757   \n",
       "v204_wired                                                 -0.118406   \n",
       "v204_nan                                                   -0.323037   \n",
       "v172.1_n                                                   -0.015467   \n",
       "v172.1_p                                                    0.051805   \n",
       "v172.1_u                                                    0.290778   \n",
       "v172.1_y                                                    0.038595   \n",
       "v172.1_nan                                                 -0.318922   \n",
       "\n",
       "                                                    v204_wifi  v204_wired  \\\n",
       "response                                            -0.148913   -0.002650   \n",
       "v001                                                -0.037195   -0.082489   \n",
       "v002                                                 0.104379    0.067423   \n",
       "v4                                                   0.043348   -0.044838   \n",
       "v5                                                   0.070998   -0.036415   \n",
       "v14                                                  0.048794   -0.043985   \n",
       "v29                                                  0.101896    0.056682   \n",
       "v120                                                -0.033850   -0.037192   \n",
       "v123                                                 0.017068   -0.029950   \n",
       "v173                                                 0.031243    0.045614   \n",
       "v174                                                -0.025317   -0.042905   \n",
       "v175                                                -0.037143   -0.022360   \n",
       "v176                                                 0.050265    0.028594   \n",
       "v177                                                -0.060555   -0.029300   \n",
       "v180                                                -0.054981   -0.029743   \n",
       "v181                                                 0.010018    0.099824   \n",
       "v182                                                -0.138372   -0.071711   \n",
       "v183                                                -0.016571    0.034264   \n",
       "v185                                                -0.055139   -0.017898   \n",
       "v191                                                 0.032342    0.018522   \n",
       "v192                                                 0.032342    0.018522   \n",
       "v196                                                 0.063331   -0.026424   \n",
       "v197                                                 0.063331   -0.026424   \n",
       "v198                                                      NaN         NaN   \n",
       "v200                                                 0.029781    0.014407   \n",
       "v178_certified                                      -0.024300   -0.011756   \n",
       "v178_emailinexistent                                 0.012032    0.065733   \n",
       "v178_validdomain                                     0.005244    0.089239   \n",
       "v178_verified                                        0.280222    0.087643   \n",
       "v178_nan                                            -0.296234   -0.143308   \n",
       "...                                                       ...         ...   \n",
       "v193_yes                                             0.278148    0.086545   \n",
       "v193_nan                                            -0.296234   -0.143308   \n",
       "v194_yes                                             0.296234    0.143308   \n",
       "v194_nan                                            -0.296234   -0.143308   \n",
       "v195_low                                             0.063331   -0.026424   \n",
       "v195_moderate                                        0.276453    0.147937   \n",
       "v195_nan                                            -0.296234   -0.143308   \n",
       "v199_moderate                                        0.296234    0.143308   \n",
       "v199_nan                                            -0.296234   -0.143308   \n",
       "v201_moderate by proxy reputation and country code   0.299100    0.144695   \n",
       "v201_moderate risk                                  -0.029781   -0.014407   \n",
       "v201_nan                                            -0.296234   -0.143308   \n",
       "v202_good                                            0.296234    0.143308   \n",
       "v202_nan                                            -0.296234   -0.143308   \n",
       "v203_no                                              0.296234    0.143308   \n",
       "v203_nan                                            -0.296234   -0.143308   \n",
       "v204_business                                       -0.034411   -0.016647   \n",
       "v204_cellular                                       -0.149379   -0.072265   \n",
       "v204_college                                        -0.017172   -0.008307   \n",
       "v204_dialup                                         -0.017172   -0.008307   \n",
       "v204_mobile                                         -0.219503   -0.106188   \n",
       "v204_residential                                    -0.244757   -0.118406   \n",
       "v204_wifi                                            1.000000   -0.109982   \n",
       "v204_wired                                          -0.109982    1.000000   \n",
       "v204_nan                                            -0.300056   -0.145158   \n",
       "v172.1_n                                             0.059917   -0.022065   \n",
       "v172.1_p                                            -0.045610    0.040331   \n",
       "v172.1_u                                             0.270852    0.142633   \n",
       "v172.1_y                                             0.047903   -0.028984   \n",
       "v172.1_nan                                          -0.296234   -0.143308   \n",
       "\n",
       "                                                    v204_nan  v172.1_n  \\\n",
       "response                                            0.005322 -0.045414   \n",
       "v001                                                0.053279  0.018721   \n",
       "v002                                                0.046851 -0.035833   \n",
       "v4                                                  0.061015 -0.023792   \n",
       "v5                                                  0.075617 -0.008706   \n",
       "v14                                                 0.051670 -0.023801   \n",
       "v29                                                -0.012713 -0.013708   \n",
       "v120                                                0.143450 -0.024368   \n",
       "v123                                                0.063949 -0.018716   \n",
       "v173                                                0.059518 -0.074746   \n",
       "v174                                               -0.055338  0.074804   \n",
       "v175                                                0.165227 -0.215025   \n",
       "v176                                               -0.173165  0.215696   \n",
       "v177                                                0.121614 -0.150750   \n",
       "v180                                                0.227068 -0.170986   \n",
       "v181                                               -0.140203 -0.053073   \n",
       "v182                                                0.435554 -0.054048   \n",
       "v183                                                0.093479 -0.075338   \n",
       "v185                                                0.214553 -0.162560   \n",
       "v191                                               -0.122082 -0.018557   \n",
       "v192                                               -0.122082 -0.018557   \n",
       "v196                                               -0.072091 -0.010958   \n",
       "v197                                               -0.072091 -0.010958   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                               -0.099252  0.005975   \n",
       "v178_certified                                     -0.032072  0.263955   \n",
       "v178_emailinexistent                               -0.045416 -0.006903   \n",
       "v178_validdomain                                   -0.139178 -0.021156   \n",
       "v178_verified                                      -0.866443  0.038576   \n",
       "v178_nan                                            0.987261 -0.059431   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                           -0.871616  0.067180   \n",
       "v193_nan                                            0.987261 -0.059431   \n",
       "v194_yes                                           -0.987261  0.059431   \n",
       "v194_nan                                            0.987261 -0.059431   \n",
       "v195_low                                           -0.072091 -0.010958   \n",
       "v195_moderate                                      -0.955916  0.061350   \n",
       "v195_nan                                            0.987261 -0.059431   \n",
       "v199_moderate                                      -0.987261  0.059431   \n",
       "v199_nan                                            0.987261 -0.059431   \n",
       "v201_moderate by proxy reputation and country code -0.996813  0.060006   \n",
       "v201_moderate risk                                  0.099252 -0.005975   \n",
       "v201_nan                                            0.987261 -0.059431   \n",
       "v202_good                                          -0.987261  0.059431   \n",
       "v202_nan                                            0.987261 -0.059431   \n",
       "v203_no                                            -0.987261  0.059431   \n",
       "v203_nan                                            0.987261 -0.059431   \n",
       "v204_business                                      -0.045416 -0.006903   \n",
       "v204_cellular                                      -0.197154  0.065831   \n",
       "v204_college                                       -0.022664 -0.003445   \n",
       "v204_dialup                                        -0.022664 -0.003445   \n",
       "v204_mobile                                        -0.289705 -0.008062   \n",
       "v204_residential                                   -0.323037 -0.015467   \n",
       "v204_wifi                                          -0.300056  0.059917   \n",
       "v204_wired                                         -0.145158 -0.022065   \n",
       "v204_nan                                            1.000000 -0.060197   \n",
       "v172.1_n                                           -0.060197  1.000000   \n",
       "v172.1_p                                           -0.060197 -0.009150   \n",
       "v172.1_u                                           -0.909306 -0.141990   \n",
       "v172.1_y                                           -0.079076 -0.012020   \n",
       "v172.1_nan                                          0.987261 -0.059431   \n",
       "\n",
       "                                                    v172.1_p  v172.1_u  \\\n",
       "response                                           -0.010143  0.009589   \n",
       "v001                                                0.039101 -0.055188   \n",
       "v002                                               -0.040280 -0.022865   \n",
       "v4                                                 -0.031783 -0.048033   \n",
       "v5                                                 -0.048088 -0.061443   \n",
       "v14                                                -0.032463 -0.039713   \n",
       "v29                                                -0.061391  0.019671   \n",
       "v120                                               -0.015127 -0.134820   \n",
       "v123                                               -0.016883 -0.055532   \n",
       "v173                                               -0.059899 -0.015655   \n",
       "v174                                                0.058405  0.011945   \n",
       "v175                                               -0.271529  0.040504   \n",
       "v176                                                0.269962 -0.032460   \n",
       "v177                                                0.016069 -0.030689   \n",
       "v180                                               -0.175666 -0.053170   \n",
       "v181                                               -0.053073  0.163087   \n",
       "v182                                               -0.085544 -0.364803   \n",
       "v183                                               -0.075338 -0.017642   \n",
       "v185                                               -0.190442 -0.041469   \n",
       "v191                                               -0.018557  0.130693   \n",
       "v192                                               -0.018557  0.130693   \n",
       "v196                                               -0.010958  0.077176   \n",
       "v197                                               -0.010958  0.077176   \n",
       "v198                                                     NaN       NaN   \n",
       "v200                                                0.005975 -0.042078   \n",
       "v178_certified                                      0.263955 -0.075650   \n",
       "v178_emailinexistent                               -0.006903  0.048620   \n",
       "v178_validdomain                                   -0.021156  0.148996   \n",
       "v178_verified                                       0.038576  0.811306   \n",
       "v178_nan                                           -0.059431 -0.922211   \n",
       "...                                                      ...       ...   \n",
       "v193_yes                                            0.067180  0.804728   \n",
       "v193_nan                                           -0.059431 -0.922211   \n",
       "v194_yes                                            0.059431  0.922211   \n",
       "v194_nan                                           -0.059431 -0.922211   \n",
       "v195_low                                           -0.010958  0.077176   \n",
       "v195_moderate                                       0.061350  0.890484   \n",
       "v195_nan                                           -0.059431 -0.922211   \n",
       "v199_moderate                                       0.059431  0.922211   \n",
       "v199_nan                                           -0.059431 -0.922211   \n",
       "v201_moderate by proxy reputation and country code  0.060006  0.912504   \n",
       "v201_moderate risk                                 -0.005975  0.042078   \n",
       "v201_nan                                           -0.059431 -0.922211   \n",
       "v202_good                                           0.059431  0.922211   \n",
       "v202_nan                                           -0.059431 -0.922211   \n",
       "v203_no                                             0.059431  0.922211   \n",
       "v203_nan                                           -0.059431 -0.922211   \n",
       "v204_business                                      -0.006903  0.048620   \n",
       "v204_cellular                                       0.065831  0.152270   \n",
       "v204_college                                       -0.003445  0.024262   \n",
       "v204_dialup                                        -0.003445  0.024262   \n",
       "v204_mobile                                        -0.008062  0.280704   \n",
       "v204_residential                                    0.051805  0.290778   \n",
       "v204_wifi                                          -0.045610  0.270852   \n",
       "v204_wired                                          0.040331  0.142633   \n",
       "v204_nan                                           -0.060197 -0.909306   \n",
       "v172.1_n                                           -0.009150 -0.141990   \n",
       "v172.1_p                                            1.000000 -0.141990   \n",
       "v172.1_u                                           -0.141990  1.000000   \n",
       "v172.1_y                                           -0.012020 -0.186519   \n",
       "v172.1_nan                                         -0.059431 -0.922211   \n",
       "\n",
       "                                                    v172.1_y  v172.1_nan  \n",
       "response                                           -0.032629    0.010840  \n",
       "v001                                               -0.057636    0.060728  \n",
       "v002                                                0.002458    0.039053  \n",
       "v4                                                 -0.000563    0.061561  \n",
       "v5                                                 -0.012874    0.079078  \n",
       "v14                                                 0.001402    0.052564  \n",
       "v29                                                 0.023207   -0.010858  \n",
       "v120                                                0.006430    0.145945  \n",
       "v123                                               -0.007136    0.066902  \n",
       "v173                                               -0.058134    0.060699  \n",
       "v174                                                0.057632   -0.056421  \n",
       "v175                                               -0.369961    0.163113  \n",
       "v176                                                0.369446   -0.171096  \n",
       "v177                                               -0.217245    0.120156  \n",
       "v180                                               -0.348206    0.224360  \n",
       "v181                                               -0.027995   -0.138417  \n",
       "v182                                               -0.108406    0.436544  \n",
       "v183                                               -0.152879    0.092288  \n",
       "v185                                               -0.341731    0.211820  \n",
       "v191                                               -0.024377   -0.120527  \n",
       "v192                                               -0.024377   -0.120527  \n",
       "v196                                               -0.014395   -0.071173  \n",
       "v197                                               -0.014395   -0.071173  \n",
       "v198                                                     NaN         NaN  \n",
       "v200                                                0.007848    0.038805  \n",
       "v178_certified                                     -0.006404   -0.031664  \n",
       "v178_emailinexistent                               -0.009068   -0.044837  \n",
       "v178_validdomain                                   -0.027790   -0.137405  \n",
       "v178_verified                                       0.088766   -0.879486  \n",
       "v178_nan                                           -0.078068    1.000000  \n",
       "...                                                      ...         ...  \n",
       "v193_yes                                            0.088249   -0.884640  \n",
       "v193_nan                                           -0.078068    1.000000  \n",
       "v194_yes                                            0.078068   -1.000000  \n",
       "v194_nan                                           -0.078068    1.000000  \n",
       "v195_low                                           -0.014395   -0.071173  \n",
       "v195_moderate                                       0.080590   -0.968711  \n",
       "v195_nan                                           -0.078068    1.000000  \n",
       "v199_moderate                                       0.078068   -1.000000  \n",
       "v199_nan                                           -0.078068    1.000000  \n",
       "v201_moderate by proxy reputation and country code  0.078824   -0.990417  \n",
       "v201_moderate risk                                 -0.007848   -0.038805  \n",
       "v201_nan                                           -0.078068    1.000000  \n",
       "v202_good                                           0.078068   -1.000000  \n",
       "v202_nan                                           -0.078068    1.000000  \n",
       "v203_no                                             0.078068   -1.000000  \n",
       "v203_nan                                           -0.078068    1.000000  \n",
       "v204_business                                      -0.009068   -0.044837  \n",
       "v204_cellular                                       0.034042   -0.194643  \n",
       "v204_college                                       -0.004525   -0.022375  \n",
       "v204_dialup                                        -0.004525   -0.022375  \n",
       "v204_mobile                                        -0.002714   -0.286015  \n",
       "v204_residential                                    0.038595   -0.318922  \n",
       "v204_wifi                                           0.047903   -0.296234  \n",
       "v204_wired                                         -0.028984   -0.143308  \n",
       "v204_nan                                           -0.079076    0.987261  \n",
       "v172.1_n                                           -0.012020   -0.059431  \n",
       "v172.1_p                                           -0.012020   -0.059431  \n",
       "v172.1_u                                           -0.186519   -0.922211  \n",
       "v172.1_y                                            1.000000   -0.078068  \n",
       "v172.1_nan                                         -0.078068    1.000000  \n",
       "\n",
       "[71 rows x 71 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix = dataset_normalized.corr()\n",
    "corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting a list (series) of correlations between the target and all features. I am interested in magnitude, so I will take absoulute values. Then I will sort them in descending order. Items in the top of the list are likely to be more important factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "response                                              1.000000\n",
       "v192                                                  0.194025\n",
       "v191                                                  0.194025\n",
       "v29                                                   0.172579\n",
       "v204_wifi                                             0.148913\n",
       "v204_business                                         0.105443\n",
       "v204_cellular                                         0.097367\n",
       "v204_residential                                      0.077241\n",
       "v002                                                  0.072353\n",
       "v182                                                  0.066963\n",
       "v120                                                  0.051613\n",
       "v177                                                  0.048428\n",
       "v172.1_n                                              0.045414\n",
       "v174                                                  0.044762\n",
       "v173                                                  0.042317\n",
       "v195_low                                              0.034318\n",
       "v197                                                  0.034318\n",
       "v196                                                  0.034318\n",
       "v172.1_y                                              0.032629\n",
       "v201_moderate risk                                    0.029653\n",
       "v200                                                  0.029653\n",
       "v181                                                  0.026332\n",
       "v204_mobile                                           0.024923\n",
       "v178_certified                                        0.024196\n",
       "v180                                                  0.022540\n",
       "v193_yes                                              0.022007\n",
       "v178_validdomain                                      0.021854\n",
       "v193_not sure                                         0.021854\n",
       "v183                                                  0.021228\n",
       "v184_lower fraud risk                                 0.020346\n",
       "                                                        ...   \n",
       "v184_data entry review                                0.012306\n",
       "v186_fraud score 601 to 799                           0.012306\n",
       "v193_no                                               0.012306\n",
       "v178_emailinexistent                                  0.012306\n",
       "v203_nan                                              0.010840\n",
       "v203_no                                               0.010840\n",
       "v202_nan                                              0.010840\n",
       "v202_good                                             0.010840\n",
       "v201_nan                                              0.010840\n",
       "v186_nan                                              0.010840\n",
       "v172.1_nan                                            0.010840\n",
       "v184_nan                                              0.010840\n",
       "v199_moderate                                         0.010840\n",
       "v195_nan                                              0.010840\n",
       "v194_nan                                              0.010840\n",
       "v194_yes                                              0.010840\n",
       "v178_nan                                              0.010840\n",
       "v193_nan                                              0.010840\n",
       "v199_nan                                              0.010840\n",
       "v172.1_p                                              0.010143\n",
       "v172.1_u                                              0.009589\n",
       "v4                                                    0.007668\n",
       "v186_fraud score 1 to 100                             0.007036\n",
       "v201_moderate by proxy reputation and country code    0.006696\n",
       "v204_nan                                              0.005322\n",
       "v204_wired                                            0.002650\n",
       "v184_moderate fraud risk                              0.002095\n",
       "v186_fraud score 301 to 600                           0.002095\n",
       "v123                                                  0.001951\n",
       "v198                                                       NaN\n",
       "Name: response, Length: 71, dtype: float64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_with_target_abs_desc = corr_matrix[target].apply(np.abs).sort_values(ascending=False)\n",
    "corr_with_target_abs_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that many features have the same correlation with the target (e.g. v191 and v192). Thus, I can exclude some redundant features (e.g. dimensionality reduction). \n",
    "Below I define a function that will exclude redundant features and return a list of 'unique' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def exclude_similar_features_and_get_unique(corr_with_target_series, similarity_param = 0.000001):\n",
    "    exclusion_boolean_list = [True] # first item not to be excluded\n",
    "    for i in range(1, len(corr_with_target_series)):\n",
    "        if np.isnan(corr_with_target_series[i]):\n",
    "            exclusion_boolean_list.append(False)\n",
    "        elif (corr_with_target_series[i-1] - corr_with_target_series[i])<=similarity_param:\n",
    "            exclusion_boolean_list.append(False)\n",
    "        else:\n",
    "            exclusion_boolean_list.append(True)\n",
    "    unique_features = list(corr_with_target_series[exclusion_boolean_list].index)\n",
    "    return unique_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_features = exclude_similar_features_and_get_unique(corr_with_target_abs_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['response',\n",
       " 'v192',\n",
       " 'v29',\n",
       " 'v204_wifi',\n",
       " 'v204_business',\n",
       " 'v204_cellular',\n",
       " 'v204_residential',\n",
       " 'v002',\n",
       " 'v182',\n",
       " 'v120',\n",
       " 'v177',\n",
       " 'v172.1_n',\n",
       " 'v174',\n",
       " 'v173',\n",
       " 'v195_low',\n",
       " 'v172.1_y',\n",
       " 'v201_moderate risk',\n",
       " 'v181',\n",
       " 'v204_mobile',\n",
       " 'v178_certified',\n",
       " 'v180',\n",
       " 'v193_yes',\n",
       " 'v178_validdomain',\n",
       " 'v183',\n",
       " 'v184_lower fraud risk',\n",
       " 'v176',\n",
       " 'v186_fraud score 101 to 300',\n",
       " 'v178_verified',\n",
       " 'v195_moderate',\n",
       " 'v185',\n",
       " 'v14',\n",
       " 'v204_dialup',\n",
       " 'v001',\n",
       " 'v175',\n",
       " 'v5',\n",
       " 'v184_data entry review',\n",
       " 'v203_nan',\n",
       " 'v172.1_p',\n",
       " 'v172.1_u',\n",
       " 'v4',\n",
       " 'v186_fraud score 1 to 100',\n",
       " 'v201_moderate by proxy reputation and country code',\n",
       " 'v204_nan',\n",
       " 'v204_wired',\n",
       " 'v184_moderate fraud risk',\n",
       " 'v123']"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will create a new dataset that will contain only the target and 'unique' features.\n",
    "\n",
    "_Note: in fact I have tried to run the logistic regression both with redundant features and without. The results are the same. So it is mainly about computational efficiency._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_normalized_unique_features=dataset_normalized[unique_features] # - use this to keep only unique features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_3) Trying the model with basic settings_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the strongest predictors logistic regression is a good ML-algorithm. I choose Regression, because it will expclicitly show which factors have more impact on the target and which less. I will use L2 and L1 regularization that allows to deal with collinearity and overfitting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) I will split the data to the train and test sets. Training set will be used to train and calibrate the model. Test set is used to assess the final model. Test set is 20% of the data and train set - 80%. Random state is set to 1, so that the data is split in the same manner every time I run the split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(dataset_normalized_unique_features, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Creating a model. C (L2 regularization parameter) is set to 1, but will be calibrated later. I will fit intercept to have less biased coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(penalty='l2', C=1, fit_intercept=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) getting predictor names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get columns names except the target\n",
    "predictors = get_col_names_without_target(dataset_normalized_unique_features, target='response')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###\n",
    "len(predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) I use cross validation to see what is the accuracy of the model with basic settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here I define a function to conveniently do cross-validation and print the results\n",
    "def print_scores(model, X_features, Y_target):\n",
    "    scores = cross_val_score(model, X_features, Y_target, cv=5)\n",
    "    print('accuracies =', scores)\n",
    "    print('mean accuracy =', scores.mean())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.80645161  0.83870968  0.76612903  0.80487805  0.83606557]\n",
      "mean accuracy = 0.810446789026\n"
     ]
    }
   ],
   "source": [
    "print_scores(log_reg, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) It is good to compare performance of my model with majority class prediction model. This is the minimum accuracy that my model should achive. Majority class naive prediction is when you predict that every record belongs to a majority class. In this case it would give the following accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.807131280388979"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-train[target].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516129032258064"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-test[target].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My model gives essentially the same results, so it is not a very bad model, but  also not a good one. Thus, need to calibrate it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_3) Calibrating the model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) we have an inbalanced dataset - 80.7% of data has response value of 0 and 19.3% - 1. I will try to use 'balanced' class_weight to tackle that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_balanced = LogisticRegression(penalty='l2', C=1, fit_intercept=True, random_state=1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.62903226  0.62096774  0.62903226  0.64227642  0.62295082]\n",
      "mean accuracy = 0.6288519001\n"
     ]
    }
   ],
   "source": [
    "print_scores(log_reg_balanced, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is actually much worse, so no need to use class_weight functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) trying Logistic Regression with L1 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg_l1 = LogisticRegression(penalty='l1', C=1, fit_intercept=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.78225806  0.83870968  0.78225806  0.80487805  0.8442623 ]\n",
      "mean accuracy = 0.810473230063\n"
     ]
    }
   ],
   "source": [
    "print_scores(log_reg_l1, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log.reg. model with L1 regularization give better rezults than the majority class prediction model. It also outperforms log.reg. with L2  regularization. So, I will stick to that model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Calibration regularization parameter C. Firstly, I try a wide band of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c param = 0.01\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "c param = 0.1\n",
      "accuracies = [ 0.80645161  0.81451613  0.80645161  0.81300813  0.83606557]\n",
      "mean accuracy = 0.815298611738\n",
      "\n",
      "c param = 0.5\n",
      "accuracies = [ 0.79032258  0.83870968  0.78225806  0.78861789  0.8442623 ]\n",
      "mean accuracy = 0.808834100768\n",
      "\n",
      "c param = 1\n",
      "accuracies = [ 0.78225806  0.83870968  0.78225806  0.80487805  0.8442623 ]\n",
      "mean accuracy = 0.810473230063\n",
      "\n",
      "c param = 2\n",
      "accuracies = [ 0.79032258  0.83870968  0.78225806  0.80487805  0.83606557]\n",
      "mean accuracy = 0.810446789026\n",
      "\n",
      "c param = 5\n",
      "accuracies = [ 0.7983871   0.83870968  0.77419355  0.79674797  0.83606557]\n",
      "mean accuracy = 0.808820772766\n",
      "\n",
      "c param = 15\n",
      "accuracies = [ 0.81451613  0.83870968  0.76612903  0.79674797  0.83606557]\n",
      "mean accuracy = 0.810433675992\n",
      "\n",
      "c param = 100\n",
      "accuracies = [ 0.81451613  0.83870968  0.76612903  0.78861789  0.81967213]\n",
      "mean accuracy = 0.805528971207\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for C_param in [0.01, 0.1, 0.5, 1, 2, 5, 15, 100]:\n",
    "    log_reg_l1_diff_c = LogisticRegression(penalty='l1', C=C_param, fit_intercept=True, random_state=1)\n",
    "    print('c param =', C_param)\n",
    "    print_scores(log_reg_l1_diff_c, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal C parameter is somewhere between 0.1 - 1, so I will narrow down the search in this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c param = 0.05\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "c param = 0.1\n",
      "accuracies = [ 0.80645161  0.81451613  0.80645161  0.81300813  0.83606557]\n",
      "mean accuracy = 0.815298611738\n",
      "\n",
      "c param = 0.2\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "c param = 0.3\n",
      "accuracies = [ 0.7983871   0.83870968  0.79032258  0.79674797  0.8442623 ]\n",
      "mean accuracy = 0.81368592348\n",
      "\n",
      "c param = 0.4\n",
      "accuracies = [ 0.7983871   0.83870968  0.78225806  0.78861789  0.8442623 ]\n",
      "mean accuracy = 0.810447003994\n",
      "\n",
      "c param = 0.5\n",
      "accuracies = [ 0.79032258  0.83870968  0.78225806  0.78861789  0.8442623 ]\n",
      "mean accuracy = 0.808834100768\n",
      "\n",
      "c param = 0.6\n",
      "accuracies = [ 0.7983871   0.83870968  0.78225806  0.78861789  0.8442623 ]\n",
      "mean accuracy = 0.810447003994\n",
      "\n",
      "c param = 0.7\n",
      "accuracies = [ 0.79032258  0.83870968  0.78225806  0.78861789  0.8442623 ]\n",
      "mean accuracy = 0.808834100768\n",
      "\n",
      "c param = 0.8\n",
      "accuracies = [ 0.78225806  0.83870968  0.78225806  0.79674797  0.8442623 ]\n",
      "mean accuracy = 0.808847213803\n",
      "\n",
      "c param = 1\n",
      "accuracies = [ 0.78225806  0.83870968  0.78225806  0.80487805  0.8442623 ]\n",
      "mean accuracy = 0.810473230063\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for C_param in [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1]:\n",
    "    log_reg_l1_diff_c = LogisticRegression(penalty='l1', C=C_param, fit_intercept=True, random_state=1)\n",
    "    print('c param =', C_param)\n",
    "    print_scores(log_reg_l1_diff_c, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal value is 0.2. I will use that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Will look for optimal Tolerance for stopping criteria (tol)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tol param = 1e-08\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "tol param = 1e-06\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "tol param = 0.0001\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "tol param = 0.01\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "tol param = 0.1\n",
      "accuracies = [ 0.80645161  0.83870968  0.7983871   0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tol in [1e-8, 1e-6, 1e-4, 1e-2, 1e-1]:\n",
    "    log_reg_l1_diff_tol = LogisticRegression(penalty='l1', C=0.2, tol = tol, fit_intercept=True, random_state=1)\n",
    "    print('tol param =', tol)\n",
    "    print_scores(log_reg_l1_diff_tol, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All give the same results, so I will stick to the default value of 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) I will perform cross-validation of the model with optimal parameters on the training set. Then, will check it's performance on the whole train set and the check it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.80645161  0.81451613  0.80645161  0.81300813  0.83606557]\n",
      "mean accuracy = 0.815298611738\n"
     ]
    }
   ],
   "source": [
    "# cross validation on train set\n",
    "log_reg_final = LogisticRegression(penalty='l1', C=0.1, fit_intercept=True, random_state=1)\n",
    "print_scores(log_reg_final, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8233387358184765"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on train-set and evaluate on train set\n",
    "log_reg_final.fit(train[predictors], train[target])\n",
    "log_reg_final.score(train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85161290322580641"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model on the test set\n",
    "log_reg_final.score(test[predictors], test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is consistent (from 0.80 to 0.85). Model is not overfitting, since the score on the whole train set is 0.82 which is consistent with other scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_4) Determening the key factors with the final model_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Will train the model on the whole dataset and check the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82901554404145072"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_set = dataset_normalized_unique_features\n",
    "log_reg_final.score(full_set[predictors], full_set[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) getting the coefficients on the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "coef_array = log_reg_final.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_series = pd.Series(coef_array[0], predictors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly as with correlation coefficients, I will get the absolute values and sort them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_coefficients = coef_series.apply(np.abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the list of factors from the most important factor (v29) to the least important factors (v184_Lower Fraud Risk and all below it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v29                                                   0.355103\n",
       "v204_wifi                                             0.312712\n",
       "v192                                                  0.270813\n",
       "v204_business                                         0.175122\n",
       "v195_low                                              0.085339\n",
       "v182                                                  0.083630\n",
       "v120                                                  0.068040\n",
       "v002                                                  0.051416\n",
       "v204_residential                                      0.047279\n",
       "v204_cellular                                         0.044461\n",
       "v5                                                    0.028367\n",
       "v177                                                  0.004279\n",
       "v178_validdomain                                      0.000000\n",
       "v172.1_n                                              0.000000\n",
       "v174                                                  0.000000\n",
       "v173                                                  0.000000\n",
       "v172.1_y                                              0.000000\n",
       "v201_moderate risk                                    0.000000\n",
       "v181                                                  0.000000\n",
       "v204_mobile                                           0.000000\n",
       "v178_certified                                        0.000000\n",
       "v180                                                  0.000000\n",
       "v193_yes                                              0.000000\n",
       "v123                                                  0.000000\n",
       "v184_moderate fraud risk                              0.000000\n",
       "v184_data entry review                                0.000000\n",
       "v204_wired                                            0.000000\n",
       "v204_nan                                              0.000000\n",
       "v201_moderate by proxy reputation and country code    0.000000\n",
       "v186_fraud score 1 to 100                             0.000000\n",
       "v4                                                    0.000000\n",
       "v172.1_u                                              0.000000\n",
       "v172.1_p                                              0.000000\n",
       "v203_nan                                              0.000000\n",
       "v175                                                  0.000000\n",
       "v184_lower fraud risk                                 0.000000\n",
       "v001                                                  0.000000\n",
       "v204_dialup                                           0.000000\n",
       "v14                                                   0.000000\n",
       "v185                                                  0.000000\n",
       "v195_moderate                                         0.000000\n",
       "v178_verified                                         0.000000\n",
       "v186_fraud score 101 to 300                           0.000000\n",
       "v176                                                  0.000000\n",
       "v183                                                  0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "v29                   0.355103\n",
       "v204_wifi             0.312712\n",
       "v192                  0.270813\n",
       "v204_business         0.175122\n",
       "v195_low              0.085339\n",
       "v182                  0.083630\n",
       "v120                  0.068040\n",
       "v002                  0.051416\n",
       "v204_residential      0.047279\n",
       "v204_cellular         0.044461\n",
       "v5                    0.028367\n",
       "v177                  0.004279\n",
       "v178_validdomain      0.000000\n",
       "v172.1_n              0.000000\n",
       "v174                  0.000000\n",
       "v173                  0.000000\n",
       "v172.1_y              0.000000\n",
       "v201_moderate risk    0.000000\n",
       "v181                  0.000000\n",
       "v204_mobile           0.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top15_coef = sorted_coefficients[:20]\n",
    "top15_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['v29', 'v204_wifi', 'v192', 'v204_business', 'v195_low', 'v182', 'v120',\n",
       "       'v002', 'v204_residential', 'v204_cellular', 'v5', 'v177',\n",
       "       'v178_validdomain', 'v172.1_n', 'v174'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top15_coef.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3551034 ,  0.31271249,  0.27081309,  0.17512241,  0.0853392 ,\n",
       "        0.08363011,  0.06804037,  0.05141584,  0.04727882,  0.04446084,\n",
       "        0.02836743,  0.00427913,  0.        ,  0.        ,  0.        ])"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top15_coef.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAl0AAAJDCAYAAAAxXqHAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAIABJREFUeJzs3Xl8nOV57//vpX2f0eJN0sgLmGAj\n2bItzGIwbZM00CRAAoQlJNBATFJoe5r216ZtfiQlTZvlnJ42B7OYhISQxRCyOadOCCEkLDbgBe/G\neLdlGWxJlm0t1nqfP2bskYVtjaSZeWb5vF8vvZh55nlmLt3I9lf3XHPf5pwTAAAAYivD6wIAAADS\nAaELAAAgDghdAAAAcUDoAgAAiANCFwAAQBwQugAAAOKA0AUAABAHhC4AAIA4IHQBAADEQZbXBQxV\nUVHhpkyZEtPX6BvokyRlZSTctx93jEUQ4xDGWIQxFmGMRRDjEMZYBK1Zs6bZOTcuknMTbqSmTJmi\n1atXx/Q1WjpbJEnlBeUxfZ1kwFgEMQ5hjEUYYxHGWAQxDmGMRZCZ7Y30XN5eBAAAiANCFwAAQBwQ\nugAAAOKA0AUAABAHhC4AAIA4IHQBAADEAaELAAAgDghdAAAAcUDoAgAAiANCFwAAQBwQugAAAOKA\n0AUAABAHhC4AAIA4IHQBAADEAaELAAAgDghdAAAAcUDoAgAAiANCFwAAQBwQugAAAOKA0AUAABAH\nhC4AAIA4IHQBAADEQdqFro7uPv3d09v00zXveF0KAABII2kXugpyMnXoWI+eXHlQ/QPO63IAAECa\nSLvQZWa6/bJK7Ws9od9uZbYLAADER9qFLkn6kxllqvTn6rEXd3ldCgAASBNpGbqyMky3XTJRq/ce\n0dp9R7wuBwAApIGIQpeZXW1m28xsh5l9/gyPf8bMNprZOjN72cxmho5PMbOu0PF1ZvZItL+B0bpu\nzniV5GXpWy8x2wUAAGJv2NBlZpmSFku6RtJMSbeeDFWD/NA5V+ecq5f0dUn/Meixnc65+tDXZ6JV\n+FgV5GTq9ksn69eb3tbelg6vywEAACkuK4Jz5kva4ZzbJUlmtlTSdZK2nDzBOXds0PmFkkb9scC+\ngT61dLaM9vKItHa1SpKunePTkhdND/1+q/7+mqkxfc1EdXIs0h3jEMZYhDEWYYxFEOMQxliMXCRv\nL1ZJ2j/ofmPo2GnM7F4z26ngTNdfDXpoqpm9YWZ/MLMrz/QCZrbIzFab2eqW5tgGrsHGFefomroK\n/WLdYbV19sbtdQEAQPqJZKbLznDsXTNZzrnFkhab2W2SviDpDkkHJdU451rMbJ6kn5vZRUNmxuSc\nWyJpiSQ1NDS48oLyEX4bo1NeUK77/jhHv1z/on614bju+5PpcXndRBSvMU90jEMYYxHGWIQxFkGM\nQxhjEblIZroaJQUG3a+W1HSO85dKul6SnHPdzrmW0O01knZKumB0pcbGeyYW66oLxum7K/bqRG+/\n1+UAAIAUFUnoWiVpuplNNbMcSbdIWjb4BDMbPEX0QUnbQ8fHhRrxZWbTJE2XlHAfF1y0cJqa27v1\ni3UHvC4FAACkqGFDl3OuT9J9kp6VtFXS0865zWb2gJldGzrtPjPbbGbrJH1OwbcWJWmhpA1mtl7S\nM5I+45xLuM67y88r18xJJXrspd0aYGsgAAAQA5H0dMk5t1zS8iHH7h90+6/Pct1PJP1kLAXGg5lp\n0cJp+h9PrdMf3jqsP75wvNclAQCAFJOWK9KfyQdnTdIkX56WsDUQAACIAUJXSHZmhv58wRSt3NWi\njY1HvS4HAACkGELXILfMr1FRbpYeY2sgAAAQZYSuQUrysnXr/ID+e+NBNR7p9LocAACQQghdQ/z5\ngqkySd95ZY/XpQAAgBRC6Bqi0p+vD82apKWv79PRLrYGAgAA0UHoOoO7r5ymjp5+LX19n9elAACA\nFEHoOoPaKp8uP69c33llj3r6BrwuBwAApABC11l8euE0vX3shP7vhnNtMwkAABAZQtdZ/NEF4zR9\nfJGWvLhLzrE1EAAAGBtC11mYmT69cJrefPu4Xt7R7HU5AAAgyRG6zuG6+kqNK85layAAADBmhK5z\nyM3K1J2XT9FL25u19eAxr8sBAABJjNA1jI9fUqP87Ex966XdXpcCAACSGKFrGP6CHN18cUDL1h/Q\n20dPeF0OAABIUoSuCHxqwVT1Dzh9d8Uer0sBAABJitAVgZryAl1TO0k/eG2v2rv7vC4HAAAkIUJX\nhO6+cqqOn+jTU6v2e10KAABIQoSuCM2pKdX8KWV6/OXd6utnayAAADAyhK4R+PTCaTrQ1qXlm972\nuhQAAJBkCF0j8N4Lx2taRaEeY2sgAAAwQoSuEcjIMN115VRtPHBUr+1u9bocAACQRAhdI3TD3GqV\nFeboMbYGAgAAI0DoGqG87Ex98rLJev7NQ9px6LjX5QAAgCRB6BqFT1w6WblZGWwNBAAAIkboGoXy\nolzdOK9aP117QIeOszUQAAAYHqFrlO66Yqp6Bwb05Mq9XpcCAACSAKFrlKaNK9L7ZkzQk6/uVVdP\nv9flAACABEfoGoNFC6eprbNXz6xhayAAAHBuhK4xaJhcqvqAX996ebf6B1gsFQAAnB2hawzMTIsW\nTtPelk49t4WtgQAAwNkRusboAxdNVKAsX0tYLBUAAJwDoWuMMjNMd18xTWv3tWnNXrYGAgAAZ0bo\nioKbGqrly89mtgsAAJwVoSsKCnKydPulNfrNlne0p7nD63IAAEACInRFyR2XTVF2Roa+/TJbAwEA\ngHcjdEXJ+JI8XT+nUj9es1+tHT1elwMAABIMoSuK7r5ymk70Duj7r7I1EAAAOB2hK4oumFCsP37P\nOD2xYo9O9LI1EAAACCN0RdmnF05TS0ePfvbGAa9LAQAACYTQFWWXTSvXRZUleuylXRpgayAAABBC\n6Iqyk1sD7TrcoRe2HfK6HAAAkCAIXTHwZ3WTVOnLY7FUAABwCqErBrIzM/SpK6bqtd2tWr+/zety\nAABAAiB0xcjNFwdUnJulx15itgsAABC6YqY4L1u3XVKj5RsPan9rp9flAAAAjxG6YujOBVOUYcbW\nQAAAgNAVS5N8+bq2vlJPrdqvI2wNBABAWiN0xdiihdPU1dvP1kAAAKQ5QleMXTixRFddME5PrGRr\nIAAA0hmhKw7uWThNze09+ulatgYCACBdEbri4LLzylVX5dO3XtqlfrYGAgAgLRG64uDU1kDNHXpu\nyztelwMAADxA6IqTa2onqro0X0te3Ol1KQAAwAOErjjJyszQ3VdM1dp9bVq9p9XrcgAAQJwRuuLo\nYxcH5C/I1qNshA0AQNohdMVRQU6WPnnpZP126zvacajd63IAAEAcEbri7JOXT1FOZoa+xUbYAACk\nFUJXnFUU5eqGedX66doDOnT8hNflAACAOCF0eeDTV05T78CAnlixx+tSAABAnEQUuszsajPbZmY7\nzOzzZ3j8M2a20czWmdnLZjZz0GP/GLpum5l9IJrFJ6upFYX6wMyJenLlXnV093ldDgAAiINhQ5eZ\nZUpaLOkaSTMl3To4VIX80DlX55yrl/R1Sf8RunampFskXSTpakkPhZ4v7S26apqOnejT0lX7vS4F\nAADEQSQzXfMl7XDO7XLO9UhaKum6wSc4544Nulso6eReN9dJWuqc63bO7Za0I/R8aW9uTakunlKq\nx1/erd7+Aa/LAQAAMZYVwTlVkgZPxzRKumToSWZ2r6TPScqR9CeDrn11yLVV53qxvoE+tXS2RFDW\n6LV2JcbipLdeMk6fe+otPbV6u66pq/CkhkQZC68xDmGMRRhjEcZYBDEOYYzFyEUy02VnOPauXZud\nc4udc+dJ+gdJXxjJtWa2yMxWm9nqlubYBq5EcuUFpZpSkacnVzbJOTbCBgAglUUy09UoKTDofrWk\npnOcv1TSwyO51jm3RNISSWpoaHDlBeURlDV28Xqdc/nsVdP1Dz/ZqDebpCume1dPIoxFImAcwhiL\nMMYijLEIYhzCGIvIRTLTtUrSdDObamY5CjbGLxt8gplNH3T3g5K2h24vk3SLmeWa2VRJ0yW9Pvay\nU8f1c6o0rjhXj7IRNgAAKW3Y0OWc65N0n6RnJW2V9LRzbrOZPWBm14ZOu8/MNpvZOgX7uu4IXbtZ\n0tOStkj6taR7nXP9Mfg+klZuVqbuvHyKXtrerM1NR70uBwAAxEgkby/KObdc0vIhx+4fdPuvz3Ht\nVyR9ZbQFpoPbL5msxS/s0GMv7tJ/3jLH63IAAEAMsCJ9AvAVZOvW+TX65YaDajzS6XU5AAAgBghd\nCeJTV0yVJD3+8h5vCwEAADFB6EoQVf58fXjWJC1dtU9HO3u9LgcAAEQZoSuBLFp4njp7+vX91/Z6\nXQoAAIgyQlcCmVlZoiunV+i7K/boRC8f8gQAIJUQuhLMPQvP0+Hj3fr5Gwe8LgUAAEQRoSvBLDi/\nXDMnlWjJS7s0MMDWQAAApApCV4IxM91z1TTtOtyh59885HU5AAAgSghdCejP6iapyp+vJWwNBABA\nyiB0JaDszAzddcVUrdpzRGv2HvG6HAAAEAWErgR188UB+fKzme0CACBFELoSVGFulm6/tEa/2fKO\ndh1u97ocAAAwRoSuBHbH5VOUnZGhb7282+tSAADAGBG6Etj44jzdMK9Kz6xp1OHj3V6XAwAAxoDQ\nleDuvnKaevsH9L2Ve7wuBQAAjAGhK8GdN65I75sxQU++uledPX1elwMAAEaJ0JUE7lk4TW2dvXp6\n1X6vSwEAAKNE6EoCDVPKNG9yqb718m719Q94XQ4AABgFQleSWLRwmhqPdGn5pre9LgUAAIwCoStJ\nvH/GBE2rKNSSF3fKOTbCBgAg2RC6kkRGhunuK6dp04FjWrmzxetyAADACBG6kshH51apoihHj764\ny+tSAADACBG6kkhedqbuvHyK/vDWYW09eMzrcgAAwAgQupLM7ZdOVkFOph5jtgsAgKRC6Eoy/oIc\nfawhoGXrm9TU1uV1OQAAIEKEriR01xVT5SR95xU2wgYAIFkQupJQoKxAH6ybpB+9vl9Hu3q9LgcA\nAESA0JWkFi2cpvbuPv3wtX1elwIAACJA6EpStVU+LTi/XN95Zbe6+/q9LgcAAAyD0JXEFi08T4eO\nd+sX65q8LgUAAAyD0JXEFk6v0IUTi/XYi7s0MMDWQAAAJDJCVxIzM91z1TRtP9Su3791yOtyAADA\nORC6ktyHZlWq0penR/7AYqkAACQyQleSy87M0KeumKrXd7dq3f42r8sBAABnQehKAbfMr1FxXpaW\nvLjT61IAAMBZELpSQFFulj7WENBvNr+jE70sHwEAQCIidKWIi6eUqm/AaevBY16XAgAAzoDQlSLq\nqv2SpE0HjnpcCQAAOBNCV4qo9OWprDBHGwldAAAkJEJXijAz1Vb5tKGR0AUAQCIidKWQWVU+bT/U\nTjM9AAAJiNCVQmqrfOqnmR4AgIRE6EohddU+SaKvCwCABEToSiGVvjyVF+ZoI31dAAAkHEJXCjnZ\nTM9MFwAAiYfQlWLqaKYHACAhEbpSTF11sJl+C830AAAkFEJXiqmrCjbTszI9AACJhdCVYiaFmulZ\nJBUAgMRC6EoxZqa6ah8zXQAAJBhCVwo62Uzf1UMzPQAAiYLQlYJOrkxPMz0AAImD0JWCZlXTTA8A\nQKIhdKWgiSV5qijKYZFUAAASCKErBZ1amZ5PMAIAkDAIXSlqVpVP2w8dp5keAIAEQehKUbVVPg04\n0UwPAECCIHSlqDqa6QEASCiErhQVbKbPZWV6AAASBKErRZmZ6qpKmOkCACBBELpSWB3N9AAAJIyI\nQpeZXW1m28xsh5l9/gyPf87MtpjZBjN73swmD3qs38zWhb6WRbN4nFtdtT/UTM9sFwAAXhs2dJlZ\npqTFkq6RNFPSrWY2c8hpb0hqcM7NkvSMpK8PeqzLOVcf+ro2SnUjAnVVwWZ61usCAMB7WRGcM1/S\nDufcLkkys6WSrpO05eQJzrkXBp3/qqTbR1tQ30CfWjpbRnt5RFq7WmP6/IkiK8upvDBbq/cd0ofn\nlJzxnHQZi+EwDmGMRRhjEcZYBDEOYYzFyEXy9mKVpP2D7jeGjp3NXZJ+Neh+npmtNrNXzez6M11g\nZotC56xuaY5t4EonZqYZlYXa2tThdSkAAKS9SGa67AzH3BlPNLtdUoOkqwYdrnHONZnZNEm/M7ON\nzrmdpz2Zc0skLZGkhoYGV15QHlHxYxWv1/HS3JpxenDHduVn+VSQc/b/3ekwFpFgHMIYizDGIoyx\nCGIcwhiLyEUy09UoKTDofrWkpqEnmdn7JP2zpGudc90njzvnmkL/3SXp95LmjKFejFBdaGX6raxM\nDwCApyIJXaskTTezqWaWI+kWSad9CtHM5kh6VMHAdWjQ8VIzyw3drpC0QIN6wRB7s0Ir07NIKgAA\n3hr27UXnXJ+Z3SfpWUmZkh53zm02swckrXbOLZP0DUlFkn5sZpK0L/RJxRmSHjWzAQUD3ledc4Su\nOJpQkqdxxbnayCKpAAB4KpKeLjnnlktaPuTY/YNuv+8s162QVDeWAjF2dVU+VqYHAMBjrEifBuqq\nfNpxqF2dPX1elwIAQNoidKWBk830W5popgcAwCuErjRQF2qmp68LAADvELrSwISSPI0vzmU7IAAA\nPEToShN1VT5mugAA8BChK03UVvm083C7OrpppgcAwAuErjQxqzrUTM/K9AAAeILQlSbqqkLN9PR1\nAQDgCUJXmhgfaqZnkVQAALxB6Eojs6p92kDoAgDAE4SuNEIzPQAA3iF0pZG6Kp8czfQAAHiC0JVG\nTjbTb6CZHgCAuCN0pZHxJXmaUEIzPQAAXiB0pRlWpgcAwBuErjRTV+XXzsPtaqeZHgCAuCJ0pZm6\n6pJgM30TzfQAAMQToSvN1J5cmZ63GAEAiCtCV5oZX5yniSV52tjY5nUpAACkFUJXGqqlmR4AgLgj\ndKWhuiqfdjV30EwPAEAcEbrS0Kzq4Mr0m5ntAgAgbghdaYhmegAA4o/QlYbGFedqYkkeK9MDABBH\nhK40VVvl0wZCFwAAcUPoSlOzqn3a3dyhju5+r0sBACAtELrSVF1VsJl+29sdXpcCAEBaIHSlqZPN\n9FuaCF0AAMQDoStNjSvO1SRfnrYebPe6FAAA0gKhK43VVvm09SAzXQAAxAOhK43VVfm0t+UEK9MD\nABAHhK40Vlcd7OvadrDT40oAAEh9hK40VhdqpuctRgAAYo/QlcYqinI1oSRHW2imBwAg5ghdaW7G\npEK9yUwXAAAxR+hKczMmFWpvywkdP9HrdSkAAKQ0QleamzGpUJK06cAxjysBACC1EbrS3IzKIknS\nJja/BgAgpghdaa6sMFsTSnK0kdAFAEBMEbqgGZMKCV0AAMQYoQuaWVmk3c0dOkYzPQAAMUPowqlm\n+s000wMAEDOELpwKXRsPtHlcCQAAqYvQBZUWZqvKn6+NzHQBABAzhC5IkmqrSlg2AgCAGCJ0QVJw\n82ua6QEAiB1CFyRJddV+SSySCgBArBC6ICk40yURugAAiBVCFyRJZYU5NNMDABBDhC6cUlfl08ZG\nlo0AACAWCF04pa7apz0tnTTTAwAQA4QunFJLXxcAADFD6MIpJ5vpNzYSugAAiDZCF04JN9MTugAA\niDZCF05TV+Xj7UUAAGKA0IXTnGymP9pFMz0AANFE6MJpTvZ1bWa2CwCAqCJ04TSnmukJXQAARBWh\nC6cpLcxRdWm+NhC6AACIKkIX3oVmegAAoi+i0GVmV5vZNjPbYWafP8PjnzOzLWa2wcyeN7PJgx67\nw8y2h77uiGbxiI3aKp/2tnTqaCfN9AAARMuwocvMMiUtlnSNpJmSbjWzmUNOe0NSg3NulqRnJH09\ndG2ZpC9KukTSfElfNLPS6JWPWJhVHVqZvonZLgAAoiWSma75knY453Y553okLZV03eATnHMvOOc6\nQ3dflVQduv0BSc8551qdc0ckPSfp6uiUjlipraSZHgCAaMuK4JwqSfsH3W9UcObqbO6S9KtzXFt1\nrhfrG+hTS2dLBGWNXmtXa0yfP5mccSxMqvTnas3eQ2rp9Me/KA/wMxHGWIQxFmGMRRDjEMZYjFwk\nocvOcMyd8USz2yU1SLpqJNea2SJJiySpOlD9rgsQfzMmFWrLwQ6vywAAIGVEEroaJQUG3a+W1DT0\nJDN7n6R/lnSVc6570LV/NOTa3w+91jm3RNISSWpoaHDlBeURlDV28XqdZDB0LOZNHqfnt7YqSyXy\nFWR7VFX88TMRxliEMRZhjEUQ4xDGWEQukp6uVZKmm9lUM8uRdIukZYNPMLM5kh6VdK1z7tCgh56V\n9KdmVhpqoP/T0DEkuJOLpNJMDwBAdAwbupxzfZLuUzAsbZX0tHNus5k9YGbXhk77hqQiST82s3Vm\ntix0baukLysY3FZJeiB0DAnuZOja0EjoAgAgGiJ5e1HOueWSlg85dv+g2+87x7WPS3p8tAXCG/6C\nHAXK8lkkFQCAKGFFepxVXZWPZSMAAIgSQhfOqq7Kr32tnWrr7PG6FAAAkh6hC2d1qpn+wDGPKwEA\nIPkRunBWtVUlkliZHgCAaCB04az8BTmqKSvQxgNtXpcCAEDSI3ThnGimBwAgOghdOKfaKp/2t3bR\nTA8AwBgRunBOs6qDzfTMdgEAMDaELpxTbSWhCwCAaCB04Zx8BdmqKStgZXoAAMaI0IVh1VX72IMR\nAIAxInRhWHVVPjUe6dKRDprpAQAYLUIXhnVqZfomZrsAABgtQheGdbKZnrcYAQAYPUIXhuUryNbk\ncprpAQAYC0IXIlLLyvQAAIwJoQsRoZkeAICxIXQhIrOqWCQVAICxIHQhIhcRugAAGBNCFyLiyw82\n02/kE4wAAIwKoQsRq6OZHgCAUSN0IWJ1VT4daOtSK830AACMGKELEaujrwsAgFEjdCFiJ5vpWSQV\nAICRI3QhYr78bE2hmR4AgFEhdGFEWJkeAIDRIXRhRGZV00wPAMBoELowIrU00wMAMCqELozIqdDV\n2OZxJQAAJBdCF0akJC9bUysKtYFmegAARoTQhRGbXe3Tema6AAAYEUIXRqw+4Nc7x7p18GiX16UA\nAJA0CF0YsdkBvyRp3T5muwAAiBShCyM2s7JEOZkZWsdbjAAARIzQhRHLzcrUjMoSZroAABgBQhdG\nZU7Ar40Hjqp/wHldCgAASYHQhVGZHfCps6df2w8d97oUAACSAqELo1IfKJVEMz0AAJEidGFUppQX\nyJefrXX7CV0AAESC0IVRMTPNDvgJXQAARIjQhVGrD/j11jvH1dHd53UpAAAkPEIXRm1OwK8BJ208\nwD6MAAAMh9CFUZtV7ZMk3mIEACAChC6MWnlRrmrKCrSe0AUAwLAIXRiTeprpAQCICKELY1If8Ovg\n0RN659gJr0sBACChEbowJrMDfkn0dQEAMBxCF8bkosoSZWcaoQsAgGEQujAmedmZmjGphO2AAAAY\nBqELYza72q8NjW3qH3BelwIAQMIidGHM6gN+dfT0a+fhdq9LAQAgYRG6MGb1NaFmet5iBADgrAhd\nGLOp5YUqzsvSGzTTAwBwVoQujFlGhqk+4GdlegAAzoHQhaioD/i17Z3j6urp97oUAAASEqELUVEf\n8Kt/wGnjgaNelwIAQEIidCEqTq5Mz1uMAACcGaELUVFRlKvq0nxWpgcA4CwIXYia+oCf0AUAwFkQ\nuhA19QG/DrR16dDxE16XAgBAwiF0IWrqT/V10UwPAMBQEYUuM7vazLaZ2Q4z+/wZHl9oZmvNrM/M\nbhzyWL+ZrQt9LYtW4Ug8tVU+ZWaY1u0/4nUpAAAknKzhTjCzTEmLJb1fUqOkVWa2zDm3ZdBp+yTd\nKenvzvAUXc65+ijUigSXl52pCycW09cFAMAZDBu6JM2XtMM5t0uSzGyppOsknQpdzrk9occGxlpQ\n30CfWjpbxvo059Ta1RrT508m0R6LCyfl6debWnS4o1kZZlF97ljiZyKMsQhjLMIYiyDGIYyxGLlI\n3l6skrR/0P3G0LFI5ZnZajN71cyuP9MJZrYodM7qlubYBi7EVm1VkTq6+7W3mWZ6AAAGi2Sm60zT\nFW4Er1HjnGsys2mSfmdmG51zO097MueWSFoiSQ0NDa68oHwETz968XqdZBCtsbjy/BxJu7T7sFPD\n5OQbX34mwhiLMMYijLEIYhzCGIvIRTLT1SgpMOh+taSmSF/AOdcU+u8uSb+XNGcE9SHJTKsoUnFu\nltY30tcFAMBgkYSuVZKmm9lUM8uRdIukiD6FaGalZpYbul0haYEG9YIh9WRkmGYFfDTTAwAwxLCh\nyznXJ+k+Sc9K2irpaefcZjN7wMyulSQzu9jMGiXdJOlRM9scunyGpNVmtl7SC5K+OuRTj0hB9QG/\n3jx4XCd6+70uBQCAhBFJT5ecc8slLR9y7P5Bt1cp+Lbj0OtWSKobY41IMrOr/eobcNp04KgappR5\nXQ4AAAmBFekRdfU1wZXpeYsRAIAwQheibnxxnqr8+YQuAAAGIXQhJmbTTA8AwGkIXYiJ+oBfjUe6\n1Nze7XUpAAAkBEIXYqI+UCpJWs9sFwAAkghdiJHaqhJlZhhvMQIAEELoQkwU5GTpggnFhC4AAEII\nXYiZ+oBf6/e3aWBgJFt1AgCQmghdiJk5Ab+OnejT7pYOr0sBAMBzhC7EzOxAaJHUfbzFCAAAoQsx\nc/74IhXmZGp9I6ELAABCF2ImM8M0q9pPMz0AACJ0IcZmB/zaevCYTvT2e10KAACeInQhpuoDfvX2\nO205eMzrUgAA8BShCzE1p4ZmegAAJEIXYmxCSZ4m+fLo6wIApD1CF2JuNs30AAAQuhB79TV+7Wvt\nVGtHj9elAADgGUIXYq4+tEjqema7AABpjNCFmKur8inDpDcIXQCANEboQswV5mbpggnFzHQBANIa\noQtxUR/wa31jm5xzXpcCAIAnCF2Ii9kBv9o6e7WnpdPrUgAA8AShC3FBMz0AIN0RuhAXF0woVkFO\nJut1AQDSFqELcZGZYaqr8vEJRgBA2iJ0IW7qA35tbTqm7r5+r0sBACDuCF2Im/qAXz39A9p68LjX\npQAAEHeELsRNfU2wmX7dviOwypr6AAAgAElEQVQeVwIAQPwRuhA3E0vyNL44l2Z6AEBaInQhbsws\ntEjqUa9LAQAg7ghdiKv6Gr92N3eorbPH61IAAIgrQhfiqr461NfFW4wAgDRD6EJc1VX7ZEboAgCk\nH0IX4qo4L1vTxxexHRAAIO0QuhB39QG/1u1vk3PO61IAAIgbQhfibnbAryOdvdrX2ul1KQAAxA2h\nC3FXH6CZHgCQfghdiLv3TChWXnYGoQsAkFYIXYi7rMwM1VX5CF0AgLRC6IIn6gN+bW46pp6+Aa9L\nAQAgLghd8ER9oFQ9fQN68+1jXpcCAEBcELrgidkBnySa6QEA6YPQBU9U+fNVUZSrdfsIXQCA9EDo\ngifMLLhIaiOhCwCQHghd8MycGr92He7Q0c5er0sBACDmCF3wzOzq4CKp65ntAgCkAUIXPDMr4JOZ\n2PwaAJAWCF3wTElets4bV8QnGAEAaYHQBU/NrvZr3f42Oee8LgUAgJgidMFT9TV+tXT0qPFIl9el\nAAAQU4QueGpOINhMz1uMAIBUR+iCp94zsVi5WRmELgBAyiN0wVPZmRmqrfIRugAAKY/QBc/VB/za\ndOCoevsHvC4FAICYIXTBc/UBv7r7BrTt7eNelwIAQMwQuuC5+lAz/Ru8xQgASGGELniuujRf5YU5\nrEwPAEhphC54zsxUH/DTTA8ASGmELiSE2QG/dh5u17ETvV6XAgBATEQUuszsajPbZmY7zOzzZ3h8\noZmtNbM+M7txyGN3mNn20Ncd0SocqaU+4Jdz0sbGo16XAgBATAwbuswsU9JiSddIminpVjObOeS0\nfZLulPTDIdeWSfqipEskzZf0RTMrHXvZSDWzq1mZHgCQ2iKZ6ZovaYdzbpdzrkfSUknXDT7BObfH\nObdB0tCFlj4g6TnnXKtz7oik5yRdHYW6kWJ8BdmaVlGoN/YRugAAqSkrgnOqJO0fdL9RwZmrSJzp\n2qpzXdA30KeWzpYIn350WrtaY/r8ySSRxmJGZb5e3dmq5o5mmVlcXzuRxsFrjEUYYxHGWAQxDmGM\nxchFMtN1pn/9XITPH9G1ZrbIzFab2eqW5tgGLiSu2qoitXT06u1jPV6XAgBA1EUy09UoKTDofrWk\npgifv1HSHw259vdDT3LOLZG0RJIaGhpceUF5hE8/NvF6nWSQCGOx4LxMSXu097CpdpI39STCOCQK\nxiKMsQhjLIIYhzDGInKRzHStkjTdzKaaWY6kWyQti/D5n5X0p2ZWGmqg/9PQMeBdLpxYopysDK3b\nf8TrUgAAiLphQ5dzrk/SfQqGpa2SnnbObTazB8zsWkkys4vNrFHSTZIeNbPNoWtbJX1ZweC2StID\noWPAu+RkZeiiyhKt38+yEQCA1BPJ24tyzi2XtHzIsfsH3V6l4FuHZ7r2cUmPj6FGpJH6gF9LX9+v\nvv4BZWWydi8AIHXwrxoSSn3Ar67efm1757jXpQAAEFWELiSU+gCLpAIAUhOhCwmlpqxApQXZWk/o\nAgCkGEIXEoqZaXbAz0wXACDlELqQcOoDfm0/1K7jJ3q9LgUAgKghdCHh1Af8ck7aeIClIwAAqYPQ\nhYRDMz0AIBURupBw/AU5mlJeoHX7CF0AgNRB6EJCqg/4tb6R0AUASB2ELiSk+oBf7xzr1sGjXV6X\nAgBAVBC6kJBmn+zr4i1GAECKIHQhIc2sLFFOZgbN9ACAlEHoQkLKzcrUjMoSQhcAIGUQupCw6qt9\n2njgqPoHnNelAAAwZoQuJKz6Gr86e/q1bP0BvfXOcbV29GiAAAYASFJZXhcAnE3D5DJlmPQ3T60/\ndSwrw1RWmKOKolyVF+VoXFGuKopzVVEUPDb4eFlhjrIy+b0CAJAYCF1IWIGyAr3493+sfa2dam7v\nUUt7t5rbu9V8vCf43/Zu7TrcocPt3erpG3jX9WZSaUHOqUBWXhQOZ8GwFj6ekTGgnCwCGgAgdghd\nSGjVpQWqLi045znOObV396m5PRTGjneruaMn+N+TQa29Rxsa29TS3qP27r4zPk9Vaa6+ectczZtc\nFotvBQCQ5ghdSHpmpuK8bBXnZWtqReGw53f19J8Wxlrau7XvSJuWrTukW5a8qn+9vlY3X1wTh8oB\nAOmE0IW0k5+TqUBZgQJl4Rm0ls5C3dgwQV/8+R79w082akvTMX3hQzOVTU8YACBK+BcFCPHlZ+k7\nd16su6+YqidW7tUnvv2aWjt6vC4LAJAiCF3AIFmZGfrCh2bqf900W2v3tenaB1/WlqZjXpcFAEgB\nhC7gDG6YV60f33OZevsHdMPDK7R840GvSwIAJDlCF3AWswN+/fK+KzRjUrH+4gdr9b9+s43FWQEA\no0boAs5hfEmefrToUn2soVr/53c7tOjJNTp+otfrsgAASYjQBQwjNytTX7thlv7l2ov0wrZD+uhD\nK7SnucPrsgAASYbQBUTAzHTH5VP05Kfmq7m9W9c++LJefOuw12UBAJIIoQsYgcvPr9Cy+65QpT9f\nd37ndX3rpV1yjj4vAMDwCF3ACAXKCvSTz16uD1w0Uf/631v1t0+v14nefq/LAgAkOEIXMAqFuVla\nfNtcfe79F+inbxzQzY+u1NtHT3hdFgAggRG6gFHKyDD91Xuna8kn5mnHoXZ9+MGXtWbvEa/LAgAk\nKEIXMEZ/etFE/ezeBSrIydStS17V06v2e10SACABEbqAKLhgQrF+ce8CXTKtTH//kw360rLN6u0f\n8LosAEACIXQBUeIvyDm1YfZ3V+zRJ7/9OhtmAwBOIXQBUTR4w+w1+47o2gdf1taDbJgNACB0ATFx\nw7xqPR3aMPujD63Qr9gwGwDSHqELiJH60IbZF04q1md/sFb/wYbZAJDWCF1ADI0vydPSRZfqpnnV\n+iYbZgNAWiN0ATGWm5Wpr984S1/68Ew2zAaANEboAuLAzHTngql68lPzdbi9W9c/9IpW7Wn1uiwA\nQBwRuoA4uvz8Cv3i3gUqK8jRxx97Tb9c3+R1SQCAOCF0AXE2ubxQP/ns5aoP+PWXP3pDD/1+h5yj\nwR4AUh2hC/BAaWGOnrx7vq6dXamv/3qb/ulnG1nBHgBSXJbXBQDpKjcrU/95c71qygr04As7dKDt\nhBbfNkfFedlelwYAiAFmugAPZWSY/u4D79HXbqjTKzuaddMjK3XwaJfXZQEAYoDQBSSAmy+u0Xfu\nvFiNR7p0/eJXtLnpqNclAQCijNAFJIiFF4zTM5+9TBlm+tgjK/XCtkNelwQAiCJCF5BALpxYop/f\nu0BTKgp19xOr9f1X93pdEgAgSghdQIKZUJKnp++5TAunV+gLP9+kf1++lT0bASAFELqABFSYm6XH\nPtmg2y+t0aMv7tJf/ugNnejt97osAMAYsGQEkKCyMjP05etqNbmsUF9ZvlUHj3bpsU82qLwo1+vS\nAACjwEwXkMDMTJ9eOE0PfXyuNjcd00cfXqFdh9u9LgsAMAqELiAJ/FndJP3w05fq+Ik+ffThFWyW\nDQBJiNAFJIl5k0v1s7+4/NRm2cvYLBsAkgqhC0gigzfL/qsfvaHFL7BZNgAkC0IXkGQGb5b9jWe3\n6R9/ymbZAJAM+PQikITevVl2lx76+Fw2ywaABMZMF5CkBm+WvWJni256ZKWa2tgsGwASFaELSHKD\nN8v+yEOvaNMBNssGgERE6AJSwGmbZT+6Ui+8yWbZAJBoCF1Aiji5WfbUikLd9cQqPclm2QCQUCIK\nXWZ2tZltM7MdZvb5Mzyea2ZPhR5/zcymhI5PMbMuM1sX+nokuuUDGOzkZtlXXTBO///PN+nflm9V\nH59sBICEMOynF80sU9JiSe+X1ChplZktc85tGXTaXZKOOOfON7NbJH1N0s2hx3Y65+qjXDeAszi5\nWfaXfrlZS17cpSdX7lVdtU9zAn7NqfGrPlCqib48r8sEgLQTyZIR8yXtcM7tkiQzWyrpOkmDQ9d1\nkr4Uuv2MpAfNzKJYJ4AROLlZ9lUXjNcrO5r1xv42Pf7KbvW+GFxIdWJJXiiABb/qqn0qyGEFGQCI\npUj+lq2StH/Q/UZJl5ztHOdcn5kdlVQeemyqmb0h6ZikLzjnXjrXi/UN9KmlsyWS2kettYt9605i\nLIJSdRzmTsnS3CkTJU1UT9+Atr3doY0H2rXpQLs2HDiiX216W5KUadJ54wtUV12kKeNMMyblaXa1\nU0aa/+6Uqj8Xo8FYBDEOYYzFyEUSus70t+7QfUfOds5BSTXOuRYzmyfp52Z2kXPu2GkXmy2StEiS\nqgPVEZQEYKRysjJUV12suuriU8eOdPSeCmGbDrTr2U0tau/ulyQV5jaqtrJItVWhr+oilRWy+CoA\njFYkoatRUmDQ/WpJQ3faPXlOo5llSfJJanXBTeG6Jck5t8bMdkq6QNLqwRc755ZIWiJJDQ0Nrryg\nXPEQr9dJBoxFULqNQ3mBdP446SOhrsuBAae1jQe0sfG4drzTp3X72/TdFU3qHwj+nhUoy1d9oPTU\n25IXVZYoLzvTw+8gPtLt5+JcGIsgxiGMsYhcJKFrlaTpZjZV0gFJt0i6bcg5yyTdIWmlpBsl/c45\n58xsnILhq9/MpkmaLmlX1KoHEFUZGaapFfmaWpF/6i/Srp5+bTxwVOv2H9G6/W1as6dVv1wf/L0r\nO9M0c1KJ6gN+zar26/zxRZpSUShfPjNiADDUsKEr1KN1n6RnJWVKetw5t9nMHpC02jm3TNK3JT1p\nZjsktSoYzCRpoaQHzKxPUr+kzzjneBMYSCL5OZmaP7VM86eWnTr2zrETemNfm9btb9O6/Uf04zWN\nemJleF2w8sIcTa0o1JSKQk0d9DWlvFD5Oak/MwYAZxLRx5Wcc8slLR9y7P5Bt09IuukM1/1E0k/G\nWCOABDOhJE9X107U1bUTJUn9A067m9u163CHdjd3aE9Lh3Yd7tBL2w/rmTWNp107yZd3KpBNOxnG\nKgoVKC1QThbrNQNIXXxGHMCYZWaYzh9frPPHF7/rsfbuPu0JBbHdhzu0uyUYzJZvPKi2zt7TnqO6\nNP+0mbGTs2OV/nxlZqT3JykBJD9CF4CYKsrNUm2VT7VVvnc9dqSjJxjCDodmx5o7tKe5Q6/vblVn\nT/+p83KyMjS5rCAYxMYV6srzx+mK6RXx/DYAYMwIXQA8U1qYo9LCHM2tKT3tuHNOh453a3dz6O3K\n5mAg29Xcod9vO6xH/7BLf/3e6fof75su1mEGkCwIXQASjplpQkmeJpTk6dJpp38cvbuvX//00036\nr+e3a+fhdv3Pm2anxbIVAJIfoQtAUsnNytT/vGmWzh9fpK8/+6b2H+nSY5+Yp/El7CcJILHxUSEA\nScfM9Nk/Ok+P3D5Pb719XNctfkWbm456XRYAnBOhC0DS+sBFE/Xjz1wmSbrpkZX6zea3Pa4IAM6O\n0AUgqdVW+fSLexdo+vgi3fP9NXrkDzsV3IEMABILoQtA0htfkqen7rlMf1Y3SV/91Zv6/57ZoO6+\n/uEvBIA4opEeQErIy87Ug7fO0fnjivRfz2/X3pYOPXL7PJUX5XpdGgBIYqYLQAoxM/3N+y/QN2+d\now2NR3X9Q6/orXeOe10WAEgidAFIQdfOrtTSRZeqq2dANzy0Qr/fdsjrkgCA0AUgNc2pKdWy+xao\nuqxAn/ruKn3nld002APwFKELQMqq9Ofrmc9cpvfOmKB/+eUWfeHnm9TbP+B1WQDSFKELQEorzM3S\no7fP0z1XTdMPXtunO7/zuo529npdFoA0ROgCkPIyMkz/eM0MfePGWXp9d6s+8tAr2t3c4XVZANIM\noQtA2ripIaAf3H2pjnT26PrFr2jFjmavSwKQRghdANLK/Kll+sW9V2h8ca4++fjr+uFr+7wuCUCa\nIHQBSDs15QX6yV9crgXnV+iffrZRD/xyi/oH+GQjgNgidAFISyV52fr2HQ268/IpevyV3br7iVU6\nfoIGewCxQ+gCkLayMjP0pWsv0r9eX6sXtzfrhodXaH9rp9dlAUhRhC4Aae/2SyfriT+fr7ePntB1\ni1/R6j2tXpcEIAURugBA0hXTK/SzexfIl5+t2x57TT9d2+h1SQBSDKELAELOG1ekn/3F5Zo3uVSf\ne3q9Hnx+nwbYOghAlBC6AGAQf0GOvnfXfN06P6DvvNKkry7f7XVJAFJEltcFAECiyc7M0L99pE5Z\nmX16cuVBNUzep1vn13hdFoAkx0wXAJyBmekv31ujS6f59MVfbNbafUe8LglAkiN0AcBZZGaYvvLR\n8zW+JFef/f4aHTp+wuuSACQxQhcAnIO/IFuPfmKejnb16t4frFVP34DXJQFIUoQuABjGRZU+fe2G\nWVq154i+8t9bvC4HQJKikR4AInBdfZU2Nh7Vt17erdoqn25qCHhdEoAkw0wXAETo89dcqMvPK9c/\n/3yTNjS2eV0OgCRD6AKACGVlZuj/3DpH44py9Zkn16i5vdvrkgAkEUIXAIxAeVGuHv3EPLV09Oje\nH6xVbz+N9QAiQ+gCgBGqrfLp3z9ap9d2t+rfl7/pdTkAkgSN9AAwCh+dW60NjUf1+Cu7Navap+vn\nVHldEoAEx0wXAIzSP39whuZPLdPnf7pBmw4c9bocAAmO0AUAo5SdmaHFt81VaUGO7nlyjVo7erwu\nCUACI3QBwBiMK87Vw7fP0+Hj3frLH61VH431AM6C0AUAY1Qf8Otfr6/VKzta9I1nt3ldDoAERSM9\nAETBxy4OaMOBNj364i7VVvn04dmVXpcEIMEw0wUAUXL/hy7SvMml+vtnNmjrwWNelwMgwRC6ACBK\ncrIy9PDH56o4L0v3PLlGbZ001gMII3QBQBSNL8nTw7fP08GjXfrrpevUP+C8LglAgiB0AUCUzZtc\nqn+5tlZ/eOuw/uM5GusBBBG6ACAGbrukRrfOD2jxCzv1q40HvS4HQAIgdAFAjHzp2otUH/Drb3+8\nXm+9c9zrcgB4jNAFADGSm5WpR26fp4KcYGP90a5er0sC4CFCFwDE0ERfnh6+fa72t3bqb55apwEa\n64G0RegCgBi7eEqZ7v/wTP3uzUP6z+e3e10OAI8QugAgDj5x6WTdOK9a33x+u57b8o7X5QDwAKEL\nAOLAzPSv19dqVrVPf/PUOu041O51SQDijNAFAHGSlx1srM/NytA9T67W8RM01gPphNAFAHFU6c/X\ng7fN1Z6WTv3t0+tprAfSCKELAOLssvPK9c9/NkO/2fKOFr+ww+tyAMQJoQsAPPDnC6boI3Oq9B+/\nfUu/e5PGeiAdELoAwANmpn/7SJ1mTirRXy9dp93NHV6XBCDGCF0A4JH8nGBjfVaGadH3VuvXmw7q\n0LETXpcFIEayvC4AANJZoKxAi2+bq09/b7U+8/21kqTq0nzNrSnV3Bq/5k4u1YxJJcrO5HdkINkR\nugDAY5efX6G1979fmw4c0xv7jmjtviN6fXerlq1vkiTlZWdoVpVfcyb7Q2GsVOOKcz2uGsBIEboA\nIAHkZmVq3uRSzZtceupYU1uX1u47orV727R23xE9/vJuPdq/S5JUU1ZwaiZsbk2pLpxYrCxmw4CE\nFlHoMrOrJf2XpExJ33LOfXXI47mSvidpnqQWSTc75/aEHvtHSXdJ6pf0V865Z6NWPQCksEp/vir9\n+frQrEpJ0onefm06cPRUEFuxs0U/XxecDcvPztSsat+pEDa3xq/yImbDgEQybOgys0xJiyW9X1Kj\npFVmtsw5t2XQaXdJOuKcO9/MbpH0NUk3m9lMSbdIukhSpaTfmtkFzrn+aH8jAJDq8rIz1TClTA1T\nyiRJzjkdaOvS2n1tWrs3+LbkYy/uUl9owdXJ5QWn9Ya9ZwKzYYCXIpnpmi9ph3NulySZ2VJJ10ka\nHLquk/Sl0O1nJD1oZhY6vtQ51y1pt5ntCD3fyrO9WN9An1o6W0b6fYxIa1drTJ8/mTAWQYxDGGMR\nlgxjkZ8rLZieqwXTJ0qaqK7efr3Z1KENje3a0HhcL751SD9740Dw3OwMXVRVpGnj8pVhNqLXOdEX\n/FRlXlZetL+FpMI4hCXDWJQXZutTV1Z5XcYpkYSuKkn7B91vlHTJ2c5xzvWZ2VFJ5aHjrw659l3f\nvZktkrRIkqoD1ZHWDgAYIj87U3Mml2jO5BJJwdmwprbuUyFsQ2O7frWxecTP61xw9szseFTrTTaM\nQ1gyjMXk8vykC11n+nVo6GZhZzsnkmvlnFsiaYkkNTQ0uPKC8gjKGrt4vU4yYCyCGIcwxiIs2cei\nolCaVaV3/7o8AiffgUj2sRgrxiGMsRi5SN7cb5QUGHS/WlLT2c4xsyxJPkmtEV4LAACQ8iIJXask\nTTezqWaWo2Bj/LIh5yyTdEfo9o2SfueC847LJN1iZrlmNlXSdEmvR6d0AACA5DHs24uhHq37JD2r\n4JIRjzvnNpvZA5JWO+eWSfq2pCdDjfKtCgYzhc57WsGm+z5J9/LJRQAAkI4iWqfLObdc0vIhx+4f\ndPuEpJvOcu1XJH1lDDUCAAAkPRZsAQAAiANCFwAAQBwQugAAAOKA0AUAABAHhC4AAIA4IHQBAADE\nAaELAAAgDghdAAAAcUDoAgAAiANCFwAAQBwQugAAAOKA0AUAABAHhC4AAIA4IHQBAADEAaELAAAg\nDghdAAAAcUDoAgAAiANCFwAAQBwQugAAAOLAnHNe13AaMzssaW8cXqpCUnMcXicZMBZBjEMYYxHG\nWIQxFkGMQxhjIU12zo2L5MSEC13xYmarnXMNXteRCBiLIMYhjLEIYyzCGIsgxiGMsRgZ3l4EAACI\nA0IXAABAHKRz6FridQEJhLEIYhzCGIswxiKMsQhiHMIYixFI254uAACAeErnmS4AAIC4SZvQZWb1\nZrbSzDab2QYzu3nQY39iZmvNbJOZPWFmWV7Witgys1+bWZuZ/d8hx8/4c2BmHw/9zGwwsxVmNtub\nyoHYO8efj/eG/nysM7OXzez80PHPmdmW0J+P581ssjeVR985xuIHZrYt9HfF42aWHTpuZvZNM9sR\nGo+53lQefWZ2h5ltD33dMej4PDPbGPqev2lmFjr+DTN7MzQOPzMzv3fVJ460CV2SOiV90jl3kaSr\nJf2nmfnNLEPSE5Jucc7VKrhG2B3neJ6EMEyInGpmr4X+cDxlZjlDrr3RzJyZjfpjvmZWaWbPDLr/\no1Adf2NmD5jZ+0b73HHwDUmfGHxgmJ+D3ZKucs7NkvRlxamHIdb/j83sTjN7MAp1Xmtmnx/r88TC\nKAL2H5nZ0VCwWGdm9w/z/O2xrN8j7/rzEfKwpI875+ol/VDSF0LH35DUEPrz8Yykr8elyvg421j8\nQNKFkuok5Uu6O3T8GknTQ1+LFByzpGdmZZK+KOkSSfMlfdHMSkMPP6zg93ry+746dPw5SbWhn4u3\nJP1jXItOUCkZuszsa2b2F4Puf0nSh51z2yXJOdck6ZCkcZLKJXU7594Knf6cpBviW/GonDFEhh77\nmqT/7ZybLumIpLtOXmRmxZL+StJrY3lx51yTc+7G0HNOlHS5c26Wc+5/O+fud879dizPHw1n+jkw\ns791zj0v6fiQ08/6c+CcW+GcOxI6/qqk6hiXfpKn/48j5Zxb5pz7ajxeaxRGGrAl6SXnXH3o64H4\nlRpfI/zzIUlOUknotk9SkyQ5515wznWGjsfzz0fUjHQsnHPLXYik1xX+nq+T9L3QQ69K8pvZpHh8\nD9Fyln8/PyvpOedca+jvwuckXR363kqccytDY/E9SddLknPuN865vtDTJOXPRSykZOiStFTSzYPu\nf0zSj0/eMbP5knIk7VRwJd3sQTMCN0oKxKnOiIwkRIamdv9Ewd84peA/LtcPerovK/ib6IlhXnO5\nmc0K3X7j5G/8ZvZlM7vbzKaY2abQ6b+RND40M3ClmX3XzG4c47cdDef8ORgi0p+DuyT9KmoVhnjx\n/zgkEJoN2mZmX/x/7Z15iFxVFoe/H6PEJTqjMTHbDI3bH0aIS9xwncmYxCjBBEXBfUOR4AKKiru4\nzBAZQR1RRkZBgguiIRh3sU2Lu0aTqASiQhIV1zijoqLm+Mc5lbxUXldVd7pepZvzwaXfu+/Wu/t9\nvzrnvq7Iu9i3SLo4yoOk87XOlfRgxK21mEXf3yZ3w35UHAeSLpH0Rnz2uojbWtICSe+G1en4iP9H\nIZ9b+tN+/RHY/UXOnKjDkkI97pQ0I44fk/TfOD5T0g0bk+cA0Jf5AW7NeULSKlzIlgnttsyPCuhr\nWwAgdyueDDwVUeOAlYUkqyJuMFHWFn+gvF7j4rg+vp4zGJzjYsAZkqLLzBbhImCsfP/NajNbARDK\n/H7gdDNbE+r8BOBWSa/jC/Svvd27Q/RFRI4Avi18w1g7CSTtBfzZzNZztfTCQuAQSdvi7XFQxB8M\n9NSlnQF8GJaB+msdo9E4KEnbdBxI+iv+ULm0DcXtRB+DuwpOBPYEjlNzl/NlwF7hMji3lzRj8HFy\nNPFgljQFdz3sF3ntI+lQ3IL3qZlNDKvTU+HKmAlMiHxaEScDKbAPDBH4pKQJLeQNMCvqNRH4OzAn\n1pqFwCGRZhywexyXzaNK6cv8CC4CppvZeOBe4F/Fi5JOAibh1sVBRT/aosadwMLCuqey2w9UOaug\nrC2AH8uS0kJ9JV2Br6VzB7qsg5GhvGH8EXwxHY0vyISAWABcGaZfAMzsFWJhjIfDbpWXtgFmtkjS\nKEljcZdomYg81czWhBVkg1uES+VW4LQWs+3BXVQf4212hKStgC4zWyapa2PqVCEbjIPeaDQOwup3\nD3CkmX090IXsUB+Duwy+jnwexcXAvAbpFwNzJc1rkG6ema0B3pe0Y8RNibAozofjIqwHuEXSP4HH\nzaxHvr/qJ+AeSQuApgKyUfuVpDVJNYE9DLfU1gTs2/jvqH0vaXrUcddm+ePt9oCZ/QZ8LulFYN+o\n34WSdgfeB7aL/jwQn1+dpqX5IWkkMNHMai7rh1hn3UG+h/MKfO/jz+0rbltpea0ACMvwSOCcQvQq\n1hfw4wk37CCjvi2+Am22TxUAAAOVSURBVA4vXB8PdOP1HV8Xv7a+8g33RwOTLf8/FTC0RdeDwH/w\nH+M8TL7R+DHc377eN2BJo8zsi1iALwVurLy0zWlVRH6F7yPYLCwhtUmwDbAH0B3P7NHAfEkzzOzN\nkvzewL+1foS7X3YAzgbeak/12sZ646BRwt7GgaS/AI8CJxdcUu2g6j6GDb+FGy5AilbwLQrHRwGH\n4tbNq3qxBBUfuir8vdnM7q5PLGkfYDpws6RnzOz6sOxNxq2Ps3F3ajM2WmCb2f8LaZ4I9+AOZtbs\nB33LhDBm9ol8w/E03Oq1PW6F+97MyvZNVU2r82M18EdJu8UcOAL4ANZaV+8GppnZF20ubzvpy1px\nFjAVFxNrCpfmA7Plrvf9gf+Z2WdtKm87qW+Ln4GbtG7z/BTgcjP7RtJ3kg7A95CeAtwOIGkavo4e\nVtjzl5jZkA3AEuCFOD4J+AV4pxD2jGtz8AVkGXBhp8vdS10mAC/jb4GMwV1Nz5eVF3ernBDHdwHn\nlaTpxt84apRnN+7O2gp33awELohrXcDS+uM4vw84ttNtVjYO4rwH+BI3ma8CpjYaB7iFa3Vh3Lw5\nFPoYt4h9iguBLXEr1iRgc1zYjQCG4Ztgr8WFWFd8dnPgc+BPcZ87yvoeFxfgi/RrwPA4HweMAsYC\nW0TcMbhlaTgwKuK2B77pT/sV4g/HrWjFtLX7D4s2/lucj2bdP43eD1hRO+8lz1r9ZgFP43tfRuKb\n80cX2mQFsAv+IF6JvwTR8bnRx/kxM9K+G2Nrp4h/LsZCbX7M73SdKmiLX/G1sVbnqyNewL/j2hKa\nrLGbcihpizOA5RFOL8RPApZGne8ozJ/lMdZrbXRXp+u0KYSOFyBDHzqrdRG5E/5GzXL84Tys5F7d\nzRYEfEP2y3E8FreC7B3nXQwS0TWYQpV9jIulh3FL2jLgmsK18+Pez0Z/XosLrZeijEuBywr3aSi6\n4viC+OwS4BVgZ9xasDjqVrOujom6LY60p/an/eK8rwJ7NvAeLixexd/KbZRfTXQp7rk0ynB8Ic2Z\n+L41og1/AGZ1eqxlyJCh+pA/A5QkSZIkSVIBQ/LtxSRJkiRJkk2NobyRPmkBSVPxf7RZ5GMzm9mJ\n8iQDT/bxxiFpBL7vq57J1oY3WZMkGbqkezFJkiRJkqQC0r2YJEmSJElSASm6kiRJkiRJKiBFV5Ik\nSZIkSQWk6EqSJEmSJKmAFF1JkiRJkiQV8Dtvn1m2XHohGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd49b29c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "star = mpath.Path.unit_regular_star(6)\n",
    "circle = mpath.Path.unit_circle()\n",
    "# concatenate the circle with an internal cutout of the star\n",
    "verts = np.concatenate([circle.vertices, star.vertices[::-1, ...]])\n",
    "codes = np.concatenate([circle.codes, star.codes])\n",
    "cut_star = mpath.Path(verts, codes)\n",
    "\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "x = list(top15_coef.keys()) \n",
    "y = list(top15_coef.values) \n",
    "\n",
    "ax.plot(np.arange(0, len(x), 1), y) #, '--r', marker=cut_star, markersize=15)\n",
    "ax.set_xticklabels(['empty']+x)\n",
    "ax.grid(color='g', linestyle='-', linewidth=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD9CAYAAABQvqc9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nH8SddifzyJknAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd4348c470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "star = mpath.Path.unit_regular_star(6)\n",
    "circle = mpath.Path.unit_circle()\n",
    "# concatenate the circle with an internal cutout of the star\n",
    "verts = np.concatenate([circle.vertices, star.vertices[::-1, ...]])\n",
    "codes = np.concatenate([circle.codes, star.codes])\n",
    "cut_star = mpath.Path(verts, codes)\n",
    "\n",
    "\n",
    "plt.plot(list(top15_coef.keys()), top15_coef.values,  '--r', marker=cut_star, markersize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bd46ea9c50>]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAD8CAYAAABq85ChAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJEkqwv8HTFGBKs2oJxgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd45e9f470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "star = mpath.Path.unit_regular_star(6)\n",
    "circle = mpath.Path.unit_circle()\n",
    "# concatenate the circle with an internal cutout of the star\n",
    "verts = np.concatenate([circle.vertices, star.vertices[::-1, ...]])\n",
    "codes = np.concatenate([circle.codes, star.codes])\n",
    "cut_star = mpath.Path(verts, codes)\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(10,4))\n",
    "x = ['f', 'b', 'c', 'e', 'a']\n",
    "y = [1, 2, 3, 6, 8]\n",
    "\n",
    "ax.set_xticklabels(['f', 'b', 'c', 'e', 'a'])\n",
    "ax.plot(np.arange(5), y, '--r', marker=cut_star, markersize=15)\n",
    "\n",
    "#fig.show()\n",
    "\n",
    "#plt.plot(x, y, '--r', marker=cut_star, markersize=15)\n",
    "#plt.axis.set_xticklabels(['f', 'b', 'c', 'e', 'a'])\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'set_xticklabels'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-288-0813908a21bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'd'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#plt.axis([0, 6, 0, 10])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_xticklabels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'zero'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'b'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'e'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'a'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'set_xticklabels'"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncZekBhl3SWrQ/wFBuIYp1XiAYgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd472cb0b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(['f', 'b', 'c', 'd'], [1,2,3,4], 'ro')\n",
    "#plt.axis([0, 6, 0, 10])\n",
    "plt.axis.set_xticklabels(['zero', 'f', 'b', 'c', 'e', 'a'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0,0,'empty'),\n",
       " Text(0,0,'x'),\n",
       " Text(0,0,'b'),\n",
       " Text(0,0,'c'),\n",
       " Text(0,0,'d'),\n",
       " Text(0,0,'e'),\n",
       " Text(0,0,'f')]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlMAAAD8CAYAAABTlCH6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxVWCBRRrHQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bd47729ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "x = ['x','b','c','d','e','f'] #[2,4,3,6,1,7]\n",
    "y = [1,2,3,4,5,10]\n",
    "\n",
    "ax[0].plot(x, y)\n",
    "\n",
    "ax[1].plot(np.arange(1, len(x)+1), y)\n",
    "ax[1].set_xticklabels(['empty']+x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important predictors are v29, v192, v204\\*, v195_Low\\**, v182, v120, v002, v5. \n",
    "\n",
    "These findings are consistent with the correlation table obtained earlier. The former showed consistent results, where v192, v29, v204, v182, and v120 are top factors as well.\n",
    "\n",
    "Because of similarity/redundancy I have excluded some factors earlier. Most importantly, I have excluded v191 (which is identical to v192). But, I could say that v191 is also an important regressor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\* from which I created several dummies. Important ones are - v204_wifi, v204_business, v204_residential, v204_cellular; and not important - v204_mobile, v204_nan, v204_wired. But overall I can conclude that this feature is important.\n",
    "\n",
    "\\** Below I will do some F-tests, to check if some of the factors are important statistically. It is not really part of ML-approach, it is more out of curiosity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) extra section - performing F tests on various predictors to assess their statistical significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f_regression is used to performa F tests. For the chosen predictors or groups of predictors I will show p-values of F-scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3409731,  0.5936604])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_regression(full_set[['v195_low','v195_moderate']],full_set[target])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v195 - does not appear to be a statistically significatn factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.41016123e-06])"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_regression(full_set[['v29']],full_set[target])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  5.51245921e-08])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_regression(full_set[['v192']],full_set[target])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v29 and v129 - do appear to be a statistically significant factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will use RandomForestClassifier model to do the predictions. It is a very powerful model both theoretically and empirically. It works well against ovefitting. At the same time, it has only few parameters that are not too sensitive too changes. So, it is relatively easy to calibrate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "I will use the same train and test set from Task 2. It will save space and it will allow to compare Random Forest model with Logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 1) Random_Forest with default parameters. __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(a) creating a model with default parameters. Exception - random_state=1 to get the same results for different runs. n_jobs=-1 - utilizing all the cores, it will increase speed, but does not affect accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Random_Forest_model = RandomForestClassifier(n_jobs=-1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(b) cross-validation with basic setings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.75806452  0.78225806  0.80645161  0.81300813  0.80327869]\n",
      "mean accuracy = 0.792612202431\n"
     ]
    }
   ],
   "source": [
    "print_scores(Random_Forest_model, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are not better than simply predicting the majority class. Also, the optimized logistic regression was giving better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 2) Calibrating the model - Part A, looking at paremeters separately__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Firstly will try with the number of trees to be built (n_estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators = 1\n",
      "accuracies = [ 0.66129032  0.71774194  0.71774194  0.77235772  0.71311475]\n",
      "mean accuracy = 0.716449334245\n",
      "\n",
      "n_estimators = 5\n",
      "accuracies = [ 0.75806452  0.75806452  0.76612903  0.80487805  0.81147541]\n",
      "mean accuracy = 0.779722304627\n",
      "\n",
      "n_estimators = 10\n",
      "accuracies = [ 0.75806452  0.78225806  0.80645161  0.81300813  0.80327869]\n",
      "mean accuracy = 0.792612202431\n",
      "\n",
      "n_estimators = 30\n",
      "accuracies = [ 0.79032258  0.82258065  0.79032258  0.82926829  0.81147541]\n",
      "mean accuracy = 0.808793901794\n",
      "\n",
      "n_estimators = 50\n",
      "accuracies = [ 0.79032258  0.81451613  0.80645161  0.82113821  0.83606557]\n",
      "mean accuracy = 0.813698821547\n",
      "\n",
      "n_estimators = 70\n",
      "accuracies = [ 0.7983871   0.80645161  0.81451613  0.82113821  0.83606557]\n",
      "mean accuracy = 0.815311724772\n",
      "\n",
      "n_estimators = 100\n",
      "accuracies = [ 0.79032258  0.81451613  0.82258065  0.80487805  0.81967213]\n",
      "mean accuracy = 0.810393906953\n",
      "\n",
      "n_estimators = 120\n",
      "accuracies = [ 0.79032258  0.81451613  0.7983871   0.80487805  0.82786885]\n",
      "mean accuracy = 0.807194541538\n",
      "\n",
      "n_estimators = 150\n",
      "accuracies = [ 0.79032258  0.81451613  0.7983871   0.80487805  0.82786885]\n",
      "mean accuracy = 0.807194541538\n",
      "\n",
      "n_estimators = 200\n",
      "accuracies = [ 0.79032258  0.81451613  0.7983871   0.80487805  0.82786885]\n",
      "mean accuracy = 0.807194541538\n",
      "\n",
      "n_estimators = 300\n",
      "accuracies = [ 0.79032258  0.80645161  0.80645161  0.81300813  0.81967213]\n",
      "mean accuracy = 0.807181213536\n",
      "\n",
      "n_estimators = 500\n",
      "accuracies = [ 0.7983871   0.80645161  0.80645161  0.82113821  0.81967213]\n",
      "mean accuracy = 0.810420133022\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_estimators in [1, 5, 10, 30, 50, 70, 100, 120, 150, 200, 300, 500]:\n",
    "    Random_Forest_model = RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1, random_state=1)\n",
    "    print('n_estimators =', n_estimators)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "50, 70 and 100 seems to be the best. Will stick to 50 for time-being. With less trees to built, the model will work faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Optimize the min number of observations needed to do the split (min_samples_split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min_samples_split = 2\n",
      "accuracies = [ 0.79032258  0.81451613  0.80645161  0.82113821  0.83606557]\n",
      "mean accuracy = 0.813698821547\n",
      "\n",
      "min_samples_split = 3\n",
      "accuracies = [ 0.80645161  0.81451613  0.81451613  0.81300813  0.81147541]\n",
      "mean accuracy = 0.811993482177\n",
      "\n",
      "min_samples_split = 5\n",
      "accuracies = [ 0.78225806  0.81451613  0.81451613  0.82926829  0.82786885]\n",
      "mean accuracy = 0.813685493545\n",
      "\n",
      "min_samples_split = 7\n",
      "accuracies = [ 0.79032258  0.81451613  0.81451613  0.82113821  0.81967213]\n",
      "mean accuracy = 0.812033036248\n",
      "\n",
      "min_samples_split = 10\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "min_samples_split = 12\n",
      "accuracies = [ 0.78225806  0.80645161  0.80645161  0.81300813  0.81967213]\n",
      "mean accuracy = 0.80556831031\n",
      "\n",
      "min_samples_split = 15\n",
      "accuracies = [ 0.80645161  0.80645161  0.81451613  0.81300813  0.82786885]\n",
      "mean accuracy = 0.813659267476\n",
      "\n",
      "min_samples_split = 20\n",
      "accuracies = [ 0.7983871   0.80645161  0.80645161  0.82113821  0.81147541]\n",
      "mean accuracy = 0.80878078876\n",
      "\n",
      "min_samples_split = 30\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.81300813  0.81967213]\n",
      "mean accuracy = 0.810407019988\n",
      "\n",
      "min_samples_split = 50\n",
      "accuracies = [ 0.80645161  0.80645161  0.7983871   0.80487805  0.81147541]\n",
      "mean accuracy = 0.805528756239\n",
      "\n",
      "min_samples_split = 70\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "min_samples_split = 100\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "min_samples_split = 150\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "min_samples_split = 250\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "min_samples_split = 500\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for min_samples_split in [2, 3, 5, 7, 10, 12, 15, 20, 30, 50, 70, 100, 150, 250, 500]:\n",
    "    Random_Forest_model = RandomForestClassifier(min_samples_split=min_samples_split, n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('min_samples_split =', min_samples_split)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "10 seems to be optimal, will stick to that for some time. Later will need to do a grid search with several parameters at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) Optimize class_weight parameters - whether to adjust weights due to class disbalance or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class_weight = None\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "class_weight = balanced\n",
      "accuracies = [ 0.75806452  0.81451613  0.76612903  0.7804878   0.81967213]\n",
      "mean accuracy = 0.787773922689\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for class_weight in [None, 'balanced']:\n",
    "    Random_Forest_model = RandomForestClassifier(class_weight=class_weight, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('class_weight =', class_weight)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "None is better, thus will stick to default class_weight=None."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Whether bootstrap samples are used when building trees or no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bootstrap = True\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "bootstrap = False\n",
      "accuracies = [ 0.7983871   0.80645161  0.7983871   0.82926829  0.80327869]\n",
      "mean accuracy = 0.807154557532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for bootstrap in [True, False]:\n",
    "    Random_Forest_model = RandomForestClassifier(bootstrap=bootstrap, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('bootstrap =', bootstrap)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Will use the default, bootstrap=True, it is a bit better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) Whether to use out-of-bag samples to estimate the generalization accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oob_score = True\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "oob_score = False\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for oob_score in [True, False]:\n",
    "    Random_Forest_model = RandomForestClassifier(oob_score=oob_score, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('oob_score =', oob_score)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "No difference. Will use the default, oob_score=False."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) criterion - the function to measure the quality of a split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "criterion = entropy\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.82786885]\n",
      "mean accuracy = 0.815285283736\n",
      "\n",
      "criterion = gini\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for criterion in ['entropy', 'gini']:\n",
    "    Random_Forest_model = RandomForestClassifier(criterion=criterion, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('criterion =', criterion)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The default's 'gini' is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(g) max_features - the number of features to consider when looking for the best split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features = auto\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "max_features = sqrt\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "max_features = log2\n",
      "accuracies = [ 0.79032258  0.81451613  0.80645161  0.82926829  0.83606557]\n",
      "mean accuracy = 0.815324837807\n",
      "\n",
      "max_features = None\n",
      "accuracies = [ 0.78225806  0.83870968  0.7983871   0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_features = 0.1\n",
      "accuracies = [ 0.7983871   0.80645161  0.83064516  0.82113821  0.82786885]\n",
      "mean accuracy = 0.816898186962\n",
      "\n",
      "max_features = 0.25\n",
      "accuracies = [ 0.79032258  0.81451613  0.79032258  0.82113821  0.81967213]\n",
      "mean accuracy = 0.80719432657\n",
      "\n",
      "max_features = 0.5\n",
      "accuracies = [ 0.78225806  0.81451613  0.79032258  0.80487805  0.82786885]\n",
      "mean accuracy = 0.803968735087\n",
      "\n",
      "max_features = 0.75\n",
      "accuracies = [ 0.76612903  0.81451613  0.7983871   0.81300813  0.81147541]\n",
      "mean accuracy = 0.800703159596\n",
      "\n",
      "max_features = 1.0\n",
      "accuracies = [ 0.78225806  0.83870968  0.7983871   0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_features in ['auto', 'sqrt', 'log2', None, 0.1, 0.25, 0.5, 0.75, 1.0]:\n",
    "    Random_Forest_model = RandomForestClassifier(max_features=max_features, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('max_features =', max_features)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The default's 'auto' is the best. But will try with the number of features as integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_features = 1\n",
      "accuracies = [ 0.79032258  0.82258065  0.82258065  0.81300813  0.82786885]\n",
      "mean accuracy = 0.815272170702\n",
      "\n",
      "max_features = 2\n",
      "accuracies = [ 0.80645161  0.81451613  0.82258065  0.81300813  0.81147541]\n",
      "mean accuracy = 0.813606385403\n",
      "\n",
      "max_features = 3\n",
      "accuracies = [ 0.7983871   0.82258065  0.83870968  0.81300813  0.81967213]\n",
      "mean accuracy = 0.818471536117\n",
      "\n",
      "max_features = 5\n",
      "accuracies = [ 0.79032258  0.81451613  0.80645161  0.82926829  0.83606557]\n",
      "mean accuracy = 0.815324837807\n",
      "\n",
      "max_features = 10\n",
      "accuracies = [ 0.7983871   0.80645161  0.79032258  0.79674797  0.81147541]\n",
      "mean accuracy = 0.800676933528\n",
      "\n",
      "max_features = 20\n",
      "accuracies = [ 0.7983871   0.80645161  0.80645161  0.81300813  0.82786885]\n",
      "mean accuracy = 0.810433461024\n",
      "\n",
      "max_features = 30\n",
      "accuracies = [ 0.78225806  0.82258065  0.78225806  0.82113821  0.81147541]\n",
      "mean accuracy = 0.803942079082\n",
      "\n",
      "max_features = 40\n",
      "accuracies = [ 0.77419355  0.83064516  0.7983871   0.82113821  0.81147541]\n",
      "mean accuracy = 0.807167885534\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_features in [1, 2, 3, 5, 10, 20, 30, 40]:\n",
    "    Random_Forest_model = RandomForestClassifier(max_features=max_features, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('max_features =', max_features)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "'auto' is still better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(h) The maximum depth of the tree.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth  = 1\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_depth  = 3\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_depth  = 5\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_depth  = 10\n",
      "accuracies = [ 0.79032258  0.80645161  0.80645161  0.81300813  0.81967213]\n",
      "mean accuracy = 0.807181213536\n",
      "\n",
      "max_depth  = 15\n",
      "accuracies = [ 0.81451613  0.80645161  0.81451613  0.82113821  0.83606557]\n",
      "mean accuracy = 0.818537531224\n",
      "\n",
      "max_depth  = 20\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.83606557]\n",
      "mean accuracy = 0.81852441819\n",
      "\n",
      "max_depth  = 30\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "max_depth  = 50\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n",
      "max_depth  = 100\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_depth in [1, 3, 5, 10, 15, 20, 30, 50, 100]:\n",
    "    Random_Forest_model = RandomForestClassifier(max_depth=max_depth, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('max_depth  =', max_depth )\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "30 seems optimal. Will choose it for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(j) max_leaf_nodes  - the max number of leaf nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_leaf_nodes = 2\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_leaf_nodes = 5\n",
      "accuracies = [ 0.80645161  0.80645161  0.80645161  0.80487805  0.81147541]\n",
      "mean accuracy = 0.807141659465\n",
      "\n",
      "max_leaf_nodes = 10\n",
      "accuracies = [ 0.80645161  0.80645161  0.81451613  0.80487805  0.81147541]\n",
      "mean accuracy = 0.808754562691\n",
      "\n",
      "max_leaf_nodes = 20\n",
      "accuracies = [ 0.7983871   0.80645161  0.83064516  0.81300813  0.81967213]\n",
      "mean accuracy = 0.813632826439\n",
      "\n",
      "max_leaf_nodes = 50\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.81967213]\n",
      "mean accuracy = 0.813645939474\n",
      "\n",
      "max_leaf_nodes = 100\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.81967213]\n",
      "mean accuracy = 0.813645939474\n",
      "\n",
      "max_leaf_nodes = 200\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.81967213]\n",
      "mean accuracy = 0.813645939474\n",
      "\n",
      "max_leaf_nodes = 500\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.81967213]\n",
      "mean accuracy = 0.813645939474\n",
      "\n",
      "max_leaf_nodes = 600\n",
      "accuracies = [ 0.7983871   0.80645161  0.82258065  0.82113821  0.81967213]\n",
      "mean accuracy = 0.813645939474\n",
      "\n",
      "max_leaf_nodes = None\n",
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for max_leaf_nodes in [2, 5, 10, 20, 50, 100, 200, 500, 600, None]:\n",
    "    Random_Forest_model = RandomForestClassifier(max_leaf_nodes=max_leaf_nodes, max_depth=30, min_samples_split=10, \n",
    "                                                 n_estimators=50, n_jobs=-1, random_state=1)\n",
    "    print('max_leaf_nodes =', max_leaf_nodes)\n",
    "    print_scores(Random_Forest_model, train[predictors], train[target])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The default's None is better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ 2) Calibrating the model - Part B, optimizing some parameters together with grid search__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Many parameters are interelated. In RandomForest models parameters like max_depth, min_leafs_at_split, number of trees are all aimed at reducing overfitting in one way or anothers. Thus, it is very useful to try them simultaniously. Of course, it is very computationally expensive to estimate all of them together and to try many values. Thus, I will use only some of those I have obtained in Part A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Firstly I define parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid_1 = {\n",
    "    'min_samples_split':[2,5,10,20],\n",
    "    'max_features':['auto',1,2,'log2'],\n",
    "    'max_depth':[5, 20, 30, 100, None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_1 = GridSearchCV(estimator = RandomForestClassifier(n_estimators = 50,                                                                \n",
    "                                                              max_depth=30,\n",
    "                                                              random_state=1, n_jobs=-1), \n",
    "         param_grid = param_grid_1, scoring='accuracy', n_jobs=-1, iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilja.surikovs\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81038, std: 0.00335, params: {'max_depth': 5, 'max_features': 'auto', 'min_samples_split': 2},\n",
       "  mean: 0.81038, std: 0.00335, params: {'max_depth': 5, 'max_features': 'auto', 'min_samples_split': 5},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 'auto', 'min_samples_split': 10},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 'auto', 'min_samples_split': 20},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 1, 'min_samples_split': 2},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 1, 'min_samples_split': 5},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 1, 'min_samples_split': 10},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 1, 'min_samples_split': 20},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 2, 'min_samples_split': 2},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 2, 'min_samples_split': 5},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 2, 'min_samples_split': 10},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 2, 'min_samples_split': 20},\n",
       "  mean: 0.81041, std: 0.00528, params: {'max_depth': 5, 'max_features': 'log2', 'min_samples_split': 2},\n",
       "  mean: 0.80878, std: 0.00548, params: {'max_depth': 5, 'max_features': 'log2', 'min_samples_split': 5},\n",
       "  mean: 0.80875, std: 0.00364, params: {'max_depth': 5, 'max_features': 'log2', 'min_samples_split': 10},\n",
       "  mean: 0.80714, std: 0.00225, params: {'max_depth': 5, 'max_features': 'log2', 'min_samples_split': 20},\n",
       "  mean: 0.81532, std: 0.01630, params: {'max_depth': 20, 'max_features': 'auto', 'min_samples_split': 2},\n",
       "  mean: 0.81365, std: 0.00808, params: {'max_depth': 20, 'max_features': 'auto', 'min_samples_split': 5},\n",
       "  mean: 0.81852, std: 0.00879, params: {'max_depth': 20, 'max_features': 'auto', 'min_samples_split': 10},\n",
       "  mean: 0.81206, std: 0.01079, params: {'max_depth': 20, 'max_features': 'auto', 'min_samples_split': 20},\n",
       "  mean: 0.81042, std: 0.01129, params: {'max_depth': 20, 'max_features': 1, 'min_samples_split': 2},\n",
       "  mean: 0.81205, std: 0.01585, params: {'max_depth': 20, 'max_features': 1, 'min_samples_split': 5},\n",
       "  mean: 0.81366, std: 0.01286, params: {'max_depth': 20, 'max_features': 1, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00586, params: {'max_depth': 20, 'max_features': 1, 'min_samples_split': 20},\n",
       "  mean: 0.80882, std: 0.01211, params: {'max_depth': 20, 'max_features': 2, 'min_samples_split': 2},\n",
       "  mean: 0.81043, std: 0.01592, params: {'max_depth': 20, 'max_features': 2, 'min_samples_split': 5},\n",
       "  mean: 0.81522, std: 0.00818, params: {'max_depth': 20, 'max_features': 2, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00288, params: {'max_depth': 20, 'max_features': 2, 'min_samples_split': 20},\n",
       "  mean: 0.81851, std: 0.01151, params: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 2},\n",
       "  mean: 0.81370, std: 0.01520, params: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 5},\n",
       "  mean: 0.81370, std: 0.01520, params: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 10},\n",
       "  mean: 0.81367, std: 0.01043, params: {'max_depth': 20, 'max_features': 'log2', 'min_samples_split': 20},\n",
       "  mean: 0.81370, std: 0.01520, params: {'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 2},\n",
       "  mean: 0.81369, std: 0.01693, params: {'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 5},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 10},\n",
       "  mean: 0.80878, std: 0.00747, params: {'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 20},\n",
       "  mean: 0.80555, std: 0.01380, params: {'max_depth': 30, 'max_features': 1, 'min_samples_split': 2},\n",
       "  mean: 0.81366, std: 0.01719, params: {'max_depth': 30, 'max_features': 1, 'min_samples_split': 5},\n",
       "  mean: 0.81527, std: 0.01336, params: {'max_depth': 30, 'max_features': 1, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00586, params: {'max_depth': 30, 'max_features': 1, 'min_samples_split': 20},\n",
       "  mean: 0.81205, std: 0.01213, params: {'max_depth': 30, 'max_features': 2, 'min_samples_split': 2},\n",
       "  mean: 0.81205, std: 0.01316, params: {'max_depth': 30, 'max_features': 2, 'min_samples_split': 5},\n",
       "  mean: 0.81361, std: 0.00524, params: {'max_depth': 30, 'max_features': 2, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00288, params: {'max_depth': 30, 'max_features': 2, 'min_samples_split': 20},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 2},\n",
       "  mean: 0.81206, std: 0.01298, params: {'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 5},\n",
       "  mean: 0.81532, std: 0.01630, params: {'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 10},\n",
       "  mean: 0.81206, std: 0.01079, params: {'max_depth': 30, 'max_features': 'log2', 'min_samples_split': 20},\n",
       "  mean: 0.81370, std: 0.01520, params: {'max_depth': 100, 'max_features': 'auto', 'min_samples_split': 2},\n",
       "  mean: 0.81369, std: 0.01693, params: {'max_depth': 100, 'max_features': 'auto', 'min_samples_split': 5},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 100, 'max_features': 'auto', 'min_samples_split': 10},\n",
       "  mean: 0.80878, std: 0.00747, params: {'max_depth': 100, 'max_features': 'auto', 'min_samples_split': 20},\n",
       "  mean: 0.80555, std: 0.01380, params: {'max_depth': 100, 'max_features': 1, 'min_samples_split': 2},\n",
       "  mean: 0.81366, std: 0.01719, params: {'max_depth': 100, 'max_features': 1, 'min_samples_split': 5},\n",
       "  mean: 0.81527, std: 0.01336, params: {'max_depth': 100, 'max_features': 1, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00586, params: {'max_depth': 100, 'max_features': 1, 'min_samples_split': 20},\n",
       "  mean: 0.81205, std: 0.01213, params: {'max_depth': 100, 'max_features': 2, 'min_samples_split': 2},\n",
       "  mean: 0.81205, std: 0.01316, params: {'max_depth': 100, 'max_features': 2, 'min_samples_split': 5},\n",
       "  mean: 0.81361, std: 0.00524, params: {'max_depth': 100, 'max_features': 2, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00288, params: {'max_depth': 100, 'max_features': 2, 'min_samples_split': 20},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 100, 'max_features': 'log2', 'min_samples_split': 2},\n",
       "  mean: 0.81206, std: 0.01298, params: {'max_depth': 100, 'max_features': 'log2', 'min_samples_split': 5},\n",
       "  mean: 0.81532, std: 0.01630, params: {'max_depth': 100, 'max_features': 'log2', 'min_samples_split': 10},\n",
       "  mean: 0.81206, std: 0.01079, params: {'max_depth': 100, 'max_features': 'log2', 'min_samples_split': 20},\n",
       "  mean: 0.81370, std: 0.01520, params: {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 2},\n",
       "  mean: 0.81369, std: 0.01693, params: {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 5},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 10},\n",
       "  mean: 0.80878, std: 0.00747, params: {'max_depth': None, 'max_features': 'auto', 'min_samples_split': 20},\n",
       "  mean: 0.80555, std: 0.01380, params: {'max_depth': None, 'max_features': 1, 'min_samples_split': 2},\n",
       "  mean: 0.81366, std: 0.01719, params: {'max_depth': None, 'max_features': 1, 'min_samples_split': 5},\n",
       "  mean: 0.81527, std: 0.01336, params: {'max_depth': None, 'max_features': 1, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00586, params: {'max_depth': None, 'max_features': 1, 'min_samples_split': 20},\n",
       "  mean: 0.81205, std: 0.01213, params: {'max_depth': None, 'max_features': 2, 'min_samples_split': 2},\n",
       "  mean: 0.81205, std: 0.01316, params: {'max_depth': None, 'max_features': 2, 'min_samples_split': 5},\n",
       "  mean: 0.81361, std: 0.00524, params: {'max_depth': None, 'max_features': 2, 'min_samples_split': 10},\n",
       "  mean: 0.80877, std: 0.00288, params: {'max_depth': None, 'max_features': 2, 'min_samples_split': 20},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': None, 'max_features': 'log2', 'min_samples_split': 2},\n",
       "  mean: 0.81206, std: 0.01298, params: {'max_depth': None, 'max_features': 'log2', 'min_samples_split': 5},\n",
       "  mean: 0.81532, std: 0.01630, params: {'max_depth': None, 'max_features': 'log2', 'min_samples_split': 10},\n",
       "  mean: 0.81206, std: 0.01079, params: {'max_depth': None, 'max_features': 'log2', 'min_samples_split': 20}],\n",
       " {'max_depth': 30, 'max_features': 'auto', 'min_samples_split': 10},\n",
       " 0.8201637624520085)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_1.fit(train[predictors],train[target])\n",
    "grid_search_1.grid_scores_, grid_search_1.best_params_, grid_search_1.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Will narrow down the grid search a bit. For max_features will use 'auto'. Will add number of trees (n_estimators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid_2 = {\n",
    "    'min_samples_split':[6,8,10,12],    \n",
    "    'max_depth':[26, 28, 30, 32, 34, None],\n",
    "    'n_estimators':[10, 50, 100, 300]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_2 = GridSearchCV(estimator = RandomForestClassifier(random_state=1, n_jobs=-1), \n",
    "         param_grid = param_grid_2, scoring='accuracy', n_jobs=-1, iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilja.surikovs\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81683, std: 0.02546, params: {'max_depth': 26, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': 26, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': 26, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': 26, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': 26, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': 26, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 26, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': 26, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': 26, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 26, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': 26, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 26, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': 26, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': 26, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 26, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': 26, 'min_samples_split': 12, 'n_estimators': 300},\n",
       "  mean: 0.81683, std: 0.02546, params: {'max_depth': 28, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': 28, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': 28, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': 28, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': 28, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': 28, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 28, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': 28, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': 28, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 28, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': 28, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 28, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': 28, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': 28, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 28, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': 28, 'min_samples_split': 12, 'n_estimators': 300},\n",
       "  mean: 0.81683, std: 0.02546, params: {'max_depth': 30, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': 30, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': 30, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': 30, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': 30, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': 30, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 30, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': 30, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': 30, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': 30, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 30, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': 30, 'min_samples_split': 12, 'n_estimators': 300},\n",
       "  mean: 0.81683, std: 0.02546, params: {'max_depth': 32, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': 32, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': 32, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': 32, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': 32, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': 32, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 32, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': 32, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': 32, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 32, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': 32, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 32, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': 32, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': 32, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 32, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': 32, 'min_samples_split': 12, 'n_estimators': 300},\n",
       "  mean: 0.81683, std: 0.02546, params: {'max_depth': 34, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': 34, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': 34, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': 34, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': 34, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': 34, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 34, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': 34, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': 34, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': 34, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': 34, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': 34, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': 34, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': 34, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': 34, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': 34, 'min_samples_split': 12, 'n_estimators': 300},\n",
       "  mean: 0.81683, std: 0.02546, params: {'max_depth': None, 'min_samples_split': 6, 'n_estimators': 10},\n",
       "  mean: 0.80877, std: 0.01065, params: {'max_depth': None, 'min_samples_split': 6, 'n_estimators': 50},\n",
       "  mean: 0.81691, std: 0.01235, params: {'max_depth': None, 'min_samples_split': 6, 'n_estimators': 100},\n",
       "  mean: 0.81366, std: 0.01065, params: {'max_depth': None, 'min_samples_split': 6, 'n_estimators': 300},\n",
       "  mean: 0.79748, std: 0.01765, params: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 10},\n",
       "  mean: 0.80718, std: 0.01316, params: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 100},\n",
       "  mean: 0.81690, std: 0.01019, params: {'max_depth': None, 'min_samples_split': 8, 'n_estimators': 300},\n",
       "  mean: 0.80397, std: 0.01281, params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 10},\n",
       "  mean: 0.82016, std: 0.01206, params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 50},\n",
       "  mean: 0.81366, std: 0.00935, params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 100},\n",
       "  mean: 0.81687, std: 0.00930, params: {'max_depth': None, 'min_samples_split': 10, 'n_estimators': 300},\n",
       "  mean: 0.80388, std: 0.01401, params: {'max_depth': None, 'min_samples_split': 12, 'n_estimators': 10},\n",
       "  mean: 0.80557, std: 0.01264, params: {'max_depth': None, 'min_samples_split': 12, 'n_estimators': 50},\n",
       "  mean: 0.81529, std: 0.00979, params: {'max_depth': None, 'min_samples_split': 12, 'n_estimators': 100},\n",
       "  mean: 0.81686, std: 0.01083, params: {'max_depth': None, 'min_samples_split': 12, 'n_estimators': 300}],\n",
       " {'max_depth': 26, 'min_samples_split': 10, 'n_estimators': 50},\n",
       " 0.8201637624520085)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_2.fit(train[predictors],train[target])\n",
    "grid_search_2.grid_scores_, grid_search_2.best_params_, grid_search_2.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Will test for various numbers of trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid_3 = {\n",
    "    'n_estimators':[40, 45, 50, 55, 60, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grid_search_3 = GridSearchCV(estimator = RandomForestClassifier(max_depth=26,\n",
    "                                                                min_samples_split=10,\n",
    "                                                                random_state=1, n_jobs=-1), \n",
    "         param_grid = param_grid_3, scoring='accuracy', n_jobs=-1, iid=False, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ilja.surikovs\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:761: DeprecationWarning: The grid_scores_ attribute was deprecated in version 0.18 in favor of the more elaborate cv_results_ attribute. The grid_scores_ attribute will not be available from 0.20\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([mean: 0.81039, std: 0.00922, params: {'n_estimators': 40},\n",
       "  mean: 0.81691, std: 0.01003, params: {'n_estimators': 45},\n",
       "  mean: 0.82016, std: 0.01206, params: {'n_estimators': 50},\n",
       "  mean: 0.82016, std: 0.01206, params: {'n_estimators': 55},\n",
       "  mean: 0.82016, std: 0.01206, params: {'n_estimators': 60},\n",
       "  mean: 0.81366, std: 0.00935, params: {'n_estimators': 100}],\n",
       " {'n_estimators': 50},\n",
       " 0.8201637624520085)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search_3.fit(train[predictors],train[target])\n",
    "grid_search_3.grid_scores_, grid_search_3.best_params_, grid_search_3.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The optimized model has the following parameters: max_depth=26, min_samples_split=10, n_estimators=50. The other parameters has their default values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__3) Let's check the model (a) with cross-validation on the train set, (b) then simply on the whole train set, (c) then on test set.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracies = [ 0.81451613  0.81451613  0.81451613  0.81300813  0.8442623 ]\n",
      "mean accuracy = 0.820163762452\n"
     ]
    }
   ],
   "source": [
    "Random_Forest_model = RandomForestClassifier(max_depth=26, min_samples_split=10, \n",
    "                                             n_estimators=50, n_jobs=-1, random_state=1)\n",
    "print_scores(Random_Forest_model, train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89951377633711505"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model on train-set and evaluate on train set\n",
    "Random_Forest_model.fit(train[predictors], train[target])\n",
    "Random_Forest_model.score(train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that the model is overfitting. Thus, it is likely that it could be further optimized. In theory I should reach the point where the score on train set (where target is known) and on test set (where target is not known) are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85161290322580641"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check model on the test set\n",
    "Random_Forest_model.score(test[predictors], test[target])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, this model gives a higher accuracy than a majority class prediction model would give. Also, it gives a higher accuracy than the logistic regression in the previous task. However, I do not see a significant improvement, thus a higher focus should be put on feature selection and dealing with missing values. Also, it would be interesting to try some other models like XGboost or some SVM.\n",
    "It is a bit dissapointing the the model does not perform much better than majority class prediction. However, if we would focus not just on accuracy, but on other aspects like precision and recall, than perhaps it would be more useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Deal with imbalaced dataset. Out of 798 observations, response variable is 0 in 645 observations, and it is 1 in 153 cases. It is not a very big disbalance, but it is possible that prediction accuracy would be better if I would deal with this imbalancing. (a) The simplest approach is to randomly remove 492 rows where response variable is 0, this would result in a balaced dataset where we have 153 cases of response variable being 0 and 153 casee being 1. (b) A bit better approach would be to put more weight on obseravations where response is 1. Each such observation would weigh 4.2 (645/153). (c) Employ some of the many other approaches of dealing with imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Columns v173, v175 and v177 contain some date information. It would be good to understand what these dates are about and then to extract some valuable features. It could be: duration, starting and end time in hours, days, months, etc. Such information could be helpful at making better predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) I am mainly removing columns with many NAs. For rows I was more conservative - I was removing only those that had all NA values except for key columns. It might be beneficial to apply a threshold and remove rows that has too many missing values (similarly as I did with columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "4) Use better techniques for dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Use SVM for sparse datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "6) Drop columns that has too few variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "type(corr_with_target_abs_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp_list = ['a','b','c']\n",
    "for i, item in enumerate(tmp_list,1):\n",
    "    print('i=',i)\n",
    "    print('item=',item)\n",
    "\n",
    "print()\n",
    "for j in range(1, len(tmp_list)):\n",
    "    print(tmp_list[j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc[[True, True, True] + [False]*74]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc[my_bool_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc[my_bool_list].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr_with_target_abs_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.fit(train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.score(train[predictors], train[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.score(test[predictors], test[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame(log_reg.coef_, columns=predictors)\n",
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max(coef_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_normalized.corr()['response'].apply(np.abs).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_plot = plt.matshow(dataset_filled.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt_fig = plt_plot.get_figure()\n",
    "plt_fig.savefig('fig_plt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.savefig('graph_1.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pylab qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.matshow(dataset_filled.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_filled.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "corr = dataset_filled.corr()\n",
    "myplot_seaborn=sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myfig_seaborn = myplot_seaborn.get_figure()\n",
    "myfig_seaborn.savefig('fig_seaborn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "take only text columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace ',' with '.' in floats => use regex (*[N*int','M*int])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "impute missing values!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "normalize all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one hot encoder / create dummies for text information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "train regularized regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dataset_full_with_dummies.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_col_names_without_target(dataframe, target = target):\n",
    "    column_names_list = list(dataframe.columns)\n",
    "    if target in column_names_list:\n",
    "        column_names_list.remove(target)\n",
    "    return column_names_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_col_names_without_target(dataframe, target = target):\n",
    "    all_column_names_list = list(dataframe.columns)\n",
    "    col_names_without_target = all_column_names_list.remove[target]\n",
    "    return list(col_names_without_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mylist = ['a', 'b', 'c']\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mylist.remove('a')\n",
    "mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'a' in mylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset_orange.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_orange.save(\"output_dataset_orange.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_normalized.to_csv(\"output_dataset_normalized.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_not_cleaned_keys_dropped.to_csv('output_dataset_full_not_cleaned_keys_dropped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(dataset_full_with_dummies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_dummies_np[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_np2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dataset_full_np2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_np2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_np2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dataset_full_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(dataset_full_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_0_and_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(dataset_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_dummies.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(dataset_full_with_dummies.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_with_dummies.to_csv('output_dataset_full_with_dummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_time_converted.loc[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_time_converted.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_time_converted.to_csv('output_dataset_full_time_converted.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_filled.to_csv('output_dataset_filled.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_0.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_example = dataset_1.copy(deep=True)\n",
    "dataset_example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset_example = dataset_example.dropna(axis=0, how='all', subset=all_columns_no_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dataset_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set(dataset_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list(dataset_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " df1 = pd.DataFrame({'A': ['yes', 'yes', 'no', 'maybe'],\n",
    "                        'B': ['cat1', 'cat1', 'cat2', np.nan],\n",
    "                        'C': [1.6, 5.3, 0.0, 7.3],\n",
    "                        'D': [6, 3, 2, 2]},  index=[0, 1, 2, 3])\n",
    "df1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df1, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'A': [1, 3, 4, 5],\n",
    "                        'B': [3.5, 6.6, 7.89, np.nan],\n",
    "                        'C': [1.6, 5.3, 0.0, 7.3],\n",
    "                        'D': [6, 3, np.nan, 2]},  index=[0, 1, 2, 3])\n",
    "df2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_np = df2.values\n",
    "df2_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame({'E': [4, 3, 3, 5]})\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2['e'] = df3\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "domain = Orange.data.Domain([size, height, shape], speed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df2_orange = Orange.data.Table(my_domain, df2_np)\n",
    "#df2_orange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2_orange.domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(df2_orange.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_np = np.array([[1, 2, 3], [5, 9.8, 14.7],\n",
    "                    [2, 4, np.nan], [1, 2, 3.5], \n",
    "                    [1, 2, 3], [3, 6.1, 8.9],\n",
    "                    [2, 4, 6], [3, 5.9, np.nan],\n",
    "                    [1, 2, 3], [1, 1.8, 3.3]],)\n",
    "set_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_pd = pd.DataFrame(set_np, columns = ['A', 'B', 'C'])\n",
    "set_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_filled=set_pd.copy(deep=True)\n",
    "set_filled.fillna(set_pd.median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_pd.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orange_set = Orange.data.Table(set_np)\n",
    "orange_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Orange.preprocess import Impute\n",
    "imputer = Orange.preprocess.Impute.ModelConstructor()\n",
    "imputer.learner_continuous = imputer.learner_discrete = Orange.classification.tree.TreeLearner(min_subset=20)\n",
    "#imputer.learner_continuous = Orange.ensemble.forest.RandomForestLearner\n",
    "imputer = imputer(orange_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Orange.preprocess import Impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df = pd.DataFrame([[1, 7, np.nan, np.nan, np.nan], [1, 7, np.nan, np.nan, np.nan],\n",
    "                    [1, 2, 3, 4, 5], [3, 4, 5, 1, np.nan],\n",
    "                    [6, 4, 5, np.nan, np.nan], [1, 2, np.nan, np.nan, np.nan], \n",
    "                    [1, 7, np.nan, np.nan, np.nan], [1, 7, np.nan, np.nan, np.nan],\n",
    "                    [1, 7, np.nan, np.nan, np.nan], [1, 7, np.nan, np.nan, np.nan],\n",
    "                    [1, 7, np.nan, np.nan, np.nan], [1, 7, np.nan, np.nan, np.nan]],\n",
    "                    columns=['key1','A','B','C','D'])\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df4 = na_df.copy(deep=True)\n",
    "na_df4 = drop_rows_and_cols_with_NA_below_thresholds(na_df4, key_names=key_names, col_thresh=0.1, row_thresh=0.6).loc[:100]\n",
    "na_df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df4 = na_df.copy(deep=True)\n",
    "na_df4 = na_df4.dropna(axis=1, thresh=1) # droping NA columns\n",
    "na_df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df2=na_df.copy(deep=True)\n",
    "na_df2 = na_df2.dropna(axis=0, how='all',subset={'B','C','A'})\n",
    "na_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df_columns = list(na_df.columns)\n",
    "na_df_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df_columns_set=set(na_df_columns)\n",
    "na_df_columns_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(na_df_columns_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set(na_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df3=na_df.copy(deep=True)\n",
    "drop_NA_only_columns_and_rows(na_df3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[1,2,3] - [1,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set([1,2,3]) - set([1,2,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old drop NA function v1\n",
    "def drop_NA_only_columns_and_rows(input_df, key_names=key_names):\n",
    "    df = input_df.copy(deep=True)\n",
    "    df_columns = set(df)\n",
    "    df_columns_without_keys = df_columns - set(key_names)\n",
    "    df = df.dropna(axis=0, how='all', subset=df_columns_without_keys) # droping rows that have all NA values except for keys\n",
    "    df = df.dropna(axis=1, how='all') # droping NA columns\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old drop NA function v2\n",
    "def drop_rows_with_NA_only_and_cols_with_NA_below_threshold(input_df, key_names=key_names, threshold_percent=0.20):\n",
    "    df = input_df.copy(deep=True)\n",
    "    \n",
    "    df_columns = set(df)\n",
    "    df_columns_without_keys = df_columns - set(key_names)\n",
    "    df = df.dropna(axis=0, how='all', subset=df_columns_without_keys) # droping rows that have all NA values except for keys\n",
    "    \n",
    "    number_of_rows = len(df)\n",
    "    threshold_integer = round(threshold_percent * number_of_rows)\n",
    "    df = df.dropna(axis=1, thresh=threshold_integer) # droping columns that have non-NA cell count is below threshold\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old with regex\n",
    "#function to replace ',' with '.'\n",
    "#import re\n",
    "def replace_commas_with_dots_in_string(single_string):\n",
    "    if type(single_string) == str:\n",
    "        ##df = input_dataframe.copy(deep=True)\n",
    "        ##regex_input = '^[0-9]+,[0-9]+$'\n",
    "        ##regex_output = '^[0-9]+\\.[0-9]+$'\n",
    "        ##single_string = re.sub('^[0-9]+,[0-9]+$', '^[0-9]+\\.[0-9]+$', single_string)\n",
    "        ##df.replace(to_replace=regex_input, value=regex_output, regex=True)\n",
    "        single_string = single_string.replace(',','.')\n",
    "    return single_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dataset_full_clean.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names_no_key_no_target = get_col_names_without_target_and_keys(dataset_full_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean[col_names_no_key_no_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean[col_names_no_key_no_target].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean[col_names_no_key_no_target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_names_no_key_no_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean['v4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_clean['v12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df = pd.DataFrame([[1.5, \"2,5\", 3, 4, 5], [3.5, 4.5, 5, 1, np.nan],\n",
    "                    [6, \"4,4\", 5, \"text with 4,5\", \"4,5 another text\"], [1, 2, np.nan, np.nan, np.nan]],\n",
    "                    columns=['key1','A','B','C','D'])\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df = na_df.applymap(replace_commas_with_dots_in_string)\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df = na_df.applymap(convert_floats_in_string_to_floats)\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replace_commas_with_dots_in_df(na_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df.replace(\",\",\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df.drop('B', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df = na_df.drop(['A','B'], axis=1)\n",
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "na_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full.to_csv('output_dataset_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int(5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_not_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_string_only = dataset_full.select_dtypes(include=['object']).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_string_only.drop(['v173','v175','v177'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_string_only = df_string_only.drop(['v173','v175','v177'], axis=1).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_string_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df_string_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame([['A', \"B\", 'YES', 4, 5], ['A', \"B\", 'YES', 1, np.nan],\n",
    "                    ['C', \"D\", 'NO', 5, 4], ['C',np.nan, 'NO', np.nan, np.nan]],\n",
    "                    columns=['col1','col2','col3','col4','col5'])\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.get_dummies(df1, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.core.dtypes.common.is_datetime_or_timedelta_dtype(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.core.dtypes.common.is_datetime64_ns_dtype(dates)|pd.core.dtypes.common.is_timedelta64_ns_dtype(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(dates, errors='raise', dayfirst=False, yearfirst=False, utc=None, box=True, format=None, exact=True, unit=None, infer_datetime_format=False, origin='unix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_dates = dataset_full['v197'].copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fake_dates.loc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old\n",
    "def get_col_names_without_target_and_keys(dataframe, key_names = key_names, target = target):\n",
    "    all_column_names_set = set(dataframe)\n",
    "    col_names_without_target_and_keys = all_column_names_set - set(key_names) - set([target])\n",
    "    return list(col_names_without_target_and_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def check_if_date(element):\n",
    "    if type(element) == str:\n",
    "        ##df = input_dataframe.copy(deep=True)\n",
    "        ##regex_input = '^[0-9]+,[0-9]+$'\n",
    "        ##regex_output = '^[0-9]+\\.[0-9]+$'\n",
    "        ##single_string = re.sub('^[0-9]+,[0-9]+$', '^[0-9]+\\.[0-9]+$', single_string)\n",
    "        ##df.replace(to_replace=regex_input, value=regex_output, regex=True)\n",
    "        single_string = single_string.replace(',','.')\n",
    "    return single_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) Python uses '.' as a decimal point. However, in datasets sometimes we get ',' as a decimal point. Need to replace ',' with '.'. After this is done, will need to convert floats stored as string to Python floats. Integer columns will also be converted to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_commas_with_dots_in_string(single_string):\n",
    "    if type(single_string) == str:\n",
    "        single_string = single_string.replace(',','.')\n",
    "    return single_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# applying a function on each cell of a dataframe\n",
    "dataset_full = dataset_full.applymap(replace_commas_with_dots_in_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to convert floats stored as string to floats\n",
    "def convert_floats_in_string_to_floats(element):\n",
    "    if type(element) == str or type(element) == int:\n",
    "        try:\n",
    "            return float(element)\n",
    "        except (ValueError, TypeError):\n",
    "            return element\n",
    "    return element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not want to convert key and response columns to float, so need to obtain a list of all columns except for keys and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_to_convert = get_col_names_without_target_and_keys(dataset_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all columns except for keys and response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full.loc[:,columns_to_convert] = (dataset_full[columns_to_convert]).applymap(convert_floats_in_string_to_floats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(dataset_full_copy, format='%Y-%m-%d %H:%M', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(500, format='%Y-%m-%d %H:%M', errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.to_datetime(date, format='%Y-%m-%d %H:%M', errors='ignore').loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date = dataset_full['v173'].copy(deep=True)\n",
    "date.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old\n",
    "def col_to_datetime(input_col):\n",
    "    if input_col.dtype=='O':\n",
    "        col=pd.to_datetime(input_col, format='%Y-%m-%d %H:%M', errors='ignore')\n",
    "        return col\n",
    "    else:\n",
    "        return input_col    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def col_to_datetime(input_col):\n",
    "    if input_col.dtype=='O': #in pandas dataframe columns containing Strings, has type Object, or 'O'\n",
    "        col_datetime=pd.to_datetime(input_col, format='%Y-%m-%d %H:%M', errors='ignore') #convert to datetime only if format is '%Y-%m-%d %H:%M'\n",
    "        if col_datetime.dtype=='datetime64[ns]': \n",
    "            epoch_timestamp_col = col_datetime - dt.datetime(1970, 1, 1)\n",
    "            sec_float_col = epoch_timestamp_col / np.timedelta64(1, 's')\n",
    "            return sec_float_col\n",
    "        return col_datetime\n",
    "    else:\n",
    "        return input_col           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates2=dates.apply(col_to_datetime)\n",
    "dates2.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(dates2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "dates3=dataset_full_copy.apply(col_to_datetime)\n",
    "dates3.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates3.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_copy.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dates = dataset_full[['v173','v175','v177']].copy(deep=True)\n",
    "dates.loc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "type(dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full_copy = dataset_full.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old - removed because Orange does not have imputation library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "(a) In Pandas, Numpy and Scikit learn packages there is no possibility to impute missing values with machine learning algorithms (e.g. to predict value). For that I would need to use Orange package. But to use that package I would need to tranform dataframes from Pandas to Orange.\n",
    "\n",
    "_Note: Both Pandas and Orange dataframes are just wrapers for NumPay, so doing this transformation is not computationally expensive._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 functions to convert Pandas dataframe to Orange table/dataframe\n",
    "def get_feature_description_for_orange_from_pandas(pandas_df):\n",
    "    feature_list = [Orange.data.ContinuousVariable(col) for col in list(pandas_df.columns)]\n",
    "    return Domain(feature_list)\n",
    "\n",
    "def pandas_to_orange_df(pandas_df):\n",
    "    np_array = pandas_df.values\n",
    "    orange_table_domain = get_feature_description_for_orange_from_pandas(pandas_df)\n",
    "    orange_table = Orange.data.Table(orange_table_domain, np_array)\n",
    "    return orange_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_orange = pandas_to_orange_df(dataset_full_with_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_full[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drop_string_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full['response'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full['response'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dataset_full.nunique().columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dataset_full.nunique().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list(dataset_full.nunique().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_counts_in_string_cols_series = dataset_full_clean.select_dtypes(include=[object]).nunique()\n",
    "unique_counts_in_string_cols_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique_counts_in_string_cols_series[unique_counts_in_string_cols_series>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#copy\n",
    "def remove_string_cols_with_unique_value_count_over_threshold(input_df, unique_count_threshold = 20):\n",
    "    df = input_df.copy(deep=True)\n",
    "    unique_counts_in_string_cols_series = df.select_dtypes(include=[object]).nunique() #string is 'object' type\n",
    "    string_cols_over_threshold_series = unique_counts_in_string_cols_series[unique_counts_in_string_cols_series>unique_count_threshold]\n",
    "    list_of_string_cols_over_threshold = list(string_cols_over_threshold_series.keys())\n",
    "    df = df.drop(list_of_string_cols_over_threshold, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_some_string_cols_removed = remove_string_cols_with_unique_value_count_over_threshold(dataset_full_clean, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_with_some_string_cols_removed.to_csv('df_with_some_string_cols_removed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_full.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " dataset_full.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#old\n",
    "def exclude_similar_features_and_get_unique(corr_with_target_series, similarity_param = 0.000001):\n",
    "    exclusion_boolean_list = [True] # first item not to be excluded\n",
    "    for i in range(1, len(corr_with_target_series)):\n",
    "        #print(i,')')\n",
    "        #print('corr_with_target_series[i] =',corr_with_target_series[i])\n",
    "        #print('corr_with_target_series[i-1] =',corr_with_target_series[i-1])\n",
    "        if np.isnan(corr_with_target_series[i]):\n",
    "            exclusion_boolean_list.append(False)\n",
    "            #print('is NAN')\n",
    "        elif (corr_with_target_series[i-1] - corr_with_target_series[i])<=similarity_param:\n",
    "            exclusion_boolean_list.append(False)\n",
    "            #print('similar')\n",
    "        else:\n",
    "            exclusion_boolean_list.append(True)\n",
    "            #print('not_similar')\n",
    "    unique_features = list(corr_with_target_series[exclusion_boolean_list].index)\n",
    "    #print()\n",
    "    #print(boolean_list)\n",
    "    #print()\n",
    "    #print(len(boolean_list))\n",
    "    #print()\n",
    "    #print(len(corr_with_target_series))\n",
    "    return unique_features\n",
    "    #return boolean_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
